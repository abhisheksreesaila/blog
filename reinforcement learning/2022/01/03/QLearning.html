<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Q-Learning Intro | ABLearn</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Q-Learning Intro" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Understand the basics of Q-Learning via theory and code" />
<meta property="og:description" content="Understand the basics of Q-Learning via theory and code" />
<link rel="canonical" href="https://ablearn.io/reinforcement%20learning/2022/01/03/QLearning.html" />
<meta property="og:url" content="https://ablearn.io/reinforcement%20learning/2022/01/03/QLearning.html" />
<meta property="og:site_name" content="ABLearn" />
<meta property="og:image" content="https://ablearn.io/images/post-thumbnails/ql.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-01-03T00:00:00-06:00" />
<script type="application/ld+json">
{"mainEntityOfPage":{"@type":"WebPage","@id":"https://ablearn.io/reinforcement%20learning/2022/01/03/QLearning.html"},"description":"Understand the basics of Q-Learning via theory and code","@type":"BlogPosting","url":"https://ablearn.io/reinforcement%20learning/2022/01/03/QLearning.html","headline":"Q-Learning Intro","dateModified":"2022-01-03T00:00:00-06:00","datePublished":"2022-01-03T00:00:00-06:00","image":"https://ablearn.io/images/post-thumbnails/ql.png","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://ablearn.io/feed.xml" title="ABLearn" /><link rel="shortcut icon" type="image/x-icon" href="/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Q-Learning Intro | ABLearn</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Q-Learning Intro" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Understand the basics of Q-Learning via theory and code" />
<meta property="og:description" content="Understand the basics of Q-Learning via theory and code" />
<link rel="canonical" href="https://ablearn.io/reinforcement%20learning/2022/01/03/QLearning.html" />
<meta property="og:url" content="https://ablearn.io/reinforcement%20learning/2022/01/03/QLearning.html" />
<meta property="og:site_name" content="ABLearn" />
<meta property="og:image" content="https://ablearn.io/images/post-thumbnails/ql.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-01-03T00:00:00-06:00" />
<script type="application/ld+json">
{"mainEntityOfPage":{"@type":"WebPage","@id":"https://ablearn.io/reinforcement%20learning/2022/01/03/QLearning.html"},"description":"Understand the basics of Q-Learning via theory and code","@type":"BlogPosting","url":"https://ablearn.io/reinforcement%20learning/2022/01/03/QLearning.html","headline":"Q-Learning Intro","dateModified":"2022-01-03T00:00:00-06:00","datePublished":"2022-01-03T00:00:00-06:00","image":"https://ablearn.io/images/post-thumbnails/ql.png","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://ablearn.io/feed.xml" title="ABLearn" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head><body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/">ABLearn</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About Me</a><a class="page-link" href="/search/">Search</a><a class="page-link" href="/categories/">Categories</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Q-Learning Intro</h1><p class="page-description">Understand the basics of Q-Learning via theory and code</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2022-01-03T00:00:00-06:00" itemprop="datePublished">
        Jan 3, 2022
      </time>
       â€¢ <span class="read-time" title="Estimated read time">
    
    
      7 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/categories/#Reinforcement Learning">Reinforcement Learning</a>
        
      
      </p>
    

    
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h1"><a href="#Q-Learning">Q-Learning </a>
<ul>
<li class="toc-entry toc-h2"><a href="#Initialize">Initialize </a></li>
<li class="toc-entry toc-h2"><a href="#Choosing-Action">Choosing Action </a></li>
<li class="toc-entry toc-h2"><a href="#TR-Update">TR Update </a></li>
<li class="toc-entry toc-h2"><a href="#Bringing-it-all-together">Bringing it all together </a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#Limitations-of-Q-Learning">Limitations of Q Learning </a></li>
<li class="toc-entry toc-h1"><a href="#Problems-with-DQN">Problems with DQN </a></li>
<li class="toc-entry toc-h1"><a href="#References">References </a></li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2022-01-03-QLearning.ipynb
-->

<div class="container" id="notebook-container">
        
    
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Q-Learning">
<a class="anchor" href="#Q-Learning" aria-hidden="true"><span class="octicon octicon-link"></span></a>Q-Learning<a class="anchor-link" href="#Q-Learning"> </a>
</h1>
<p>The purpose of this blog post is to learn Q learning using code using the theory from the last blog post.</p>
<ul>
<li>You can find the previous one here.  <a href="https://ablearn.io/machine%20learning/2021/08/02/RL.html">Reinforcement Learning - Basics</a>
</li>
</ul>
<p>We will use openAI gym to apply Q learning for a FrozenLakeNoSlip environment I will explain the important parts of the code but the full code can be downloaded <a href="https://colab.research.google.com/drive/1PbJnJonr8VWaOMnbosgxHbDYX4kwntMp?usp=sharing">here</a></p>
<hr>
<p><em>Initialize the OPENAI GYM. Print the action and states (observation space) just to get an idea as to how many actions and states we are dealing with</em></p>
<div class="highlight"><pre><span></span><span class="k">try</span><span class="p">:</span>
    <span class="n">register</span><span class="p">(</span>
        <span class="nb">id</span><span class="o">=</span><span class="s1">'FrozenLakeNoSlip-v0'</span><span class="p">,</span>
        <span class="n">entry_point</span><span class="o">=</span><span class="s1">'gym.envs.toy_text:FrozenLakeEnv'</span><span class="p">,</span>
        <span class="n">kwargs</span><span class="o">=</span><span class="p">{</span><span class="s1">'map_name'</span> <span class="p">:</span> <span class="s1">'4x4'</span><span class="p">,</span> <span class="s1">'is_slippery'</span><span class="p">:</span><span class="kc">False</span><span class="p">},</span>
        <span class="n">max_episode_steps</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
        <span class="n">reward_threshold</span><span class="o">=</span><span class="mf">0.78</span><span class="p">,</span> <span class="c1"># optimum = .8196</span>
    <span class="p">)</span>
<span class="k">except</span><span class="p">:</span>
    <span class="k">pass</span>

<span class="n">env_name</span> <span class="o">=</span> <span class="s2">"FrozenLakeNoSlip-v0"</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="n">env_name</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Observation space:"</span><span class="p">,</span> <span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Action space:"</span><span class="p">,</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="p">)</span>

<span class="sd">""" Output</span>
<span class="sd">#Observation space: Discrete(16)</span>
<span class="sd">#Action space: Discrete(4)</span>
<span class="sd">"""</span>
</pre></div>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<h2 id="Initialize">
<a class="anchor" href="#Initialize" aria-hidden="true"><span class="octicon octicon-link"></span></a>Initialize<a class="anchor-link" href="#Initialize"> </a>
</h2>
<p>Initialize parameters such as "discount factor", "learning rate" and "Q table"</p>
<div class="highlight"><pre><span></span><span class="c1"># Create a class</span>
<span class="k">class</span> <span class="nc">QAgent</span><span class="p">():</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">discount_rate</span><span class="o">=</span><span class="mf">0.97</span><span class="p">,</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">):</span>
          <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">env</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">state_size</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">n</span>  <span class="c1"># init state size</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">=</span> <span class="mf">1.0</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">discount_rate</span> <span class="o">=</span> <span class="n">discount_rate</span>  <span class="c1">#  discount factor</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">learning_rate</span>  <span class="c1">#  learning rate</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">build_model</span><span class="p">()</span>     <span class="c1"># Init Q-Table. Just random values for begin with      </span>

    <span class="k">def</span> <span class="nf">build_model</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>    <span class="c1"># random Q-Table with rows(state size) X columns (action size)</span>
         <span class="bp">self</span><span class="o">.</span><span class="n">q_table</span> <span class="o">=</span> <span class="mf">1e-4</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">state_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">action_size</span><span class="p">])</span>
</pre></div>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<h2 id="Choosing-Action">
<a class="anchor" href="#Choosing-Action" aria-hidden="true"><span class="octicon octicon-link"></span></a>Choosing Action<a class="anchor-link" href="#Choosing-Action"> </a>
</h2>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_action</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>

<span class="c1"># get the Q-value for a state from the "Q table". You will get 4 possible values , since there 4 actions.  (q1, q2, q3, q4)</span>
    <span class="n">q_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_table</span><span class="p">[</span><span class="n">state</span><span class="p">]</span>  

<span class="c1"># get the max of the  (q1, q2, q3, q4). That represents the "BEST ACTION". Its called "greedy approach" since we are always trying to priortize "MAX". however there is a inherent problem with choosing "MAX". See below explanation. </span>
    <span class="n">action_greedy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">q_state</span><span class="p">)</span>

<span class="c1"># Choose "random" action based on a arbritary criteria (epsilon. you can call it anything. its totally arbritary. we are just trying to avoid max all the time. Thats all)</span>
<span class="c1"># If the random.random() &lt; epsilon, choose random, else choose greedy</span>
    <span class="n">action_random</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">get_action</span><span class="p">(</span><span class="n">state</span><span class="p">)</span> 
    <span class="k">return</span> <span class="n">action_random</span>  <span class="k">if</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="k">else</span> <span class="n">action_greedy</span>
</pre></div>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<h2 id="TR-Update">
<a class="anchor" href="#TR-Update" aria-hidden="true"><span class="octicon octicon-link"></span></a>TR Update<a class="anchor-link" href="#TR-Update"> </a>
</h2>
<p>This is the main function which updates the Q table based on the bellman equation.</p>
<p>From the last blog post</p>
<blockquote>
<p>Q(s,a) =  Q(s,a) + $ \alpha $ [$ R(s,a,s^1)  + \gamma  max_{a'} Q(s',a')$ - Q(s,a)]</p>
</blockquote>
<p>One can read this as</p>
<ul>
<li><strong>Qvalue = Current Qvalue +  learning rate (expected future reward - current Q value)</strong></li>
<li><strong>Qvalue = Current Qvalue +  learning rate (BELLMAN ERROR)</strong></li>
</ul>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">experience</span><span class="p">):</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span> <span class="o">=</span> <span class="n">experience</span>    <span class="n">q_next</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_table</span><span class="p">[</span><span class="n">next_state</span><span class="p">]</span>
    <span class="n">q_next</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">action_size</span><span class="p">])</span> <span class="k">if</span> <span class="n">done</span> <span class="k">else</span> <span class="n">q_next</span>

    <span class="c1">#sample = [R + Discount * max(q values)] </span>
    <span class="n">sample</span> <span class="o">=</span> <span class="n">reward</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">discount_rate</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">q_next</span><span class="p">)</span> 

     <span class="c1"># our update function for q(s,a) = q(s,a) + learning rate(bellman error)</span>
                                                             <span class="c1"># bellman error : sample - q(s,a))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">q_table</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">]</span> <span class="o">=</span>  <span class="bp">self</span><span class="o">.</span><span class="n">q_table</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="p">(</span><span class="n">sample</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_table</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">])</span>

    <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">*</span> <span class="mf">0.99</span>
</pre></div>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<h2 id="Bringing-it-all-together">
<a class="anchor" href="#Bringing-it-all-together" aria-hidden="true"><span class="octicon octicon-link"></span></a>Bringing it all together<a class="anchor-link" href="#Bringing-it-all-together"> </a>
</h2>
<div class="highlight"><pre><span></span><span class="n">total_reward</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">ep</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>  <span class="c1"># no of episodes</span>
  <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>  <span class="c1"># reset, start from the beginning</span>
  <span class="n">done</span> <span class="o">=</span> <span class="kc">False</span>
  <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
    <span class="n">action</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">get_action</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>   <span class="c1"># pass the "current state" and get a "random" or "greedy action"</span>
    <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span>  <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span> <span class="c1"># step through the env. the openaigym will do the rest. it will give you the reward, tell you if its complete and give other meta info</span>

    <span class="n">agent</span><span class="o">.</span><span class="n">train</span><span class="p">((</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">))</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>        <span class="c1"># set the current state to the next state</span>
    <span class="n">total_reward</span> <span class="o">+=</span> <span class="n">reward</span>   <span class="c1"># add the rewards</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"s:"</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="s2">"a:"</span><span class="p">,</span> <span class="n">action</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">"Episode: </span><span class="si">{}</span><span class="s2">, Total reward: </span><span class="si">{}</span><span class="s2">, eps: </span><span class="si">{}</span><span class="s2">"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">ep</span><span class="p">,</span><span class="n">total_reward</span><span class="p">,</span><span class="n">agent</span><span class="o">.</span><span class="n">eps</span><span class="p">))</span>
    <span class="n">env</span><span class="o">.</span><span class="n">render</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">agent</span><span class="o">.</span><span class="n">q_table</span><span class="p">)</span>
    <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mf">0.05</span><span class="p">)</span>
    <span class="n">clear_output</span><span class="p">(</span><span class="n">wait</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Limitations-of-Q-Learning">
<a class="anchor" href="#Limitations-of-Q-Learning" aria-hidden="true"><span class="octicon octicon-link"></span></a>Limitations of Q Learning<a class="anchor-link" href="#Limitations-of-Q-Learning"> </a>
</h1>
<p>Q learning is good where the finite number of states and actions AND there are relatively small.  If there are too many states and actions, the number of possibilities are very high and there wont be sufficient memory to hold the table. But the more important concern is that some problems cannot be easily expressed as distinct states and actions.</p>
<p>In the example from FROZEN LAKE, the states were SAFE, FROZEN, HOLE, GOAL. The actions were up, down, left and right.</p>
<p>Lets say in the game pac-man such as these, the states are very "similar" and yet by using a q table it will be shown as 2 different states, and hence we can conclude that it cannot be easily translated. Here state is not a "single unique distinct" entity rather it is more characterized as a "collection" of features.</p>
<p><img src="https://abhisheksreesaila.github.io/blog/images/rl/pac1.png" alt="" title="Pacman State 1"></p>
<p><img src="https://abhisheksreesaila.github.io/blog/images/rl/pac2.png" alt="" title="Pacman State 2"></p>
<p>Useful way to think about it a set of odd numbers.  S={1,3,5}  S1={7,9,11}. Both represent odd numbers. They are similar and represented as a collection. Both "S" and "S1" are states represented by "ODD" numbers but they contain different values in them. Just like the pac-man example, we need a "same/similar" action to be performed on them but looking at them individually value by value we fail to capture the "pattern" or the "essence"</p>
<p>The point here is we need to represent "state" as a collection and capture the essence rather than treating it as a single distinct entity. It is in cases like this, we take the help of a function which take in a bunch of features, "understand their essence/patterns" and spits out a Q-values for each action. See below.</p>
<p><img src="https://abhisheksreesaila.github.io/blog/images/rl/univeral_function_approx.png" alt="" title="Univeral Function Approximator"></p>
<p>The best known universal function approximators today are none other than "neural networks". We can employ neural networks to learn the Q-values for different states. Neural Networks need some basic components to work effectively.
Lets see what they are and how we get can them.</p>
<ul>
<li>
<strong>Input</strong> : Bunch of features</li>
<li>
<strong>Output</strong> : Q-Values for each action (of course, like before, we will chose the one with the highest value)</li>
<li>
<strong>Loss Function</strong> : We can take the Bellman Error and Minimize them. However, the loss function we chose has to be "differentiable". In other words, we should be apply backpropogation to transfer the loss from the "output layer" all the way back to the first hidden layer to update the "<strong>weights or parameters $ \phi $</strong>" in the network. Lets see how we can convert "bellman error" to a differentiable function after which we are set to use the "neural networks" in our world of "Q learning".
<div class="flash">
    <svg class="octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info" viewbox="0 0 14 16" version="1.1" width="14" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M6.3 5.69a.942.942 0 01-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 01-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"></path></svg>
    <strong>Note: </strong>Since we combined "deep learning" with "Q learning" - we call it. "Deep Q Learning"
</div>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Our incremental update formula is</p>
<blockquote>
<p>$ Q(s,a,\phi) $ =  $ Q(s,a, \phi) $ + $ \alpha $ [$ R(s,a,s^1)  + \gamma  max_{a'} Q(s',a', \phi)$ - $ Q(s,a, \phi)] $</p>
</blockquote>
<p>Consider y = $ R(s,a,s^1)  + \gamma  max_{a'} Q(s',a', \phi)$</p>
<blockquote>
<p>$Q(s,a, \phi)$ =  $ Q(s,a, \phi) $  - $ \alpha [Q(s,a, \phi) - y]$</p>
</blockquote>
<p>Just like the mean squared loss, we can square the bellman error to punish the large variances.</p>
<blockquote>
<p>$ Q(s,a, \phi)$ =  $ Q(s,a, \phi) $ - $ \alpha [Q(s,a, \phi) - y]^2 $</p>
</blockquote>
<p>In the backward pass the gradient update of the parameters $ \phi $ of the Q function then looks like this, which can be optimized via stochastic gradient descent. Lets apply differential function w.r.t to $ \phi $ (parameters) to both sides of the equation</p>
<p>$ \phi  = \phi - \alpha \frac{\partial (Q(s,a) - y)^2}{\partial \phi} $</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Problems-with-DQN">
<a class="anchor" href="#Problems-with-DQN" aria-hidden="true"><span class="octicon octicon-link"></span></a>Problems with DQN<a class="anchor-link" href="#Problems-with-DQN"> </a>
</h1>
<ol>
<li>
<p>Corelated samples</p>
<ul>
<li>Unlike the SGD, the samples chosen during "Q learning" are close to each other and can overfit and does not converge to the global minima</li>
<li>Solution : maintain a queue of past experiences;  use them to update the gradients. Breaks the close corelations</li>
</ul>
</li>
<li>
<p>Moving Targets</p>
<ul>
<li>Use 2 NN</li>
<li>Compute the labels using the TARGET Network. fix it for a while</li>
<li>Q Network will work as normal</li>
<li>Stablizes the training for a certain duration</li>
</ul>
</li>
</ol>
<ul>
<li>Mentioned in the paper Mink et al.  </li>
<li>shown to solve a lot of problems</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="References">
<a class="anchor" href="#References" aria-hidden="true"><span class="octicon octicon-link"></span></a>References<a class="anchor-link" href="#References"> </a>
</h1>
<p><a href="https://www.youtube.com/watch?v=RaIcFiNqP-0&amp;list=PLYgyoWurxA_8ePNUuTLDtMvzyf-YW7im2&amp;index=10">Target Networks</a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">

</div>
</div>
</div>
</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="abhisheksreesaila/blog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/reinforcement%20learning/2022/01/03/QLearning.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Always Learning</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/abhisheksreesaila" target="_blank" title="abhisheksreesaila"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/asreesaila" target="_blank" title="asreesaila"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
