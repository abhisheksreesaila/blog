<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.0">Jekyll</generator><link href="https://ablearn.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://ablearn.io/" rel="alternate" type="text/html" /><updated>2022-10-23T01:48:54-05:00</updated><id>https://ablearn.io/feed.xml</id><title type="html">ABLearn</title><subtitle>Always Learning</subtitle><entry><title type="html">Kalman Filter - Intro</title><link href="https://ablearn.io/robotics/2022/10/21/KalmanFiltering.html" rel="alternate" type="text/html" title="Kalman Filter - Intro" /><published>2022-10-21T00:00:00-05:00</published><updated>2022-10-21T00:00:00-05:00</updated><id>https://ablearn.io/robotics/2022/10/21/KalmanFiltering</id><content type="html" xml:base="https://ablearn.io/robotics/2022/10/21/KalmanFiltering.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2022-10-21-KalmanFiltering.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Kalman-Filter&quot;&gt;Kalman Filter&lt;a class=&quot;anchor-link&quot; href=&quot;#Kalman-Filter&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;Its an iterative algorithm to reduce noise (or uncertainity) and arrive to a really good approximate value efficiently.&lt;/p&gt;
&lt;p&gt;Kalman filter has the these high level components:-&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;PREDICTION&lt;/strong&gt; - Predict the next state of the object. &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;MEASUREMENT&lt;/strong&gt; - Measure the current state of the object.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CONFIDENCE&lt;/strong&gt; - How good (or bad) the PREDICTED or MEASUREMENT values&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;Intuition&quot;&gt;Intuition&lt;a class=&quot;anchor-link&quot; href=&quot;#Intuition&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;Kalman filter (or variants) have the following intuition.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CURRENT STATE&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Suppose current time is 3:00pm&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;PREDICTION over MEASUREMENT&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;After few mins, we look at the watch again and we PREDICT its 3:04pm, but its shows 3:55pm!! &lt;/li&gt;
&lt;li&gt;We know the MEASUREMENT is wrong and  have every reason to believe the PREDICTION is more acccurate and set the watch to be 3:04pm. Our CONFIDENCE in the MEASUREMENT is shaken, we believe it is so noisy to show 3:55pm &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;MEASUREMENT over PREDICTION&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;On the other hand, if the measurement shows 3:05 pm, we have greater confidence that MEASUREMENT is correct, since its more or less in line with our expectations and since its actually measured, thats should be more accurate. Here we trust or more CONFIDENCE in the MEASUREMENT. &lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;Visual&quot;&gt;Visual&lt;a class=&quot;anchor-link&quot; href=&quot;#Visual&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;&lt;img src=&quot;https://abhisheksreesaila.github.io/blog/images/general/KF1.png&quot; alt=&quot;&quot; title=&quot;Kalman Filter&quot; /&gt;&lt;/p&gt;
&lt;h1 id=&quot;3-Main-Calculations&quot;&gt;3 Main Calculations&lt;a class=&quot;anchor-link&quot; href=&quot;#3-Main-Calculations&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;h2 id=&quot;Calculate-the-kalman-gain&quot;&gt;Calculate the kalman gain&lt;a class=&quot;anchor-link&quot; href=&quot;#Calculate-the-kalman-gain&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;KG = $\frac {Eest} {Eest + Emea} $ ...(1)  0 &amp;lt;= KG &amp;lt;= 1&lt;/p&gt;
&lt;p&gt;meaurements are more accurate if KG is close to 1
estimate are more correct if KG is close to 0&lt;/p&gt;
&lt;h2 id=&quot;Calculate-the-current-estimate&quot;&gt;Calculate the current estimate&lt;a class=&quot;anchor-link&quot; href=&quot;#Calculate-the-current-estimate&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;EST(t) = EST(t-1) + KG [MEA - EST(t-1)]  ....(2)&lt;/p&gt;
&lt;p&gt;ESTt-1 = Previous Estimate
ESTt = Current Estimate
MEA = MEasurement
Emea = Error in measurement&lt;/p&gt;
&lt;h2 id=&quot;Calculate-the-error-in-estimate&quot;&gt;Calculate the error in estimate&lt;a class=&quot;anchor-link&quot; href=&quot;#Calculate-the-error-in-estimate&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt; Eest(t) = Emea * Eest(t-1) / Emea + Eest(t-1) ....(3)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;===&amp;gt; Eest(t) = [1-KG] Eest(t-1)&lt;/p&gt;
&lt;p&gt;What does this mean?&lt;/p&gt;
&lt;p&gt;If measurement comes with a very little error, then we are closing in on the true value.  Emea is low, then KG is high (close to 1).
Then (1-KG) is very low. that means its drops the value of Eest(t-1) significantly, hence moving towards the &quot;true&quot; value.  Intuitively it makes sense, measurements are good, the system should converge to a good estimate.  Note that, in the real world, no matter how good the measurement is there is always noise, it will never be perfect.&lt;/p&gt;
&lt;p&gt;The opposite is also true. Measurements are unstable, then Emeais large, KG is low, 1-KG is high (or close to 1). Then we can trust the measurements coming, so we do not move the needle as much and try to keep the value close to the current error in estimate value as much as possible.&lt;/p&gt;
&lt;p&gt;Here is a fantastic explaination
&lt;a href=&quot;https://www.youtube.com/watch?v=X9cC0o9viTo&amp;amp;list=PLX2gX-ftPVXU3oUFNATxGXY90AULiqnWT&amp;amp;index=4&quot;&gt;https://www.youtube.com/watch?v=X9cC0o9viTo&amp;amp;list=PLX2gX-ftPVXU3oUFNATxGXY90AULiqnWT&amp;amp;index=4&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Time for some code to explain and see how kalman filter can find the &quot;true&quot; value&quot; with an example&lt;/p&gt;
&lt;h1 id=&quot;Code&quot;&gt;Code&lt;a class=&quot;anchor-link&quot; href=&quot;#Code&quot;&gt; &lt;/a&gt;&lt;/h1&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# TRUE VALUE. We will see how quickly and how close kalman filter can get&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;true_temp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;72&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;#=======================================================&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;#measurement read from the sensor&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;readSensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randint&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;68&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;76&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;#Initial Setting&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;initial_estimate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;68&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;intitial_error_in_estimate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;initial_measurement&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;75&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;error_in_measurement&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;#declare variable to be used later &lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;new_error_in_estimate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;current_estimate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;store_estimates_for_graph&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;#kalman filter iterations&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;duration&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;time_elapsed&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time_elapsed&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;duration&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;

  &lt;span class=&quot;c1&quot;&gt;# if its first time, use these initial values else,&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time_elapsed&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;error_in_estimate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;intitial_error_in_estimate&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;previous_estimate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;initial_estimate&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;measurement&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;initial_measurement&lt;/span&gt;
    
  &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
   &lt;span class=&quot;c1&quot;&gt;# use calculated values from the prev iteration&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;error_in_estimate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;new_error_in_estimate&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;previous_estimate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;current_estimate&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;measurement&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;readSensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

 &lt;span class=&quot;c1&quot;&gt;# 3 calculations from above &lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;kalman_gain&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;error_in_estimate&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;error_in_estimate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;error_in_measurement&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;current_estimate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;previous_estimate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kalman_gain&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;measurement&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;previous_estimate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;new_error_in_estimate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kalman_gain&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;error_in_estimate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

  &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;New Estimate -&amp;gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;current_estimate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;store_estimates_for_graph&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;current_estimate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;time_elapsed&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;plt&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;total_time&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;duration&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;store_estimates_for_graph&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# Plot the chart&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;x1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;total_time&lt;/span&gt; 
&lt;span class=&quot;n&quot;&gt;y1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;true_temp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;duration&lt;/span&gt;
  
&lt;span class=&quot;c1&quot;&gt;# second plot with x1 and y1 data&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;--&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
 

&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;time elapsed&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;temperature&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;kalman filter&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# display&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;References&quot;&gt;References&lt;a class=&quot;anchor-link&quot; href=&quot;#References&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;The only and only Michel Van Biezen Explanation example for Kalman Filter
&lt;a href=&quot;https://www.youtube.com/watch?v=PZrFFg5_Sd0&amp;amp;list=PLX2gX-ftPVXU3oUFNATxGXY90AULiqnWT&amp;amp;index=5&quot;&gt;https://www.youtube.com/watch?v=PZrFFg5_Sd0&amp;amp;list=PLX2gX-ftPVXU3oUFNATxGXY90AULiqnWT&amp;amp;index=5&lt;/a&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://ablearn.io/images/post-thumbnails/robotics.jpg" /><media:content medium="image" url="https://ablearn.io/images/post-thumbnails/robotics.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">comma ai - self driving for the masses</title><link href="https://ablearn.io/self%20driving/2022/10/01/comma.html" rel="alternate" type="text/html" title="comma ai - self driving for the masses" /><published>2022-10-01T00:00:00-05:00</published><updated>2022-10-01T00:00:00-05:00</updated><id>https://ablearn.io/self%20driving/2022/10/01/comma</id><content type="html" xml:base="https://ablearn.io/self%20driving/2022/10/01/comma.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2022-10-01-comma.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Introduction&quot;&gt;Introduction&lt;a class=&quot;anchor-link&quot; href=&quot;#Introduction&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;This is a multi blog posts about the openpilot software developed by comma ai.  I wil try to deconstruct as much as i can and along the way tell you why it could be the future of self driving.&lt;/p&gt;
&lt;h1 id=&quot;Principle&quot;&gt;Principle&lt;a class=&quot;anchor-link&quot; href=&quot;#Principle&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;Every self driving car (except TESLA) will tell you the reason why they want to develop autonomous cars is to make driving safer and humans are bad drivers. Humans crash cars! Humans are distracted!. Yes all of this is absolutely true! but for 1-5% of the population. the other 95% are good drivers and been doing so for the last 100 years!  So, there is nothing wrong with the system. We only need to is to make our commute better like less annoying (better GPS?), less fatigue (better cruise control?) and a good music system. What comma ai is focusing (and every other company should focus) is a reduction of fatigue and making driving chill! thats it.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;So, how do we do that? write a software that copies the best driving agent in the world - humans! &lt;/strong&gt;&lt;/p&gt;
&lt;h1 id=&quot;Definitions&quot;&gt;Definitions&lt;a class=&quot;anchor-link&quot; href=&quot;#Definitions&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;Now we believe in the priniciple, lets define few car terms so that we understand&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ACC - Adaptive Cruise Control&lt;/li&gt;
&lt;li&gt;LKAS - Lane Keep Assistance System&lt;/li&gt;
&lt;li&gt;CAN - Controller Area Network&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;Core-Ideas-of-comma-ai&quot;&gt;Core Ideas of comma ai&lt;a class=&quot;anchor-link&quot; href=&quot;#Core-Ideas-of-comma-ai&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;next..&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://ablearn.io/images/post-thumbnails/comma-ai.png" /><media:content medium="image" url="https://ablearn.io/images/post-thumbnails/comma-ai.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">ML Model Optimization</title><link href="https://ablearn.io/mschine%20learning/2022/07/05/NeuralOptimization.html" rel="alternate" type="text/html" title="ML Model Optimization" /><published>2022-07-05T00:00:00-05:00</published><updated>2022-07-05T00:00:00-05:00</updated><id>https://ablearn.io/mschine%20learning/2022/07/05/NeuralOptimization</id><content type="html" xml:base="https://ablearn.io/mschine%20learning/2022/07/05/NeuralOptimization.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2022-07-05-NeuralOptimization.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Definition&quot;&gt;Definition&lt;a class=&quot;anchor-link&quot; href=&quot;#Definition&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;Top techniques&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Quantization&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;convert floats to integers &lt;/li&gt;
&lt;li&gt;slight reduction in accuracy but 2-4x gain in speed&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;code here&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Pruning&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Make the model sparse. &lt;/li&gt;
&lt;li&gt;Certain % of weights (parameters) is set to 0&lt;/li&gt;
&lt;li&gt;reduction in accuracy for gain in speed&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Code here&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Knowledge distillation&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;uses teacher and student network&lt;/li&gt;
&lt;li&gt;take the output of teacher network and use its output to run student network&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Multi task learning&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;use a single backbone but multi-heads to do variety of tasks. More on multi task learning can be found here&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;References&quot;&gt;References&lt;a class=&quot;anchor-link&quot; href=&quot;#References&quot;&gt; &lt;/a&gt;&lt;/h1&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://ablearn.io/images/post-thumbnails/ModelOptimization.jpeg" /><media:content medium="image" url="https://ablearn.io/images/post-thumbnails/ModelOptimization.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Object Tracking</title><link href="https://ablearn.io/computer%20vision/2022/06/25/ObjectTracking.html" rel="alternate" type="text/html" title="Object Tracking" /><published>2022-06-25T00:00:00-05:00</published><updated>2022-06-25T00:00:00-05:00</updated><id>https://ablearn.io/computer%20vision/2022/06/25/ObjectTracking</id><content type="html" xml:base="https://ablearn.io/computer%20vision/2022/06/25/ObjectTracking.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2022-06-25-ObjectTracking.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Definition&quot;&gt;Definition&lt;a class=&quot;anchor-link&quot; href=&quot;#Definition&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;SORT - simple online tracking tool&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;SORT is a method used to track object at real time. the output could look like below&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tracking by detection&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;making use of object detection in the first frame&lt;/li&gt;
&lt;li&gt;use kalman filter for estimating the motion in the next frame&lt;/li&gt;
&lt;li&gt;associate bounding boxes between frames using hungarian algorithm (optimal matching/association algorithm)&lt;ul&gt;
&lt;li&gt;use IOU as cost&lt;/li&gt;
&lt;li&gt;but can use linear cost or exponential cost as well&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Code &lt;em&gt;here&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DeepSort&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Improve SORT by including &quot;visual appearance&quot; along with kalman filter for better predicability&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;References&quot;&gt;References&lt;a class=&quot;anchor-link&quot; href=&quot;#References&quot;&gt; &lt;/a&gt;&lt;/h1&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://ablearn.io/images/post-thumbnails/ObjectTracking.jpeg" /><media:content medium="image" url="https://ablearn.io/images/post-thumbnails/ObjectTracking.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Multi Task Learning</title><link href="https://ablearn.io/computer%20vision/2022/03/25/Hydranets.html" rel="alternate" type="text/html" title="Multi Task Learning" /><published>2022-03-25T00:00:00-05:00</published><updated>2022-03-25T00:00:00-05:00</updated><id>https://ablearn.io/computer%20vision/2022/03/25/Hydranets</id><content type="html" xml:base="https://ablearn.io/computer%20vision/2022/03/25/Hydranets.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2022-03-25-Hydranets.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Definition&quot;&gt;Definition&lt;a class=&quot;anchor-link&quot; href=&quot;#Definition&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Multi Task Learning&lt;/strong&gt; - Adaption of a single neural network model to multiple tasks such as segmentation, depth estimation etc.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Segmenation&lt;/strong&gt; :  Per Pixel Classification of an image to identify objects such as trees, roads, people etc.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Depth estimation&lt;/strong&gt; : Estimating the distance from the camera&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We will producing a output such as the one below.

&lt;center&gt;
    &lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/iXpnkIxrbq0&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;
&lt;/p&gt;
&lt;h1 id=&quot;Explanation&quot;&gt;Explanation&lt;a class=&quot;anchor-link&quot; href=&quot;#Explanation&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;h2 id=&quot;What-tasks-can-be-combined?&quot;&gt;What tasks can be combined?&lt;a class=&quot;anchor-link&quot; href=&quot;#What-tasks-can-be-combined?&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Based on this &lt;a href=&quot;https://arxiv.org/pdf/1905.07553.pdf&quot;&gt;paper&lt;/a&gt; the following observations were made&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Segmentation + anything = becomes better&lt;/li&gt;
&lt;li&gt;Anything + segmentation = make it worse&lt;/li&gt;
&lt;li&gt;Anything + Normals = makes it better&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;Architecture&quot;&gt;Architecture&lt;a class=&quot;anchor-link&quot; href=&quot;#Architecture&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;The 2 most recent and popular ones are&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/&quot;&gt;Google Pathways&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=IHH47nZ7FZU&amp;amp;t=248s&quot;&gt;Tesla's Hydranets&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;Our-Implementation&quot;&gt;Our Implementation&lt;a class=&quot;anchor-link&quot; href=&quot;#Our-Implementation&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;We will focus on Tesla's Hydranets in this blog post. We will recreate a modified version of the hydranets based on this &lt;a href=&quot;https://arxiv.org/pdf/1809.04766.pdf&quot;&gt;Real-Time Joint Semantic Segmentation and Depth Estimation Using
Asymmetric Annotations&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://abhisheksreesaila.github.io/blog/images/mtl/ModifiedHydranets.png&quot; alt=&quot;&quot; title=&quot;Architecture&quot; /&gt;&lt;/p&gt;
&lt;h3 id=&quot;Code-Walkthrough&quot;&gt;Code Walkthrough&lt;a class=&quot;anchor-link&quot; href=&quot;#Code-Walkthrough&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Its a encoder - decoder network. We use &lt;strong&gt;&lt;a href=&quot;https://arxiv.org/abs/1801.04381&quot;&gt;MobileNetv2&lt;/a&gt;&lt;/strong&gt; as encoder and &lt;strong&gt;&lt;a href=&quot;https://arxiv.org/pdf/1810.03272.pdf&quot;&gt;RefineNet&lt;/a&gt;&lt;/strong&gt; as decoder.&lt;/p&gt;
&lt;h4 id=&quot;Encoder&quot;&gt;Encoder&lt;a class=&quot;anchor-link&quot; href=&quot;#Encoder&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;img src=&quot;https://abhisheksreesaila.github.io/blog/images/mtl/mobilenet_v2.jpeg&quot; alt=&quot;&quot; title=&quot;MobileNetv2&quot; /&gt;&lt;/p&gt;
&lt;p&gt;There are 2 important concepts :-&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;DepthWise Separable Convolutions&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;https://abhisheksreesaila.github.io/blog/images/mtl/DepthWise.jpg&quot; alt=&quot;&quot; title=&quot;Depth Wise&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://abhisheksreesaila.github.io/blog/images/mtl/NormalConv.jpg&quot; alt=&quot;&quot; title=&quot;Standard Convolutions&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://abhisheksreesaila.github.io/blog/images/mtl/NormalConv.jpg&quot; alt=&quot;&quot; title=&quot;Standard Convolutions&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://abhisheksreesaila.github.io/blog/images/mtl/ReducedParams.jpg&quot; alt=&quot;&quot; title=&quot;Reduction in parameters&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Residual Networks &amp;amp; Inverted Residual Networks&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;https://abhisheksreesaila.github.io/blog/images/mtl/ResidualvsNonResidual.png&quot; alt=&quot;&quot; title=&quot;Residual vs NonResidual Block&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://abhisheksreesaila.github.io/blog/images/mtl/ResidualNetwork.jpg&quot; alt=&quot;&quot; title=&quot;Residual Block&quot; /&gt;&lt;/p&gt;
&lt;h4 id=&quot;Decoder&quot;&gt;Decoder&lt;a class=&quot;anchor-link&quot; href=&quot;#Decoder&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;img src=&quot;https://abhisheksreesaila.github.io/blog/images/mtl/RefineNet.png&quot; alt=&quot;&quot; title=&quot;RefineNet&quot; /&gt;&lt;/p&gt;
&lt;h5 id=&quot;Highlights-of-the-network&quot;&gt;Highlights of the network&lt;a class=&quot;anchor-link&quot; href=&quot;#Highlights-of-the-network&quot;&gt; &lt;/a&gt;&lt;/h5&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;RCU - Residual Conv Unit&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Simplified version of the convolution unit in the original ResNet where the batch-normalization layers are removed&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Multi Resolution Fusion&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;All path inputs are then fused into a high-resolution feature map by the multi-resolution fusion block.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Chained Residual Pooling&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The proposed chained residual pooling aims to capture background context from a large
image region. It is able to efficiently pool features with multiple window sizes and fuse     them together using learnable weights. In particular, this component is built as a
chain of multiple pooling blocks, each consisting of one max-pooling layer and one   convolution layer. One pooling block takes the output of the previous pooling block as input. Therefore, the current pooling block is able to re-use the result from the previous pooling operation and thus access the features from a large region without using a large pooling window.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here is the &lt;a href=&quot;https://colab.research.google.com/drive/1Q8Oi37D-Qf2d5Oemo5aXuq1kPC5vePCZ?usp=sharing&quot;&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;h5 id=&quot;Output&quot;&gt;Output&lt;a class=&quot;anchor-link&quot; href=&quot;#Output&quot;&gt; &lt;/a&gt;&lt;/h5&gt;&lt;p&gt;&lt;img src=&quot;https://abhisheksreesaila.github.io/blog/images/mtl/HydranetOutput.png&quot; alt=&quot;&quot; title=&quot;Output&quot; /&gt;

&lt;center&gt;
    &lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/iXpnkIxrbq0&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;
&lt;/p&gt;
&lt;h1 id=&quot;References&quot;&gt;References&lt;a class=&quot;anchor-link&quot; href=&quot;#References&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/DrSleep/multi-task-refinenet/blob/master/src/notebooks/ExpNYUDKITTI_joint.ipynb/&quot;&gt;Implementation Notebook&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1801.04381.pdf&quot;&gt;Architecture Images From This Paper&lt;/a&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://ablearn.io/images/post-thumbnails/mtl.png" /><media:content medium="image" url="https://ablearn.io/images/post-thumbnails/mtl.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Actor Critic</title><link href="https://ablearn.io/reinforcement%20learning/2022/01/10/ActorCritic.html" rel="alternate" type="text/html" title="Actor Critic" /><published>2022-01-10T00:00:00-06:00</published><updated>2022-01-10T00:00:00-06:00</updated><id>https://ablearn.io/reinforcement%20learning/2022/01/10/ActorCritic</id><content type="html" xml:base="https://ablearn.io/reinforcement%20learning/2022/01/10/ActorCritic.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2022-01-10-ActorCritic.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Actor-Critic-Reinforcement-Model&quot;&gt;Actor Critic Reinforcement Model&lt;a class=&quot;anchor-link&quot; href=&quot;#Actor-Critic-Reinforcement-Model&quot;&gt; &lt;/a&gt;&lt;/h1&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;There are 2 networks&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Actor - policy network - gives action &lt;/li&gt;
&lt;li&gt;Critic - uses action to go to next state - evaluates state - critizes the action to give the best state possible based on the (reward setup + discount factor)&lt;ul&gt;
&lt;li&gt;good actions lead to good qvalues - so critism goes down - then actor starts producing good action and qnetworks critizes keeps going down  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Explanation coming soon...&lt;/p&gt;
&lt;p&gt;Meanwhile code &lt;a href=&quot;https://colab.research.google.com/drive/14LdUWgd0DZ7krc3NgWM0djlO1mfr2SF_?usp=sharing&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://ablearn.io/images/post-thumbnails/ActorCritic.png" /><media:content medium="image" url="https://ablearn.io/images/post-thumbnails/ActorCritic.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Q-Learning Intro</title><link href="https://ablearn.io/reinforcement%20learning/2022/01/03/QLearning.html" rel="alternate" type="text/html" title="Q-Learning Intro" /><published>2022-01-03T00:00:00-06:00</published><updated>2022-01-03T00:00:00-06:00</updated><id>https://ablearn.io/reinforcement%20learning/2022/01/03/QLearning</id><content type="html" xml:base="https://ablearn.io/reinforcement%20learning/2022/01/03/QLearning.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2022-01-03-QLearning.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Q-Learning&quot;&gt;Q-Learning&lt;a class=&quot;anchor-link&quot; href=&quot;#Q-Learning&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;The purpose of this blog post is to learn Q learning using code using the theory from the last blog post.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You can find the previous one here.  &lt;a href=&quot;https://ablearn.io/machine%20learning/2021/08/02/RL.html&quot;&gt;Reinforcement Learning - Basics&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We will use openAI gym to apply Q learning for a FrozenLakeNoSlip environment I will explain the important parts of the code but the full code can be downloaded &lt;a href=&quot;https://colab.research.google.com/drive/1PbJnJonr8VWaOMnbosgxHbDYX4kwntMp?usp=sharing&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;em&gt;Initialize the OPENAI GYM. Print the action and states (observation space) just to get an idea as to how many actions and states we are dealing with&lt;/em&gt;&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;try&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;register&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;id&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;FrozenLakeNoSlip-v0&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;entry_point&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;gym.envs.toy_text:FrozenLakeEnv&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;kwargs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;map_name&amp;#39;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;4x4&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;is_slippery&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;max_episode_steps&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;reward_threshold&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.78&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# optimum = .8196&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;except&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;pass&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;env_name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;FrozenLakeNoSlip-v0&amp;quot;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;env&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gym&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;make&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;env_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;Observation space:&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;observation_space&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;Action space:&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;action_space&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;sd&quot;&gt;&amp;quot;&amp;quot;&amp;quot; Output&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;#Observation space: Discrete(16)&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;#Action space: Discrete(4)&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;hr /&gt;
&lt;h2 id=&quot;Initialize&quot;&gt;Initialize&lt;a class=&quot;anchor-link&quot; href=&quot;#Initialize&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Initialize parameters such as &quot;discount factor&quot;, &quot;learning rate&quot; and &quot;Q table&quot;&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Create a class&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;QAgent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;fm&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;discount_rate&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.97&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
          &lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;fm&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;observation_space&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# init state size&lt;/span&gt;
            &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eps&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;
            &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;discount_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;discount_rate&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;#  discount factor&lt;/span&gt;
            &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;#  learning rate&lt;/span&gt;
            &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;build_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;     &lt;span class=&quot;c1&quot;&gt;# Init Q-Table. Just random values for begin with      &lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;build_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# random Q-Table with rows(state size) X columns (action size)&lt;/span&gt;
         &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q_table&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1e-4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;action_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;hr /&gt;
&lt;h2 id=&quot;Choosing-Action&quot;&gt;Choosing Action&lt;a class=&quot;anchor-link&quot; href=&quot;#Choosing-Action&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;get_action&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# get the Q-value for a state from the &amp;quot;Q table&amp;quot;. You will get 4 possible values , since there 4 actions.  (q1, q2, q3, q4)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;q_state&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q_table&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;  

&lt;span class=&quot;c1&quot;&gt;# get the max of the  (q1, q2, q3, q4). That represents the &amp;quot;BEST ACTION&amp;quot;. Its called &amp;quot;greedy approach&amp;quot; since we are always trying to priortize &amp;quot;MAX&amp;quot;. however there is a inherent problem with choosing &amp;quot;MAX&amp;quot;. See below explanation. &lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;action_greedy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;argmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q_state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Choose &amp;quot;random&amp;quot; action based on a arbritary criteria (epsilon. you can call it anything. its totally arbritary. we are just trying to avoid max all the time. Thats all)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# If the random.random() &amp;lt; epsilon, choose random, else choose greedy&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;action_random&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_action&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;action_random&lt;/span&gt;  &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eps&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;action_greedy&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;hr /&gt;
&lt;h2 id=&quot;TR-Update&quot;&gt;TR Update&lt;a class=&quot;anchor-link&quot; href=&quot;#TR-Update&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;This is the main function which updates the Q table based on the bellman equation.&lt;/p&gt;
&lt;p&gt;From the last blog post&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Q(s,a) =  Q(s,a) + $ \alpha $ [$ R(s,a,s^1)  + \gamma  max_{a'} Q(s',a')$ - Q(s,a)]&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;One can read this as&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Qvalue = Current Qvalue +  learning rate (expected future reward - current Q value)&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Qvalue = Current Qvalue +  learning rate (BELLMAN ERROR)&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;experience&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;action&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;next_state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;done&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;experience&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;q_next&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q_table&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;next_state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;q_next&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;action_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;done&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q_next&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;#sample = [R + Discount * max(q values)] &lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reward&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;discount_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q_next&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 

     &lt;span class=&quot;c1&quot;&gt;# our update function for q(s,a) = q(s,a) + learning rate(bellman error)&lt;/span&gt;
                                                             &lt;span class=&quot;c1&quot;&gt;# bellman error : sample - q(s,a))&lt;/span&gt;
    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q_table&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;action&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;  &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q_table&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;action&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q_table&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;action&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;done&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eps&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eps&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.99&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;hr /&gt;
&lt;h2 id=&quot;Bringing-it-all-together&quot;&gt;Bringing it all together&lt;a class=&quot;anchor-link&quot; href=&quot;#Bringing-it-all-together&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;total_reward&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ep&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# no of episodes&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# reset, start from the beginning&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;done&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;False&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;done&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;action&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;agent&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_action&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;   &lt;span class=&quot;c1&quot;&gt;# pass the &amp;quot;current state&amp;quot; and get a &amp;quot;random&amp;quot; or &amp;quot;greedy action&amp;quot;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;next_state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;done&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;info&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;action&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# step through the env. the openaigym will do the rest. it will give you the reward, tell you if its complete and give other meta info&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;agent&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;action&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;next_state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;done&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;next_state&lt;/span&gt;        &lt;span class=&quot;c1&quot;&gt;# set the current state to the next state&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;total_reward&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reward&lt;/span&gt;   &lt;span class=&quot;c1&quot;&gt;# add the rewards&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;s:&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;a:&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;action&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;Episode: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;, Total reward: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;, eps: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ep&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;total_reward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;agent&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eps&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;render&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;agent&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q_table&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sleep&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.05&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;clear_output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;wait&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Limitations-of-Q-Learning&quot;&gt;Limitations of Q Learning&lt;a class=&quot;anchor-link&quot; href=&quot;#Limitations-of-Q-Learning&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;Q learning is good where the finite number of states and actions AND there are relatively small.  If there are too many states and actions, the number of possibilities are very high and there wont be sufficient memory to hold the table. But the more important concern is that some problems cannot be easily expressed as distinct states and actions.&lt;/p&gt;
&lt;p&gt;In the example from FROZEN LAKE, the states were SAFE, FROZEN, HOLE, GOAL. The actions were up, down, left and right.&lt;/p&gt;
&lt;p&gt;Lets say in the game pac-man such as these, the states are very &quot;similar&quot; and yet by using a q table it will be shown as 2 different states, and hence we can conclude that it cannot be easily translated. Here state is not a &quot;single unique distinct&quot; entity rather it is more characterized as a &quot;collection&quot; of features.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://abhisheksreesaila.github.io/blog/images/rl/pac1.png&quot; alt=&quot;&quot; title=&quot;Pacman State 1&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://abhisheksreesaila.github.io/blog/images/rl/pac2.png&quot; alt=&quot;&quot; title=&quot;Pacman State 2&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Useful way to think about it a set of odd numbers.  S={1,3,5}  S1={7,9,11}. Both represent odd numbers. They are similar and represented as a collection. Both &quot;S&quot; and &quot;S1&quot; are states represented by &quot;ODD&quot; numbers but they contain different values in them. Just like the pac-man example, we need a &quot;same/similar&quot; action to be performed on them but looking at them individually value by value we fail to capture the &quot;pattern&quot; or the &quot;essence&quot;&lt;/p&gt;
&lt;p&gt;The point here is we need to represent &quot;state&quot; as a collection and capture the essence rather than treating it as a single distinct entity. It is in cases like this, we take the help of a function which take in a bunch of features, &quot;understand their essence/patterns&quot; and spits out a Q-values for each action. See below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://abhisheksreesaila.github.io/blog/images/rl/univeral_function_approx.png&quot; alt=&quot;&quot; title=&quot;Univeral Function Approximator&quot; /&gt;&lt;/p&gt;
&lt;p&gt;The best known universal function approximators today are none other than &quot;neural networks&quot;. We can employ neural networks to learn the Q-values for different states. Neural Networks need some basic components to work effectively.
Lets see what they are and how we get can them.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Input&lt;/strong&gt; : Bunch of features&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Output&lt;/strong&gt; : Q-Values for each action (of course, like before, we will chose the one with the highest value)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Loss Function&lt;/strong&gt; : We can take the Bellman Error and Minimize them. However, the loss function we chose has to be &quot;differentiable&quot;. In other words, we should be apply backpropogation to transfer the loss from the &quot;output layer&quot; all the way back to the first hidden layer to update the &quot;&lt;strong&gt;weights or parameters $ \phi $&lt;/strong&gt;&quot; in the network. Lets see how we can convert &quot;bellman error&quot; to a differentiable function after which we are set to use the &quot;neural networks&quot; in our world of &quot;Q learning&quot;.
&lt;div class=&quot;flash&quot;&gt;
    &lt;svg class=&quot;octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info&quot; viewBox=&quot;0 0 14 16&quot; version=&quot;1.1&quot; width=&quot;14&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M6.3 5.69a.942.942 0 01-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 01-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;
    &lt;strong&gt;Note: &lt;/strong&gt;Since we combined &quot;deep learning&quot; with &quot;Q learning&quot; - we call it. &quot;Deep Q Learning&quot;
&lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Our incremental update formula is&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;$ Q(s,a,\phi) $ =  $ Q(s,a, \phi) $ + $ \alpha $ [$ R(s,a,s^1)  + \gamma  max_{a'} Q(s',a', \phi)$ - $ Q(s,a, \phi)] $&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Consider y = $ R(s,a,s^1)  + \gamma  max_{a'} Q(s',a', \phi)$&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;$Q(s,a, \phi)$ =  $ Q(s,a, \phi) $  - $ \alpha [Q(s,a, \phi) - y]$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Just like the mean squared loss, we can square the bellman error to punish the large variances.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;$ Q(s,a, \phi)$ =  $ Q(s,a, \phi) $ - $ \alpha [Q(s,a, \phi) - y]^2 $&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In the backward pass the gradient update of the parameters $ \phi $ of the Q function then looks like this, which can be optimized via stochastic gradient descent. Lets apply differential function w.r.t to $ \phi $ (parameters) to both sides of the equation&lt;/p&gt;
&lt;p&gt;$ \phi  = \phi - \alpha \frac{\partial (Q(s,a) - y)^2}{\partial \phi} $&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Problems-with-DQN&quot;&gt;Problems with DQN&lt;a class=&quot;anchor-link&quot; href=&quot;#Problems-with-DQN&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Corelated samples&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Unlike the SGD, the samples chosen during &quot;Q learning&quot; are close to each other and can overfit and does not converge to the global minima&lt;/li&gt;
&lt;li&gt;Solution : maintain a queue of past experiences;  use them to update the gradients. Breaks the close corelations&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Moving Targets&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Use 2 NN&lt;/li&gt;
&lt;li&gt;Compute the labels using the TARGET Network. fix it for a while&lt;/li&gt;
&lt;li&gt;Q Network will work as normal&lt;/li&gt;
&lt;li&gt;Stablizes the training for a certain duration&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Mentioned in the paper Mink et al.  &lt;/li&gt;
&lt;li&gt;shown to solve a lot of problems&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;References&quot;&gt;References&lt;a class=&quot;anchor-link&quot; href=&quot;#References&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=RaIcFiNqP-0&amp;amp;list=PLYgyoWurxA_8ePNUuTLDtMvzyf-YW7im2&amp;amp;index=10&quot;&gt;Target Networks&lt;/a&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://ablearn.io/images/post-thumbnails/ql.png" /><media:content medium="image" url="https://ablearn.io/images/post-thumbnails/ql.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Calibrating Stereo &amp;amp; Epipolar Geometry</title><link href="https://ablearn.io/computer%20vision/2021/12/08/StereoVision-2.html" rel="alternate" type="text/html" title="Calibrating Stereo &amp; Epipolar Geometry" /><published>2021-12-08T00:00:00-06:00</published><updated>2021-12-08T00:00:00-06:00</updated><id>https://ablearn.io/computer%20vision/2021/12/08/StereoVision-2</id><content type="html" xml:base="https://ablearn.io/computer%20vision/2021/12/08/StereoVision-2.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2021-12-08-StereoVision-2.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Calibration-of-the-Stereo&quot;&gt;Calibration of the Stereo&lt;a class=&quot;anchor-link&quot; href=&quot;#Calibration-of-the-Stereo&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;In the section above we assumed the stereo is calibrated that means we know the how they are aligned with respect to each other.&lt;/p&gt;
&lt;p&gt;Suppose we take a photo of effiel tower on a iphone. Then another person take a same photo with a slight different angle from samsung android phone. Is it possible to compute Z depth information and hence reocover the 3d structure of the image?  The answer turns out to be yes.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://abhisheksreesaila.github.io/blog/images/stereo/Uncalibrated-stereo.png&quot; alt=&quot;&quot; title=&quot;Uncalibrated Stereo&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Every digital camera embeds certain metadata within the image such as the focal length etc. which can be read as internal parameters. All we need to compute are the external parameters.&lt;/p&gt;
&lt;p&gt;In practice if there are 2 camera taking a shot at the same picture at 2 different angles, if we know the internal parameters of each camera, then we can calculate the alignment ourselves and hence compute the depth.  that is what we will explore in this section&lt;/p&gt;
&lt;p&gt;Consider the above picture.  It is identical to the one in the earlier section except that left and right cameras have their own coordinate system $(x_l,y_l, z_l)$ and $ (x_r, y_r, z_r)$ respectively.&lt;/p&gt;
&lt;p&gt;Our goal is to compute the &quot;translation&quot; and &quot;rotation&quot; of one camera w.r.t the other.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Epipolar-Geometry&quot;&gt;Epipolar Geometry&lt;a class=&quot;anchor-link&quot; href=&quot;#Epipolar-Geometry&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;https://abhisheksreesaila.github.io/blog/images/stereo/epipolar_geo.png&quot; alt=&quot;&quot; title=&quot;Epipolar Plane&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The highlighted triangle is &quot;Epipolar Plane&quot;. Its the plane formed by the scene point (P) and camera origins $ o_l $  and $ o_r $ is called epipolar plane&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;$e_l$ and $e_r$ are the projection of camera's origin on the left and right image planes respectively.  They are also called epipoles&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Every scene point will have it own epipolar plane.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;Now-why-do-we-care-about-epipolar-geometry?&quot;&gt;Now why do we care about epipolar geometry?&lt;a class=&quot;anchor-link&quot; href=&quot;#Now-why-do-we-care-about-epipolar-geometry?&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;blockquote&gt;&lt;p&gt;Our goal is to find a equation such that we can calculate t, R (translation, Rotation)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&quot;https://abhisheksreesaila.github.io/blog/images/stereo/epipolar_cons.png&quot; alt=&quot;&quot; title=&quot;Epipolar Constraint&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Epipolar-Constraint&quot;&gt;Epipolar Constraint&lt;a class=&quot;anchor-link&quot; href=&quot;#Epipolar-Constraint&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Consider a vector perpendicular to $X_l$ (highlighted in pink). Lets call it N&lt;/p&gt;
&lt;p&gt;From linear algebra,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;N = Cross Product between t and $X_l$&lt;/li&gt;
&lt;li&gt;N = t X $X_l$....(1)
Also, &lt;ul&gt;
&lt;li&gt;$X_l$ * N = 0 (dot product of N and $X_l$ is 0).....(2)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Hence from (1) and (2)&lt;/p&gt;
&lt;p&gt;(t X $X_l$) * $X_l$ = 0&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;This is the epipolar constraint.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;$X_l$ is a vector composed of elements $(x_l, y_l, z_l)$ and $x_l = R x_r + t$ (from the perspective projection)
Where t = position of right camera w.r.t to left; R = orientation of right camera w.r.t to left. At the end you will end up with&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;$X_l$ E $X_r$ = 0   ...(1)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;E is a 3x3 matrix called the Essential Matrix&lt;/p&gt;
&lt;p&gt;But we notice $X_l$ and $X_r$ stil exists! Our goal is to find these values.  So using perspective projection,&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$ u = f_x * x_l/z_l + O_x $  ;   $ v = f_y * y_l/z_l + O_y $  Where $ f_x and f_x $ are focal lengths measured in pixels&lt;/p&gt;
&lt;p&gt;Substituting for $x_l$ in equation (1) and expressing in matrix form, we get rid of $x_l$  and $y_l$. but $z_l$ remains!  But $z_l$ can never be 0, since it the depth. In common man terms, the world exists infront of the camera, so world coordinate will have some value of &quot;z&quot;, hence z &amp;lt;&amp;gt; 0. Using these concepts we arrive at&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;$U_l  K^{-1}_l E K^{-1}_r U_r$ = 0&lt;/p&gt;
&lt;p&gt;$U_l$ F $U_r$ = 0&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;where $U_l$ =  $[u_l, v_l, 1]$&lt;/p&gt;
&lt;p&gt;and $U_r$ = $$ \begin{bmatrix} u_r \\  v_r \\  1 \end{bmatrix} $$&lt;/p&gt;
&lt;p&gt;Where F is called fundamental matrix. I have intentionally skipped the math but for those mathematically inclined check out explanation &lt;a href=&quot;https://www.youtube.com/watch?v=6kpBqfgSPRc&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;How-does-this-work-in-practice?&quot;&gt;How does this work in practice?&lt;a class=&quot;anchor-link&quot; href=&quot;#How-does-this-work-in-practice?&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;Suppose we are given the &quot;F&quot; matrix, we can easily get &quot;E&quot; since we &quot;K&quot; is given to us&lt;/li&gt;
&lt;/ol&gt;
&lt;ol&gt;
&lt;li&gt;Once you get E from step 1, then a technique called &quot;&lt;a href=&quot;https://keisan.casio.com/exec/system/15076953160460&quot;&gt;singular value decomposition&lt;/a&gt;&quot; we can decompose it into &quot;t&quot; and &quot;R&quot;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;Finding-correspondence&quot;&gt;Finding correspondence&lt;a class=&quot;anchor-link&quot; href=&quot;#Finding-correspondence&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;In the previous sectionn, we said given a point $u_l, v_l$ finding a matching point $u_r v_r$ is a 1D search problem i.e. we have to search only in 1 direction, horizontally. But wait! where?  Can epipolar geometry help in the telling me the section the image to search?&lt;/p&gt;
&lt;p&gt;Fortunately the answer is yes! There is only other component of EPIPOLAR geometry to the rescue! Epipolar line.&lt;/p&gt;
&lt;p&gt;Imagine looking at the second camera origin, all the points on the $X_l$ will fall on the image plane as shown in red. also, this red line will intersect with the epipolar plane. This intersection is the epipolar line. In other words, The projection of all the points on the vector $X_l$ will lie on a line called EPIPOLAR line.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://abhisheksreesaila.github.io/blog/images/stereo/epipolar-line.png&quot; alt=&quot;&quot; title=&quot;Epipolar line&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://abhisheksreesaila.github.io/blog/images/stereo/epipolarline2.png [source](https://en.wikipedia.org/wiki/Epipolar_geometry&quot; alt=&quot;&quot; /&gt;)&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://abhisheksreesaila.github.io/blog/images/stereo/epipolar-formation.gif&quot; alt=&quot;&quot; title=&quot;Epipolar line Formation- Animated&quot; /&gt;&lt;/p&gt;
&lt;p&gt;From the last section, we have&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;$U_l$ F $U_r$ = 0&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Expanding..&lt;/p&gt;
$$


\begin{bmatrix} u_{l} \\  v_{l} \\  1  \end{bmatrix}

\begin{bmatrix} f_{11} &amp;amp; f_{12} &amp;amp; f_{13} \\  f_{21} &amp;amp; f_{22} &amp;amp; f_{23}  \\  f_{31} &amp;amp; f_{32} &amp;amp; f_{33}  \end{bmatrix}

\begin{bmatrix} u_r \\  v_r \\  1 \end{bmatrix}

=  0

$$&lt;p&gt;Multiplying...&lt;/p&gt;
&lt;p&gt;$(f_{11}u_l +f_{12}v_l + f_{31})u_r + (f_{21}u_l +f_{22}v_l + f_{32})v_r + (f_{13}u_l +f_{23}v_l + f_{33})= 0 $&lt;/p&gt;
&lt;p&gt;Simplified to...&lt;/p&gt;
&lt;p&gt;$Au_r + Bv_r + C = 0$ is a simple linear equation of the line that has all the projection points of $X_l$&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;This is the equation for the epipolar line&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;Summary&quot;&gt;Summary&lt;a class=&quot;anchor-link&quot; href=&quot;#Summary&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;Assume camera matrix K is known&lt;/li&gt;
&lt;li&gt;Find a few correspondence between 2 images (using SIFT, ORB or by hand). A minimum of 8 corresponding points are sufficient. &lt;/li&gt;
&lt;li&gt;Using points from step #2, find the t and R matrices. At this point it is said to be calibrated. &lt;/li&gt;
&lt;li&gt;Now that cameras are calibrated, for every point in one image we can find a corresponding point. Turns out it is a 1D search problem along the epipolar line.&lt;/li&gt;
&lt;li&gt;Compute depth by Traingulation. &lt;/li&gt;
&lt;/ol&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;References&quot;&gt;References&lt;a class=&quot;anchor-link&quot; href=&quot;#References&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=hUVyDabn1Mg&quot;&gt;Stereo Vision&lt;/a&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://ablearn.io/images/post-thumbnails/Epipolar_Geometry.png" /><media:content medium="image" url="https://ablearn.io/images/post-thumbnails/Epipolar_Geometry.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Depth Estimation and Object Detection</title><link href="https://ablearn.io/computer%20vision/2021/12/08/StereoVisionCode.html" rel="alternate" type="text/html" title="Depth Estimation and Object Detection" /><published>2021-12-08T00:00:00-06:00</published><updated>2021-12-08T00:00:00-06:00</updated><id>https://ablearn.io/computer%20vision/2021/12/08/StereoVisionCode</id><content type="html" xml:base="https://ablearn.io/computer%20vision/2021/12/08/StereoVisionCode.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2021-12-08-StereoVisionCode.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Purpose&quot;&gt;Purpose&lt;a class=&quot;anchor-link&quot; href=&quot;#Purpose&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;Use stereo vision to find out the depth of an object(s) in an image. Specifically, we will find out how far cars &amp;amp; people are from the camera&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Dataset&lt;/strong&gt; &lt;a href=&quot;http://www.cvlibs.net/datasets/kitti/&quot;&gt;KITTI Dataset&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Output&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://abhisheksreesaila.github.io/blog/images/stereo/DepthOfCar.png&quot; alt=&quot;&quot; title=&quot;Depth of Objects&quot; /&gt;

&lt;center&gt;
    &lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/ewBLt1lZ2Ik&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;
&lt;/p&gt;
&lt;h2 id=&quot;Outline-of-the-implementation&quot;&gt;Outline of the implementation&lt;a class=&quot;anchor-link&quot; href=&quot;#Outline-of-the-implementation&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;The code is available here&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Load Left and Right Images from &lt;a href=&quot;http://www.cvlibs.net/datasets/kitti/&quot;&gt;KITTI Dataset&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Compute &lt;strong&gt;Disparity&lt;/strong&gt;. Refer &lt;a href=&quot;https://ablearn.io/computer%20vision/2021/12/07/StereoVision.html#Finding-Depth&quot;&gt;here&lt;/a&gt; for an defintion and explanation&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Apply Stereo SGBM Matcher algorithm and computer Disparity&lt;/li&gt;
&lt;li&gt;We wil obtain Disparity Map&lt;/li&gt;
&lt;li&gt;Each pixel in the Disparity gives the Disparity value&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Read calibration parameters&lt;ul&gt;
&lt;li&gt;P0 and P1 are projection matrix for gray scale&lt;/li&gt;
&lt;li&gt;P1 and P2 are projection matrix for color scale&lt;/li&gt;
&lt;li&gt;R0_rect = rotation matrix&lt;/li&gt;
&lt;li&gt;Tr_velo_to_cam &amp;amp; Tr_imu_to_velo are translation matrices&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Compute the &lt;strong&gt;Depth Map&lt;/strong&gt; (The depth map is a map that contains Z for each pixel) Check out the explanation &lt;a href=&quot;https://ablearn.io/computer%20vision/2021/12/07/StereoVision.html#Finding-Depth&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;From the calibration parameters we get the projection matrix. Decompose them to get Rotation (R), K (Camera) and T (translation matrix) through a process called &lt;strong&gt;QR Factorization&lt;/strong&gt; Check out the explanation &lt;a href=&quot;https://ablearn.io/computer%20vision/2021/12/07/CameraCalibration.html#Decomposing-Projection-Matrix&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Use YOLO Object Detector to detect cars&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;You get bounding boxes coordinates. We need a point that represent the object (not 4 coordinates of the bounding boxes).  So, we get the &quot;center&quot; of the bounding box which effectively represents in the object.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Build the pipeline and run it on image&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;Run the pipeline on the video&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;Bonus-Section&quot;&gt;Bonus Section&lt;a class=&quot;anchor-link&quot; href=&quot;#Bonus-Section&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Given the following&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;disparity map&lt;/li&gt;
&lt;li&gt;camera matrices&lt;/li&gt;
&lt;li&gt;baseline&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Use &lt;strong&gt;cv2.stereoRectify&lt;/strong&gt;. to get a perspective transformation matrix (or Q matrix)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Use the Q Matrix and &lt;strong&gt;cv2.reprojectImageTo3D&lt;/strong&gt; to get a points in 3D space (camera)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Use library such as &quot;open3d&quot; to visualize&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;https://abhisheksreesaila.github.io/blog/images/stereo/3d-reconstruction.png&quot; alt=&quot;&quot; title=&quot;3D reconstruction&quot; /&gt;&lt;/p&gt;
&lt;p&gt;see opencv documentation &lt;a href=&quot;https://docs.opencv.org/3.4/d9/d0c/group__calib3d.html#ga1bc1152bd57d63bc524204f21fde6e02&quot;&gt;here&lt;/a&gt; for explanation&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;References&quot;&gt;References&lt;a class=&quot;anchor-link&quot; href=&quot;#References&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;&lt;a href=&quot;https://courses.thinkautonomous.ai/stereo-vision&quot;&gt;Think Autonomous Course&lt;/a&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://ablearn.io/images/post-thumbnails/DepthOfCar.png" /><media:content medium="image" url="https://ablearn.io/images/post-thumbnails/DepthOfCar.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Stereo Vision Intro</title><link href="https://ablearn.io/computer%20vision/2021/12/07/StereoVision.html" rel="alternate" type="text/html" title="Stereo Vision Intro" /><published>2021-12-07T00:00:00-06:00</published><updated>2021-12-07T00:00:00-06:00</updated><id>https://ablearn.io/computer%20vision/2021/12/07/StereoVision</id><content type="html" xml:base="https://ablearn.io/computer%20vision/2021/12/07/StereoVision.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2021-12-07-StereoVision.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Purpose&quot;&gt;Purpose&lt;a class=&quot;anchor-link&quot; href=&quot;#Purpose&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;In this multi-part blog post, we try to understand&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Why we need stereo in computer vision?&lt;/li&gt;
&lt;li&gt;Given any stereo cameras, how can we calibrate and use them to find depth?&lt;/li&gt;
&lt;li&gt;Summary of steps&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;Why-do-we-need-Stereo?&quot;&gt;Why do we need Stereo?&lt;a class=&quot;anchor-link&quot; href=&quot;#Why-do-we-need-Stereo?&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;h2 id=&quot;The-loss-of-depth&quot;&gt;The loss of depth&lt;a class=&quot;anchor-link&quot; href=&quot;#The-loss-of-depth&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Given a point in the image plane (U, V), can we find a corresponding point in the world system? The answer is NO.  When we move capture a image we know that 3d world coordinate gets transformed to camera cooridinate and then a 2D image plane. Refer to this blog post for indepth details. Since we loose information, more specifically &quot;Z&quot; depth information, it is impossible to get it back.  In other words, given an image point you cannot reverse engineer WORLD coordinate since you have lost a crucial &quot;Z&quot; coordinate in the translation process. however, you havent lost &quot;X&quot;, &quot;Y&quot;.  There is hope to get it back, but we need additional help.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://abhisheksreesaila.github.io/blog/images/stereo/monocular2.png&quot; alt=&quot;&quot; title=&quot;Loss of Depth&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Since we know &quot;X&quot;, &quot;Y&quot;, we know it exists somewhere along the line shown as dotted green lines. Why? This was the same line which was used in the projection of object onto the image plane to derive all the math. So, it can also assist in the reverse direction as well.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://abhisheksreesaila.github.io/blog/images/stereo/projection2.png&quot; alt=&quot;&quot; title=&quot;Projection of the object on image plane&quot; /&gt;&lt;/p&gt;
&lt;h3 id=&quot;How-to-recover?&quot;&gt;How to recover?&lt;a class=&quot;anchor-link&quot; href=&quot;#How-to-recover?&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;The trick is to understand how nature does this. We all have 2 eyes, and we perceive depth (3d) in all objects. Don't we? The &quot;second eye&quot; provides its view of the world  in addition to the first, and both together work together perceive depth.&lt;/p&gt;
&lt;p&gt;Lets apply the same concept here. lets bring in another camera, place it horizontally along the same axis at a distance, find the exact same spot (U, V) on it, guess the point on the &quot;dotted green line&quot;.  The intersection of these 2 dotted green lines gives you the depth Z.  See below for the visual&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://abhisheksreesaila.github.io/blog/images/stereo/stereo.png&quot; alt=&quot;&quot; title=&quot;Stereo Vision&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$ u_r v_r $ and  $ u_l v_l $ are the exact same point of the image as seen in the right and left camera respectively&lt;/li&gt;
&lt;li&gt;the distance between the cameras is called baseline denoted by b&lt;/li&gt;
&lt;li&gt;the camera plane is placed at the pinhole with origin (0,0,0) on the left and (b,0,0) on the right&lt;/li&gt;
&lt;li&gt;P (x, y, z) is the scene point that we are trying to compute using the 2 cameras.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This concept of using 2 cameras to perceive depth in the real world is called Simple Stereo Vision.  In this blog post lets understand the mechanics of such a system.&lt;/p&gt;
&lt;h2 id=&quot;Finding-Depth&quot;&gt;Finding Depth&lt;a class=&quot;anchor-link&quot; href=&quot;#Finding-Depth&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Lets start with the basics. For a pinhole based camera system we know the following equations&lt;/p&gt;
&lt;p&gt;$ u_l = f_x * x/z + O_x $  ;   $ v_l = f_y * y/z  + O_y $&lt;/p&gt;
&lt;p&gt;For the right camera, its the same but the camera axis is shifted by &quot;b&quot;&lt;/p&gt;
&lt;p&gt;$ u_r = f_x * x/z + O_x $  ;   $ v_r = f_y * y/z  + O_y $&lt;/p&gt;
&lt;p&gt;Using the 4 equations, solving for x, y, z  we get&lt;/p&gt;
&lt;p&gt;$ x = \frac {b (u_l - O_x)}{ u_l - u_r } $
$ y = \frac {b f_x (v_l - O_y)}{f_y (u_l - u_r)} $&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;$ z = \frac {b f_x}{ u_l - u_r } $&lt;/strong&gt;
&lt;div class=&quot;flash&quot;&gt;
    &lt;svg class=&quot;octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info&quot; viewBox=&quot;0 0 14 16&quot; version=&quot;1.1&quot; width=&quot;14&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M6.3 5.69a.942.942 0 01-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 01-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;
    &lt;strong&gt;Note: &lt;/strong&gt;$ u_l - u_r $ is called Disparity and its inversely propotional to &quot;z&quot;. 
&lt;/div&gt;&lt;div class=&quot;flash flash-warn&quot;&gt;
    &lt;svg class=&quot;octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap&quot; viewBox=&quot;0 0 10 16&quot; version=&quot;1.1&quot; width=&quot;10&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M10 7H6l3-7-9 9h4l-3 7 9-9z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;
    &lt;strong&gt;Important: &lt;/strong&gt;If we know the internal parameters fx, fy, ox, oy and compute disparity, we compute Z and hence the depth. 
&lt;/div&gt;
If the object is closer to the camera, you will see a large disparity. for example, U value in the left camera will be 100, whereas the right camera it will be 75.  This is exact same pixel in the image but having 2 different values. The opposite is also true i.e the object is far, there will be very less difference between the U values (Say 100 and 95).  At infinite distance, U values will exactly be the same.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Disparity is propotional to baseline meaning if the distance between camera increase, disparity will increase.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I keep mentioning only &quot;U&quot; because there is no $v_l - v_r $ in the equation. which means only the horizontal component between the 2 cameras vary not the &quot;vertical component&quot;.  This proporty shows that  $ u_r, v_r $ and $ u_l $ and $ v_l $ lie along the same line (show by the yellow line). when we are computing DISPARITY to solve for X, Y, Z  in the real world, we can pick a point in the left camera $ (u_l, v_l) $ and  ONLY search along the same line in the right camera (and not wander aimlessly and search the whole image) to get the $ u_r, v_r $ .i.e  its a &quot;1D&quot; search problem. See image below for an example. This is often called the &lt;strong&gt;&quot;correspondence problem&quot;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://abhisheksreesaila.github.io/blog/images/stereo/texture.png&quot; alt=&quot;&quot; title=&quot;Finding Correspondence&quot; /&gt;
The white patch in the picture is called the &quot;scan line&quot;.&lt;/p&gt;
&lt;h2 id=&quot;Problems-with-stereo-matching&quot;&gt;Problems with stereo matching&lt;a class=&quot;anchor-link&quot; href=&quot;#Problems-with-stereo-matching&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Did we solve of finding depth just by solving 2 cameras? yes for the most part, but if the images have the repetitive texture, its impossible to compute disparity and therefore can't compute depth.  see image below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://abhisheksreesaila.github.io/blog/images/stereo/no-texture.png&quot; alt=&quot;&quot; title=&quot;Stereo Vision CANNOT be computed&quot; /&gt;&lt;/p&gt;
&lt;h1 id=&quot;Next-Steps&quot;&gt;Next Steps&lt;a class=&quot;anchor-link&quot; href=&quot;#Next-Steps&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;In the next &lt;a href=&quot;https://ablearn.io/computer%20vision/2021/12/08/StereoVision-2.html&quot;&gt;part&lt;/a&gt; we will understand how to calibrate stereo cameras and use them to compute depth&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;References&quot;&gt;References&lt;a class=&quot;anchor-link&quot; href=&quot;#References&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=hUVyDabn1Mg&quot;&gt;Stereo Vision&lt;/a&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://ablearn.io/images/post-thumbnails/sv2.png" /><media:content medium="image" url="https://ablearn.io/images/post-thumbnails/sv2.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>