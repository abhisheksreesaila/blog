<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.0">Jekyll</generator><link href="https://ablearn.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://ablearn.io/" rel="alternate" type="text/html" /><updated>2022-10-09T07:07:28-05:00</updated><id>https://ablearn.io/feed.xml</id><title type="html">ABLearn</title><subtitle>Always Learning</subtitle><entry><title type="html">comma ai - self driving for the masses</title><link href="https://ablearn.io/self%20driving/2022/10/01/comma.html" rel="alternate" type="text/html" title="comma ai - self driving for the masses" /><published>2022-10-01T00:00:00-05:00</published><updated>2022-10-01T00:00:00-05:00</updated><id>https://ablearn.io/self%20driving/2022/10/01/comma</id><content type="html" xml:base="https://ablearn.io/self%20driving/2022/10/01/comma.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2022-10-01-comma.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Introduction&quot;&gt;Introduction&lt;a class=&quot;anchor-link&quot; href=&quot;#Introduction&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;This is a multi blog posts about the openpilot software developed by comma ai.  I wil try to deconstruct as much as i can and along the way tell you why it could be the future of self driving.&lt;/p&gt;
&lt;h1 id=&quot;Principle&quot;&gt;Principle&lt;a class=&quot;anchor-link&quot; href=&quot;#Principle&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;Every self driving car (except TESLA) will tell you the reason why they want to develop autonomous cars is to make driving safer and humans are bad drivers. Humans crash cars! Humans are distracted!. Yes all of this is absolutely true! but for 1-5% of the population. the other 95% are good drivers and been doing so for the last 100 years!  So, there is nothing wrong with the system. We only need to is to make our commute better like less annoying (better GPS?), less fatigue (better cruise control?) and a good music system. What comma ai is focusing (and every other company should focus) is a reduction of fatigue and making driving chill! thats it.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;So, how do we do that? write a software that copies the best driving agent in the world - humans! &lt;/strong&gt;&lt;/p&gt;
&lt;h1 id=&quot;Definitions&quot;&gt;Definitions&lt;a class=&quot;anchor-link&quot; href=&quot;#Definitions&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;Now we believe in the priniciple, lets define few car terms so that we understand&lt;/p&gt;
&lt;p&gt;ACC
LKAS
CAN&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://ablearn.io/images/post-thumbnails/comma-ai.png" /><media:content medium="image" url="https://ablearn.io/images/post-thumbnails/comma-ai.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">ML Model Optimization</title><link href="https://ablearn.io/mschine%20learning/2022/07/05/NeuralOptimization.html" rel="alternate" type="text/html" title="ML Model Optimization" /><published>2022-07-05T00:00:00-05:00</published><updated>2022-07-05T00:00:00-05:00</updated><id>https://ablearn.io/mschine%20learning/2022/07/05/NeuralOptimization</id><content type="html" xml:base="https://ablearn.io/mschine%20learning/2022/07/05/NeuralOptimization.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2022-07-05-NeuralOptimization.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Definition&quot;&gt;Definition&lt;a class=&quot;anchor-link&quot; href=&quot;#Definition&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;Top techniques&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Quantization&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;convert floats to integers &lt;/li&gt;
&lt;li&gt;slight reduction in accuracy but 2-4x gain in speed&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;code here&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Pruning&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Make the model sparse. &lt;/li&gt;
&lt;li&gt;Certain % of weights (parameters) is set to 0&lt;/li&gt;
&lt;li&gt;reduction in accuracy for gain in speed&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Code here&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Knowledge distillation&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;uses teacher and student network&lt;/li&gt;
&lt;li&gt;take the output of teacher network and use its output to run student network&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Multi task learning&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;use a single backbone but multi-heads to do variety of tasks. More on multi task learning can be found here&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;References&quot;&gt;References&lt;a class=&quot;anchor-link&quot; href=&quot;#References&quot;&gt; &lt;/a&gt;&lt;/h1&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://ablearn.io/images/post-thumbnails/ModelOptimization.jpeg" /><media:content medium="image" url="https://ablearn.io/images/post-thumbnails/ModelOptimization.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Object Tracking</title><link href="https://ablearn.io/computer%20vision/2022/06/25/ObjectTracking.html" rel="alternate" type="text/html" title="Object Tracking" /><published>2022-06-25T00:00:00-05:00</published><updated>2022-06-25T00:00:00-05:00</updated><id>https://ablearn.io/computer%20vision/2022/06/25/ObjectTracking</id><content type="html" xml:base="https://ablearn.io/computer%20vision/2022/06/25/ObjectTracking.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2022-06-25-ObjectTracking.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Definition&quot;&gt;Definition&lt;a class=&quot;anchor-link&quot; href=&quot;#Definition&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;SORT - simple online tracking tool&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;SORT is a method used to track object at real time. the output could look like below&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Tracking by detection&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;making use of object detection in the first frame&lt;/li&gt;
&lt;li&gt;use kalman filter for estimating the motion in the next frame&lt;/li&gt;
&lt;li&gt;associate bounding boxes between frames using hungarian algorithm (optimal matching/association algorithm)&lt;ul&gt;
&lt;li&gt;use IOU as cost&lt;/li&gt;
&lt;li&gt;but can use linear cost or exponential cost as well&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Code &lt;em&gt;here&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DeepSort&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Improve SORT by including &quot;visual appearance&quot; along with kalman filter for better predicability&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;References&quot;&gt;References&lt;a class=&quot;anchor-link&quot; href=&quot;#References&quot;&gt; &lt;/a&gt;&lt;/h1&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://ablearn.io/images/post-thumbnails/ObjectTracking.jpeg" /><media:content medium="image" url="https://ablearn.io/images/post-thumbnails/ObjectTracking.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Multi Task Learning</title><link href="https://ablearn.io/computer%20vision/2022/03/25/Hydranets.html" rel="alternate" type="text/html" title="Multi Task Learning" /><published>2022-03-25T00:00:00-05:00</published><updated>2022-03-25T00:00:00-05:00</updated><id>https://ablearn.io/computer%20vision/2022/03/25/Hydranets</id><content type="html" xml:base="https://ablearn.io/computer%20vision/2022/03/25/Hydranets.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2022-03-25-Hydranets.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Definition&quot;&gt;Definition&lt;a class=&quot;anchor-link&quot; href=&quot;#Definition&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Multi Task Learning&lt;/strong&gt; - Adaption of a single neural network model to multiple tasks such as segmentation, depth estimation etc.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Segmenation&lt;/strong&gt; :  Per Pixel Classification of an image to identify objects such as trees, roads, people etc.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Depth estimation&lt;/strong&gt; : Estimating the distance from the camera&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We will producing a output such as the one below.

&lt;center&gt;
    &lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/iXpnkIxrbq0&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;
&lt;/p&gt;
&lt;h1 id=&quot;Explanation&quot;&gt;Explanation&lt;a class=&quot;anchor-link&quot; href=&quot;#Explanation&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;h2 id=&quot;What-tasks-can-be-combined?&quot;&gt;What tasks can be combined?&lt;a class=&quot;anchor-link&quot; href=&quot;#What-tasks-can-be-combined?&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Based on this &lt;a href=&quot;https://arxiv.org/pdf/1905.07553.pdf&quot;&gt;paper&lt;/a&gt; the following observations were made&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Segmentation + anything = becomes better&lt;/li&gt;
&lt;li&gt;Anything + segmentation = make it worse&lt;/li&gt;
&lt;li&gt;Anything + Normals = makes it better&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;Architecture&quot;&gt;Architecture&lt;a class=&quot;anchor-link&quot; href=&quot;#Architecture&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;The 2 most recent and popular ones are&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/&quot;&gt;Google Pathways&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=IHH47nZ7FZU&amp;amp;t=248s&quot;&gt;Tesla's Hydranets&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;Our-Implementation&quot;&gt;Our Implementation&lt;a class=&quot;anchor-link&quot; href=&quot;#Our-Implementation&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;We will focus on Tesla's Hydranets in this blog post. We will recreate a modified version of the hydranets based on this &lt;a href=&quot;https://arxiv.org/pdf/1809.04766.pdf&quot;&gt;Real-Time Joint Semantic Segmentation and Depth Estimation Using
Asymmetric Annotations&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://abhisheksreesaila.github.io/blog/images/mtl/ModifiedHydranets.png&quot; alt=&quot;&quot; title=&quot;Architecture&quot; /&gt;&lt;/p&gt;
&lt;h3 id=&quot;Code-Walkthrough&quot;&gt;Code Walkthrough&lt;a class=&quot;anchor-link&quot; href=&quot;#Code-Walkthrough&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Its a encoder - decoder network. We use &lt;strong&gt;&lt;a href=&quot;https://arxiv.org/abs/1801.04381&quot;&gt;MobileNetv2&lt;/a&gt;&lt;/strong&gt; as encoder and &lt;strong&gt;&lt;a href=&quot;https://arxiv.org/pdf/1810.03272.pdf&quot;&gt;RefineNet&lt;/a&gt;&lt;/strong&gt; as decoder.&lt;/p&gt;
&lt;h4 id=&quot;Encoder&quot;&gt;Encoder&lt;a class=&quot;anchor-link&quot; href=&quot;#Encoder&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;img src=&quot;https://abhisheksreesaila.github.io/blog/images/mtl/mobilenet_v2.jpeg&quot; alt=&quot;&quot; title=&quot;MobileNetv2&quot; /&gt;&lt;/p&gt;
&lt;p&gt;There are 2 important concepts :-&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;DepthWise Separable Convolutions&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;https://abhisheksreesaila.github.io/blog/images/mtl/DepthWise.jpg&quot; alt=&quot;&quot; title=&quot;Depth Wise&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://abhisheksreesaila.github.io/blog/images/mtl/NormalConv.jpg&quot; alt=&quot;&quot; title=&quot;Standard Convolutions&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://abhisheksreesaila.github.io/blog/images/mtl/NormalConv.jpg&quot; alt=&quot;&quot; title=&quot;Standard Convolutions&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://abhisheksreesaila.github.io/blog/images/mtl/ReducedParams.jpg&quot; alt=&quot;&quot; title=&quot;Reduction in parameters&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Residual Networks &amp;amp; Inverted Residual Networks&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;https://abhisheksreesaila.github.io/blog/images/mtl/ResidualvsNonResidual.png&quot; alt=&quot;&quot; title=&quot;Residual vs NonResidual Block&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://abhisheksreesaila.github.io/blog/images/mtl/ResidualNetwork.jpg&quot; alt=&quot;&quot; title=&quot;Residual Block&quot; /&gt;&lt;/p&gt;
&lt;h4 id=&quot;Decoder&quot;&gt;Decoder&lt;a class=&quot;anchor-link&quot; href=&quot;#Decoder&quot;&gt; &lt;/a&gt;&lt;/h4&gt;&lt;p&gt;&lt;img src=&quot;https://abhisheksreesaila.github.io/blog/images/mtl/RefineNet.png&quot; alt=&quot;&quot; title=&quot;RefineNet&quot; /&gt;&lt;/p&gt;
&lt;h5 id=&quot;Highlights-of-the-network&quot;&gt;Highlights of the network&lt;a class=&quot;anchor-link&quot; href=&quot;#Highlights-of-the-network&quot;&gt; &lt;/a&gt;&lt;/h5&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;RCU - Residual Conv Unit&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Simplified version of the convolution unit in the original ResNet where the batch-normalization layers are removed&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Multi Resolution Fusion&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;All path inputs are then fused into a high-resolution feature map by the multi-resolution fusion block.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Chained Residual Pooling&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The proposed chained residual pooling aims to capture background context from a large
image region. It is able to efficiently pool features with multiple window sizes and fuse     them together using learnable weights. In particular, this component is built as a
chain of multiple pooling blocks, each consisting of one max-pooling layer and one   convolution layer. One pooling block takes the output of the previous pooling block as input. Therefore, the current pooling block is able to re-use the result from the previous pooling operation and thus access the features from a large region without using a large pooling window.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here is the &lt;a href=&quot;https://colab.research.google.com/drive/1Q8Oi37D-Qf2d5Oemo5aXuq1kPC5vePCZ?usp=sharing&quot;&gt;code&lt;/a&gt;&lt;/p&gt;
&lt;h5 id=&quot;Output&quot;&gt;Output&lt;a class=&quot;anchor-link&quot; href=&quot;#Output&quot;&gt; &lt;/a&gt;&lt;/h5&gt;&lt;p&gt;&lt;img src=&quot;https://abhisheksreesaila.github.io/blog/images/mtl/HydranetOutput.png&quot; alt=&quot;&quot; title=&quot;Output&quot; /&gt;

&lt;center&gt;
    &lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/iXpnkIxrbq0&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;
&lt;/p&gt;
&lt;h1 id=&quot;References&quot;&gt;References&lt;a class=&quot;anchor-link&quot; href=&quot;#References&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/DrSleep/multi-task-refinenet/blob/master/src/notebooks/ExpNYUDKITTI_joint.ipynb/&quot;&gt;Implementation Notebook&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1801.04381.pdf&quot;&gt;Architecture Images From This Paper&lt;/a&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://ablearn.io/images/post-thumbnails/mtl.png" /><media:content medium="image" url="https://ablearn.io/images/post-thumbnails/mtl.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Actor Critic</title><link href="https://ablearn.io/reinforcement%20learning/2022/01/10/ActorCritic.html" rel="alternate" type="text/html" title="Actor Critic" /><published>2022-01-10T00:00:00-06:00</published><updated>2022-01-10T00:00:00-06:00</updated><id>https://ablearn.io/reinforcement%20learning/2022/01/10/ActorCritic</id><content type="html" xml:base="https://ablearn.io/reinforcement%20learning/2022/01/10/ActorCritic.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2022-01-10-ActorCritic.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Actor-Critic-Reinforcement-Model&quot;&gt;Actor Critic Reinforcement Model&lt;a class=&quot;anchor-link&quot; href=&quot;#Actor-Critic-Reinforcement-Model&quot;&gt; &lt;/a&gt;&lt;/h1&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;There are 2 networks&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Actor - policy network - gives action &lt;/li&gt;
&lt;li&gt;Critic - uses action to go to next state - evaluates state - critizes the action to give the best state possible based on the (reward setup + discount factor)&lt;ul&gt;
&lt;li&gt;good actions lead to good qvalues - so critism goes down - then actor starts producing good action and qnetworks critizes keeps going down  &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Explanation coming soon...&lt;/p&gt;
&lt;p&gt;Meanwhile code &lt;a href=&quot;https://colab.research.google.com/drive/14LdUWgd0DZ7krc3NgWM0djlO1mfr2SF_?usp=sharing&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://ablearn.io/images/post-thumbnails/ActorCritic.png" /><media:content medium="image" url="https://ablearn.io/images/post-thumbnails/ActorCritic.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Q-Learning Intro</title><link href="https://ablearn.io/reinforcement%20learning/2022/01/03/QLearning.html" rel="alternate" type="text/html" title="Q-Learning Intro" /><published>2022-01-03T00:00:00-06:00</published><updated>2022-01-03T00:00:00-06:00</updated><id>https://ablearn.io/reinforcement%20learning/2022/01/03/QLearning</id><content type="html" xml:base="https://ablearn.io/reinforcement%20learning/2022/01/03/QLearning.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2022-01-03-QLearning.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Q-Learning&quot;&gt;Q-Learning&lt;a class=&quot;anchor-link&quot; href=&quot;#Q-Learning&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;The purpose of this blog post is to learn Q learning using code using the theory from the last blog post.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You can find the previous one here.  &lt;a href=&quot;https://ablearn.io/machine%20learning/2021/08/02/RL.html&quot;&gt;Reinforcement Learning - Basics&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We will use openAI gym to apply Q learning for a FrozenLakeNoSlip environment I will explain the important parts of the code but the full code can be downloaded &lt;a href=&quot;https://colab.research.google.com/drive/1PbJnJonr8VWaOMnbosgxHbDYX4kwntMp?usp=sharing&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;em&gt;Initialize the OPENAI GYM. Print the action and states (observation space) just to get an idea as to how many actions and states we are dealing with&lt;/em&gt;&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;try&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;register&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;id&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;FrozenLakeNoSlip-v0&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;entry_point&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;gym.envs.toy_text:FrozenLakeEnv&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;kwargs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;map_name&amp;#39;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;4x4&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;is_slippery&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;max_episode_steps&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;reward_threshold&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.78&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# optimum = .8196&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;except&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;pass&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;env_name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;FrozenLakeNoSlip-v0&amp;quot;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;env&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gym&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;make&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;env_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;Observation space:&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;observation_space&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;Action space:&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;action_space&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;sd&quot;&gt;&amp;quot;&amp;quot;&amp;quot; Output&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;#Observation space: Discrete(16)&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;#Action space: Discrete(4)&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;hr /&gt;
&lt;h2 id=&quot;Initialize&quot;&gt;Initialize&lt;a class=&quot;anchor-link&quot; href=&quot;#Initialize&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Initialize parameters such as &quot;discount factor&quot;, &quot;learning rate&quot; and &quot;Q table&quot;&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Create a class&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;QAgent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;fm&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;discount_rate&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.97&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
          &lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;fm&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;observation_space&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# init state size&lt;/span&gt;
            &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eps&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;
            &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;discount_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;discount_rate&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;#  discount factor&lt;/span&gt;
            &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;#  learning rate&lt;/span&gt;
            &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;build_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;     &lt;span class=&quot;c1&quot;&gt;# Init Q-Table. Just random values for begin with      &lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;build_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# random Q-Table with rows(state size) X columns (action size)&lt;/span&gt;
         &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q_table&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1e-4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;action_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;hr /&gt;
&lt;h2 id=&quot;Choosing-Action&quot;&gt;Choosing Action&lt;a class=&quot;anchor-link&quot; href=&quot;#Choosing-Action&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;get_action&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# get the Q-value for a state from the &amp;quot;Q table&amp;quot;. You will get 4 possible values , since there 4 actions.  (q1, q2, q3, q4)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;q_state&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q_table&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;  

&lt;span class=&quot;c1&quot;&gt;# get the max of the  (q1, q2, q3, q4). That represents the &amp;quot;BEST ACTION&amp;quot;. Its called &amp;quot;greedy approach&amp;quot; since we are always trying to priortize &amp;quot;MAX&amp;quot;. however there is a inherent problem with choosing &amp;quot;MAX&amp;quot;. See below explanation. &lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;action_greedy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;argmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q_state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Choose &amp;quot;random&amp;quot; action based on a arbritary criteria (epsilon. you can call it anything. its totally arbritary. we are just trying to avoid max all the time. Thats all)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# If the random.random() &amp;lt; epsilon, choose random, else choose greedy&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;action_random&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_action&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;action_random&lt;/span&gt;  &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eps&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;action_greedy&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;hr /&gt;
&lt;h2 id=&quot;TR-Update&quot;&gt;TR Update&lt;a class=&quot;anchor-link&quot; href=&quot;#TR-Update&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;This is the main function which updates the Q table based on the bellman equation.&lt;/p&gt;
&lt;p&gt;From the last blog post&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Q(s,a) =  Q(s,a) + $ \alpha $ [$ R(s,a,s^1)  + \gamma  max_{a'} Q(s',a')$ - Q(s,a)]&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;One can read this as&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Qvalue = Current Qvalue +  learning rate (expected future reward - current Q value)&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Qvalue = Current Qvalue +  learning rate (BELLMAN ERROR)&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;experience&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;action&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;next_state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;done&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;experience&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;q_next&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q_table&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;next_state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;q_next&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;action_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;done&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q_next&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;#sample = [R + Discount * max(q values)] &lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reward&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;discount_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q_next&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 

     &lt;span class=&quot;c1&quot;&gt;# our update function for q(s,a) = q(s,a) + learning rate(bellman error)&lt;/span&gt;
                                                             &lt;span class=&quot;c1&quot;&gt;# bellman error : sample - q(s,a))&lt;/span&gt;
    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q_table&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;action&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;  &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q_table&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;action&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q_table&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;action&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;done&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eps&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eps&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.99&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;hr /&gt;
&lt;h2 id=&quot;Bringing-it-all-together&quot;&gt;Bringing it all together&lt;a class=&quot;anchor-link&quot; href=&quot;#Bringing-it-all-together&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;total_reward&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ep&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# no of episodes&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# reset, start from the beginning&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;done&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;False&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;done&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;action&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;agent&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_action&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;   &lt;span class=&quot;c1&quot;&gt;# pass the &amp;quot;current state&amp;quot; and get a &amp;quot;random&amp;quot; or &amp;quot;greedy action&amp;quot;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;next_state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;done&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;info&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;action&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# step through the env. the openaigym will do the rest. it will give you the reward, tell you if its complete and give other meta info&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;agent&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;action&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;next_state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;done&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;next_state&lt;/span&gt;        &lt;span class=&quot;c1&quot;&gt;# set the current state to the next state&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;total_reward&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reward&lt;/span&gt;   &lt;span class=&quot;c1&quot;&gt;# add the rewards&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;s:&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;a:&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;action&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;Episode: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;, Total reward: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;, eps: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ep&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;total_reward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;agent&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eps&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;render&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;agent&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q_table&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sleep&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.05&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;clear_output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;wait&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Limitations-of-Q-Learning&quot;&gt;Limitations of Q Learning&lt;a class=&quot;anchor-link&quot; href=&quot;#Limitations-of-Q-Learning&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;Q learning is good where the finite number of states and actions AND there are relatively small.  If there are too many states and actions, the number of possibilities are very high and there wont be sufficient memory to hold the table. But the more important concern is that some problems cannot be easily expressed as distinct states and actions.&lt;/p&gt;
&lt;p&gt;In the example from FROZEN LAKE, the states were SAFE, FROZEN, HOLE, GOAL. The actions were up, down, left and right.&lt;/p&gt;
&lt;p&gt;Lets say in the game pac-man such as these, the states are very &quot;similar&quot; and yet by using a q table it will be shown as 2 different states, and hence we can conclude that it cannot be easily translated. Here state is not a &quot;single unique distinct&quot; entity rather it is more characterized as a &quot;collection&quot; of features.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://abhisheksreesaila.github.io/blog/images/rl/pac1.png&quot; alt=&quot;&quot; title=&quot;Pacman State 1&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://abhisheksreesaila.github.io/blog/images/rl/pac2.png&quot; alt=&quot;&quot; title=&quot;Pacman State 2&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Useful way to think about it a set of odd numbers.  S={1,3,5}  S1={7,9,11}. Both represent odd numbers. They are similar and represented as a collection. Both &quot;S&quot; and &quot;S1&quot; are states represented by &quot;ODD&quot; numbers but they contain different values in them. Just like the pac-man example, we need a &quot;same/similar&quot; action to be performed on them but looking at them individually value by value we fail to capture the &quot;pattern&quot; or the &quot;essence&quot;&lt;/p&gt;
&lt;p&gt;The point here is we need to represent &quot;state&quot; as a collection and capture the essence rather than treating it as a single distinct entity. It is in cases like this, we take the help of a function which take in a bunch of features, &quot;understand their essence/patterns&quot; and spits out a Q-values for each action. See below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://abhisheksreesaila.github.io/blog/images/rl/univeral_function_approx.png&quot; alt=&quot;&quot; title=&quot;Univeral Function Approximator&quot; /&gt;&lt;/p&gt;
&lt;p&gt;The best known universal function approximators today are none other than &quot;neural networks&quot;. We can employ neural networks to learn the Q-values for different states. Neural Networks need some basic components to work effectively.
Lets see what they are and how we get can them.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Input&lt;/strong&gt; : Bunch of features&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Output&lt;/strong&gt; : Q-Values for each action (of course, like before, we will chose the one with the highest value)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Loss Function&lt;/strong&gt; : We can take the Bellman Error and Minimize them. However, the loss function we chose has to be &quot;differentiable&quot;. In other words, we should be apply backpropogation to transfer the loss from the &quot;output layer&quot; all the way back to the first hidden layer to update the &quot;&lt;strong&gt;weights or parameters $ \phi $&lt;/strong&gt;&quot; in the network. Lets see how we can convert &quot;bellman error&quot; to a differentiable function after which we are set to use the &quot;neural networks&quot; in our world of &quot;Q learning&quot;.
&lt;div class=&quot;flash&quot;&gt;
    &lt;svg class=&quot;octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info&quot; viewBox=&quot;0 0 14 16&quot; version=&quot;1.1&quot; width=&quot;14&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M6.3 5.69a.942.942 0 01-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 01-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;
    &lt;strong&gt;Note: &lt;/strong&gt;Since we combined &quot;deep learning&quot; with &quot;Q learning&quot; - we call it. &quot;Deep Q Learning&quot;
&lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Our incremental update formula is&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;$ Q(s,a,\phi) $ =  $ Q(s,a, \phi) $ + $ \alpha $ [$ R(s,a,s^1)  + \gamma  max_{a'} Q(s',a', \phi)$ - $ Q(s,a, \phi)] $&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Consider y = $ R(s,a,s^1)  + \gamma  max_{a'} Q(s',a', \phi)$&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;$Q(s,a, \phi)$ =  $ Q(s,a, \phi) $  - $ \alpha [Q(s,a, \phi) - y]$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Just like the mean squared loss, we can square the bellman error to punish the large variances.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;$ Q(s,a, \phi)$ =  $ Q(s,a, \phi) $ - $ \alpha [Q(s,a, \phi) - y]^2 $&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In the backward pass the gradient update of the parameters $ \phi $ of the Q function then looks like this, which can be optimized via stochastic gradient descent. Lets apply differential function w.r.t to $ \phi $ (parameters) to both sides of the equation&lt;/p&gt;
&lt;p&gt;$ \phi  = \phi - \alpha \frac{\partial (Q(s,a) - y)^2}{\partial \phi} $&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Problems-with-DQN&quot;&gt;Problems with DQN&lt;a class=&quot;anchor-link&quot; href=&quot;#Problems-with-DQN&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Corelated samples&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Unlike the SGD, the samples chosen during &quot;Q learning&quot; are close to each other and can overfit and does not converge to the global minima&lt;/li&gt;
&lt;li&gt;Solution : maintain a queue of past experiences;  use them to update the gradients. Breaks the close corelations&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Moving Targets&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Use 2 NN&lt;/li&gt;
&lt;li&gt;Compute the labels using the TARGET Network. fix it for a while&lt;/li&gt;
&lt;li&gt;Q Network will work as normal&lt;/li&gt;
&lt;li&gt;Stablizes the training for a certain duration&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;Mentioned in the paper Mink et al.  &lt;/li&gt;
&lt;li&gt;shown to solve a lot of problems&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;References&quot;&gt;References&lt;a class=&quot;anchor-link&quot; href=&quot;#References&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=RaIcFiNqP-0&amp;amp;list=PLYgyoWurxA_8ePNUuTLDtMvzyf-YW7im2&amp;amp;index=10&quot;&gt;Target Networks&lt;/a&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://ablearn.io/images/post-thumbnails/ql.png" /><media:content medium="image" url="https://ablearn.io/images/post-thumbnails/ql.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Depth Estimation and Object Detection</title><link href="https://ablearn.io/computer%20vision/2021/12/08/StereoVisionCode.html" rel="alternate" type="text/html" title="Depth Estimation and Object Detection" /><published>2021-12-08T00:00:00-06:00</published><updated>2021-12-08T00:00:00-06:00</updated><id>https://ablearn.io/computer%20vision/2021/12/08/StereoVisionCode</id><content type="html" xml:base="https://ablearn.io/computer%20vision/2021/12/08/StereoVisionCode.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2021-12-08-StereoVisionCode.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Purpose&quot;&gt;Purpose&lt;a class=&quot;anchor-link&quot; href=&quot;#Purpose&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;Use stereo vision to find out the depth of an object(s) in an image. Specifically, we will find out how far cars &amp;amp; people are from the camera&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Dataset&lt;/strong&gt; &lt;a href=&quot;http://www.cvlibs.net/datasets/kitti/&quot;&gt;KITTI Dataset&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Output&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://abhisheksreesaila.github.io/blog/images/stereo/DepthOfCar.png&quot; alt=&quot;&quot; title=&quot;Depth of Objects&quot; /&gt;

&lt;center&gt;
    &lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/ewBLt1lZ2Ik&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;/center&gt;
&lt;/p&gt;
&lt;h2 id=&quot;Outline-of-the-implementation&quot;&gt;Outline of the implementation&lt;a class=&quot;anchor-link&quot; href=&quot;#Outline-of-the-implementation&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;The code is available here&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Load Left and Right Images from &lt;a href=&quot;http://www.cvlibs.net/datasets/kitti/&quot;&gt;KITTI Dataset&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Compute &lt;strong&gt;Disparity&lt;/strong&gt;. Refer &lt;a href=&quot;https://ablearn.io/computer%20vision/2021/12/07/StereoVision.html#Finding-Depth&quot;&gt;here&lt;/a&gt; for an defintion and explanation&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Apply Stereo SGBM Matcher algorithm and computer Disparity&lt;/li&gt;
&lt;li&gt;We wil obtain Disparity Map&lt;/li&gt;
&lt;li&gt;Each pixel in the Disparity gives the Disparity value&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Read calibration parameters&lt;ul&gt;
&lt;li&gt;P0 and P1 are projection matrix for gray scale&lt;/li&gt;
&lt;li&gt;P1 and P2 are projection matrix for color scale&lt;/li&gt;
&lt;li&gt;R0_rect = rotation matrix&lt;/li&gt;
&lt;li&gt;Tr_velo_to_cam &amp;amp; Tr_imu_to_velo are translation matrices&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Compute the &lt;strong&gt;Depth Map&lt;/strong&gt; (The depth map is a map that contains Z for each pixel) Check out the explanation &lt;a href=&quot;https://ablearn.io/computer%20vision/2021/12/07/StereoVision.html#Finding-Depth&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;From the calibration parameters we get the projection matrix. Decompose them to get Rotation (R), K (Camera) and T (translation matrix) through a process called &lt;strong&gt;QR Factorization&lt;/strong&gt; Check out the explanation &lt;a href=&quot;https://ablearn.io/computer%20vision/2021/12/07/CameraCalibration.html#Decomposing-Projection-Matrix&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Use YOLO Object Detector to detect cars&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;You get bounding boxes coordinates. We need a point that represent the object (not 4 coordinates of the bounding boxes).  So, we get the &quot;center&quot; of the bounding box which effectively represents in the object.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Build the pipeline and run it on image&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;Run the pipeline on the video&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;Bonus-Section&quot;&gt;Bonus Section&lt;a class=&quot;anchor-link&quot; href=&quot;#Bonus-Section&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Given the following&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;disparity map&lt;/li&gt;
&lt;li&gt;camera matrices&lt;/li&gt;
&lt;li&gt;baseline&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Use &lt;strong&gt;cv2.stereoRectify&lt;/strong&gt;. to get a perspective transformation matrix (or Q matrix)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Use the Q Matrix and &lt;strong&gt;cv2.reprojectImageTo3D&lt;/strong&gt; to get a points in 3D space (camera)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Use library such as &quot;open3d&quot; to visualize&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;https://abhisheksreesaila.github.io/blog/images/stereo/3d-reconstruction.png&quot; alt=&quot;&quot; title=&quot;3D reconstruction&quot; /&gt;&lt;/p&gt;
&lt;p&gt;see opencv documentation &lt;a href=&quot;https://docs.opencv.org/3.4/d9/d0c/group__calib3d.html#ga1bc1152bd57d63bc524204f21fde6e02&quot;&gt;here&lt;/a&gt; for explanation&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;References&quot;&gt;References&lt;a class=&quot;anchor-link&quot; href=&quot;#References&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;&lt;a href=&quot;https://courses.thinkautonomous.ai/stereo-vision&quot;&gt;Think Autonomous Course&lt;/a&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://ablearn.io/images/post-thumbnails/DepthOfCar.png" /><media:content medium="image" url="https://ablearn.io/images/post-thumbnails/DepthOfCar.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Calibrating Stereo &amp;amp; Epipolar Geometry</title><link href="https://ablearn.io/computer%20vision/2021/12/08/StereoVision-2.html" rel="alternate" type="text/html" title="Calibrating Stereo &amp; Epipolar Geometry" /><published>2021-12-08T00:00:00-06:00</published><updated>2021-12-08T00:00:00-06:00</updated><id>https://ablearn.io/computer%20vision/2021/12/08/StereoVision-2</id><content type="html" xml:base="https://ablearn.io/computer%20vision/2021/12/08/StereoVision-2.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2021-12-08-StereoVision-2.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Calibration-of-the-Stereo&quot;&gt;Calibration of the Stereo&lt;a class=&quot;anchor-link&quot; href=&quot;#Calibration-of-the-Stereo&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;In the section above we assumed the stereo is calibrated that means we know the how they are aligned with respect to each other.&lt;/p&gt;
&lt;p&gt;Suppose we take a photo of effiel tower on a iphone. Then another person take a same photo with a slight different angle from samsung android phone. Is it possible to compute Z depth information and hence reocover the 3d structure of the image?  The answer turns out to be yes.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://abhisheksreesaila.github.io/blog/images/stereo/Uncalibrated-stereo.png&quot; alt=&quot;&quot; title=&quot;Uncalibrated Stereo&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Every digital camera embeds certain metadata within the image such as the focal length etc. which can be read as internal parameters. All we need to compute are the external parameters.&lt;/p&gt;
&lt;p&gt;In practice if there are 2 camera taking a shot at the same picture at 2 different angles, if we know the internal parameters of each camera, then we can calculate the alignment ourselves and hence compute the depth.  that is what we will explore in this section&lt;/p&gt;
&lt;p&gt;Consider the above picture.  It is identical to the one in the earlier section except that left and right cameras have their own coordinate system $(x_l,y_l, z_l)$ and $ (x_r, y_r, z_r)$ respectively.&lt;/p&gt;
&lt;p&gt;Our goal is to compute the &quot;translation&quot; and &quot;rotation&quot; of one camera w.r.t the other.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Epipolar-Geometry&quot;&gt;Epipolar Geometry&lt;a class=&quot;anchor-link&quot; href=&quot;#Epipolar-Geometry&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;https://abhisheksreesaila.github.io/blog/images/stereo/epipolar_geo.png&quot; alt=&quot;&quot; title=&quot;Epipolar Plane&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The highlighted triangle is &quot;Epipolar Plane&quot;. Its the plane formed by the scene point (P) and camera origins $ o_l $  and $ o_r $ is called epipolar plane&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;$e_l$ and $e_r$ are the projection of camera's origin on the left and right image planes respectively.  They are also called epipoles&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Every scene point will have it own epipolar plane.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;Now-why-do-we-care-about-epipolar-geometry?&quot;&gt;Now why do we care about epipolar geometry?&lt;a class=&quot;anchor-link&quot; href=&quot;#Now-why-do-we-care-about-epipolar-geometry?&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;blockquote&gt;&lt;p&gt;Our goal is to find a equation such that we can calculate t, R (translation, Rotation)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&quot;https://abhisheksreesaila.github.io/blog/images/stereo/epipolar_cons.png&quot; alt=&quot;&quot; title=&quot;Epipolar Constraint&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Epipolar-Constraint&quot;&gt;Epipolar Constraint&lt;a class=&quot;anchor-link&quot; href=&quot;#Epipolar-Constraint&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Consider a vector perpendicular to $X_l$ (highlighted in pink). Lets call it N&lt;/p&gt;
&lt;p&gt;From linear algebra,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;N = Cross Product between t and $X_l$&lt;/li&gt;
&lt;li&gt;N = t X $X_l$....(1)
Also, &lt;ul&gt;
&lt;li&gt;$X_l$ * N = 0 (dot product of N and $X_l$ is 0).....(2)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Hence from (1) and (2)&lt;/p&gt;
&lt;p&gt;(t X $X_l$) * $X_l$ = 0&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;This is the epipolar constraint.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;$X_l$ is a vector composed of elements $(x_l, y_l, z_l)$ and $x_l = R x_r + t$ (from the perspective projection)
Where t = position of right camera w.r.t to left; R = orientation of right camera w.r.t to left. At the end you will end up with&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;$X_l$ E $X_r$ = 0   ...(1)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;E is a 3x3 matrix called the Essential Matrix&lt;/p&gt;
&lt;p&gt;But we notice $X_l$ and $X_r$ stil exists! Our goal is to find these values.  So using perspective projection,&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$ u = f_x * x_l/z_l + O_x $  ;   $ v = f_y * y_l/z_l + O_y $  Where $ f_x and f_x $ are focal lengths measured in pixels&lt;/p&gt;
&lt;p&gt;Substituting for $x_l$ in equation (1) and expressing in matrix form, we get rid of $x_l$  and $y_l$. but $z_l$ remains!  But $z_l$ can never be 0, since it the depth. In common man terms, the world exists infront of the camera, so world coordinate will have some value of &quot;z&quot;, hence z &amp;lt;&amp;gt; 0. Using these concepts we arrive at&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;$U_l  K^{-1}_l E K^{-1}_r U_r$ = 0&lt;/p&gt;
&lt;p&gt;$U_l$ F $U_r$ = 0&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;where $U_l$ =  $[u_l, v_l, 1]$&lt;/p&gt;
&lt;p&gt;and $U_r$ = $$ \begin{bmatrix} u_r \\  v_r \\  1 \end{bmatrix} $$&lt;/p&gt;
&lt;p&gt;Where F is called fundamental matrix. I have intentionally skipped the math but for those mathematically inclined check out explanation &lt;a href=&quot;https://www.youtube.com/watch?v=6kpBqfgSPRc&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;How-does-this-work-in-practice?&quot;&gt;How does this work in practice?&lt;a class=&quot;anchor-link&quot; href=&quot;#How-does-this-work-in-practice?&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;Suppose we are given the &quot;F&quot; matrix, we can easily get &quot;E&quot; since we &quot;K&quot; is given to us&lt;/li&gt;
&lt;/ol&gt;
&lt;ol&gt;
&lt;li&gt;Once you get E from step 1, then a technique called &quot;&lt;a href=&quot;https://keisan.casio.com/exec/system/15076953160460&quot;&gt;singular value decomposition&lt;/a&gt;&quot; we can decompose it into &quot;t&quot; and &quot;R&quot;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;Finding-correspondence&quot;&gt;Finding correspondence&lt;a class=&quot;anchor-link&quot; href=&quot;#Finding-correspondence&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;In the previous sectionn, we said given a point $u_l, v_l$ finding a matching point $u_r v_r$ is a 1D search problem i.e. we have to search only in 1 direction, horizontally. But wait! where?  Can epipolar geometry help in the telling me the section the image to search?&lt;/p&gt;
&lt;p&gt;Fortunately the answer is yes! There is only other component of EPIPOLAR geometry to the rescue! Epipolar line.&lt;/p&gt;
&lt;p&gt;Imagine looking at the second camera origin, all the points on the $X_l$ will fall on the image plane as shown in red. also, this red line will intersect with the epipolar plane. This intersection is the epipolar line. In other words, The projection of all the points on the vector $X_l$ will lie on a line called EPIPOLAR line.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://abhisheksreesaila.github.io/blog/images/stereo/epipolar-line.png&quot; alt=&quot;&quot; title=&quot;Epipolar line&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://abhisheksreesaila.github.io/blog/images/stereo/epipolarline2.png [source](https://en.wikipedia.org/wiki/Epipolar_geometry&quot; alt=&quot;&quot; /&gt;)&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://abhisheksreesaila.github.io/blog/images/stereo/epipolar-formation.gif&quot; alt=&quot;&quot; title=&quot;Epipolar line Formation- Animated&quot; /&gt;&lt;/p&gt;
&lt;p&gt;From the last section, we have&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;$U_l$ F $U_r$ = 0&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Expanding..&lt;/p&gt;
$$


\begin{bmatrix} u_{l} \\  v_{l} \\  1  \end{bmatrix}

\begin{bmatrix} f_{11} &amp;amp; f_{12} &amp;amp; f_{13} \\  f_{21} &amp;amp; f_{22} &amp;amp; f_{23}  \\  f_{31} &amp;amp; f_{32} &amp;amp; f_{33}  \end{bmatrix}

\begin{bmatrix} u_r \\  v_r \\  1 \end{bmatrix}

=  0

$$&lt;p&gt;Multiplying...&lt;/p&gt;
&lt;p&gt;$(f_{11}u_l +f_{12}v_l + f_{31})u_r + (f_{21}u_l +f_{22}v_l + f_{32})v_r + (f_{13}u_l +f_{23}v_l + f_{33})= 0 $&lt;/p&gt;
&lt;p&gt;Simplified to...&lt;/p&gt;
&lt;p&gt;$Au_r + Bv_r + C = 0$ is a simple linear equation of the line that has all the projection points of $X_l$&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;This is the equation for the epipolar line&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;Summary&quot;&gt;Summary&lt;a class=&quot;anchor-link&quot; href=&quot;#Summary&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;Assume camera matrix K is known&lt;/li&gt;
&lt;li&gt;Find a few correspondence between 2 images (using SIFT, ORB or by hand). A minimum of 8 corresponding points are sufficient. &lt;/li&gt;
&lt;li&gt;Using points from step #2, find the t and R matrices. At this point it is said to be calibrated. &lt;/li&gt;
&lt;li&gt;Now that cameras are calibrated, for every point in one image we can find a corresponding point. Turns out it is a 1D search problem along the epipolar line.&lt;/li&gt;
&lt;li&gt;Compute depth by Traingulation. &lt;/li&gt;
&lt;/ol&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;References&quot;&gt;References&lt;a class=&quot;anchor-link&quot; href=&quot;#References&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=hUVyDabn1Mg&quot;&gt;Stereo Vision&lt;/a&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://ablearn.io/images/post-thumbnails/Epipolar_Geometry.png" /><media:content medium="image" url="https://ablearn.io/images/post-thumbnails/Epipolar_Geometry.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Stereo Vision Intro</title><link href="https://ablearn.io/computer%20vision/2021/12/07/StereoVision.html" rel="alternate" type="text/html" title="Stereo Vision Intro" /><published>2021-12-07T00:00:00-06:00</published><updated>2021-12-07T00:00:00-06:00</updated><id>https://ablearn.io/computer%20vision/2021/12/07/StereoVision</id><content type="html" xml:base="https://ablearn.io/computer%20vision/2021/12/07/StereoVision.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2021-12-07-StereoVision.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Purpose&quot;&gt;Purpose&lt;a class=&quot;anchor-link&quot; href=&quot;#Purpose&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;In this multi-part blog post, we try to understand&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Why we need stereo in computer vision?&lt;/li&gt;
&lt;li&gt;Given any stereo cameras, how can we calibrate and use them to find depth?&lt;/li&gt;
&lt;li&gt;Summary of steps&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;Why-do-we-need-Stereo?&quot;&gt;Why do we need Stereo?&lt;a class=&quot;anchor-link&quot; href=&quot;#Why-do-we-need-Stereo?&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;h2 id=&quot;The-loss-of-depth&quot;&gt;The loss of depth&lt;a class=&quot;anchor-link&quot; href=&quot;#The-loss-of-depth&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Given a point in the image plane (U, V), can we find a corresponding point in the world system? The answer is NO.  When we move capture a image we know that 3d world coordinate gets transformed to camera cooridinate and then a 2D image plane. Refer to this blog post for indepth details. Since we loose information, more specifically &quot;Z&quot; depth information, it is impossible to get it back.  In other words, given an image point you cannot reverse engineer WORLD coordinate since you have lost a crucial &quot;Z&quot; coordinate in the translation process. however, you havent lost &quot;X&quot;, &quot;Y&quot;.  There is hope to get it back, but we need additional help.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://abhisheksreesaila.github.io/blog/images/stereo/monocular2.png&quot; alt=&quot;&quot; title=&quot;Loss of Depth&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Since we know &quot;X&quot;, &quot;Y&quot;, we know it exists somewhere along the line shown as dotted green lines. Why? This was the same line which was used in the projection of object onto the image plane to derive all the math. So, it can also assist in the reverse direction as well.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://abhisheksreesaila.github.io/blog/images/stereo/projection2.png&quot; alt=&quot;&quot; title=&quot;Projection of the object on image plane&quot; /&gt;&lt;/p&gt;
&lt;h3 id=&quot;How-to-recover?&quot;&gt;How to recover?&lt;a class=&quot;anchor-link&quot; href=&quot;#How-to-recover?&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;The trick is to understand how nature does this. We all have 2 eyes, and we perceive depth (3d) in all objects. Don't we? The &quot;second eye&quot; provides its view of the world  in addition to the first, and both together work together perceive depth.&lt;/p&gt;
&lt;p&gt;Lets apply the same concept here. lets bring in another camera, place it horizontally along the same axis at a distance, find the exact same spot (U, V) on it, guess the point on the &quot;dotted green line&quot;.  The intersection of these 2 dotted green lines gives you the depth Z.  See below for the visual&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://abhisheksreesaila.github.io/blog/images/stereo/stereo.png&quot; alt=&quot;&quot; title=&quot;Stereo Vision&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$ u_r v_r $ and  $ u_l v_l $ are the exact same point of the image as seen in the right and left camera respectively&lt;/li&gt;
&lt;li&gt;the distance between the cameras is called baseline denoted by b&lt;/li&gt;
&lt;li&gt;the camera plane is placed at the pinhole with origin (0,0,0) on the left and (b,0,0) on the right&lt;/li&gt;
&lt;li&gt;P (x, y, z) is the scene point that we are trying to compute using the 2 cameras.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This concept of using 2 cameras to perceive depth in the real world is called Simple Stereo Vision.  In this blog post lets understand the mechanics of such a system.&lt;/p&gt;
&lt;h2 id=&quot;Finding-Depth&quot;&gt;Finding Depth&lt;a class=&quot;anchor-link&quot; href=&quot;#Finding-Depth&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Lets start with the basics. For a pinhole based camera system we know the following equations&lt;/p&gt;
&lt;p&gt;$ u_l = f_x * x/z + O_x $  ;   $ v_l = f_y * y/z  + O_y $&lt;/p&gt;
&lt;p&gt;For the right camera, its the same but the camera axis is shifted by &quot;b&quot;&lt;/p&gt;
&lt;p&gt;$ u_r = f_x * x/z + O_x $  ;   $ v_r = f_y * y/z  + O_y $&lt;/p&gt;
&lt;p&gt;Using the 4 equations, solving for x, y, z  we get&lt;/p&gt;
&lt;p&gt;$ x = \frac {b (u_l - O_x)}{ u_l - u_r } $
$ y = \frac {b f_x (v_l - O_y)}{f_y (u_l - u_r)} $&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;$ z = \frac {b f_x}{ u_l - u_r } $&lt;/strong&gt;
&lt;div class=&quot;flash&quot;&gt;
    &lt;svg class=&quot;octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info&quot; viewBox=&quot;0 0 14 16&quot; version=&quot;1.1&quot; width=&quot;14&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M6.3 5.69a.942.942 0 01-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 01-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;
    &lt;strong&gt;Note: &lt;/strong&gt;$ u_l - u_r $ is called Disparity and its inversely propotional to &quot;z&quot;. 
&lt;/div&gt;&lt;div class=&quot;flash flash-warn&quot;&gt;
    &lt;svg class=&quot;octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap&quot; viewBox=&quot;0 0 10 16&quot; version=&quot;1.1&quot; width=&quot;10&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M10 7H6l3-7-9 9h4l-3 7 9-9z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;
    &lt;strong&gt;Important: &lt;/strong&gt;If we know the internal parameters fx, fy, ox, oy and compute disparity, we compute Z and hence the depth. 
&lt;/div&gt;
If the object is closer to the camera, you will see a large disparity. for example, U value in the left camera will be 100, whereas the right camera it will be 75.  This is exact same pixel in the image but having 2 different values. The opposite is also true i.e the object is far, there will be very less difference between the U values (Say 100 and 95).  At infinite distance, U values will exactly be the same.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Disparity is propotional to baseline meaning if the distance between camera increase, disparity will increase.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I keep mentioning only &quot;U&quot; because there is no $v_l - v_r $ in the equation. which means only the horizontal component between the 2 cameras vary not the &quot;vertical component&quot;.  This proporty shows that  $ u_r, v_r $ and $ u_l $ and $ v_l $ lie along the same line (show by the yellow line). when we are computing DISPARITY to solve for X, Y, Z  in the real world, we can pick a point in the left camera $ (u_l, v_l) $ and  ONLY search along the same line in the right camera (and not wander aimlessly and search the whole image) to get the $ u_r, v_r $ .i.e  its a &quot;1D&quot; search problem. See image below for an example. This is often called the &lt;strong&gt;&quot;correspondence problem&quot;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://abhisheksreesaila.github.io/blog/images/stereo/texture.png&quot; alt=&quot;&quot; title=&quot;Finding Correspondence&quot; /&gt;
The white patch in the picture is called the &quot;scan line&quot;.&lt;/p&gt;
&lt;h2 id=&quot;Problems-with-stereo-matching&quot;&gt;Problems with stereo matching&lt;a class=&quot;anchor-link&quot; href=&quot;#Problems-with-stereo-matching&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Did we solve of finding depth just by solving 2 cameras? yes for the most part, but if the images have the repetitive texture, its impossible to compute disparity and therefore can't compute depth.  see image below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://abhisheksreesaila.github.io/blog/images/stereo/no-texture.png&quot; alt=&quot;&quot; title=&quot;Stereo Vision CANNOT be computed&quot; /&gt;&lt;/p&gt;
&lt;h1 id=&quot;Next-Steps&quot;&gt;Next Steps&lt;a class=&quot;anchor-link&quot; href=&quot;#Next-Steps&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;In the next &lt;a href=&quot;https://ablearn.io/computer%20vision/2021/12/08/StereoVision-2.html&quot;&gt;part&lt;/a&gt; we will understand how to calibrate stereo cameras and use them to compute depth&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;References&quot;&gt;References&lt;a class=&quot;anchor-link&quot; href=&quot;#References&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=hUVyDabn1Mg&quot;&gt;Stereo Vision&lt;/a&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://ablearn.io/images/post-thumbnails/sv2.png" /><media:content medium="image" url="https://ablearn.io/images/post-thumbnails/sv2.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Camera Calibration</title><link href="https://ablearn.io/computer%20vision/2021/12/07/CameraCalibration.html" rel="alternate" type="text/html" title="Camera Calibration" /><published>2021-12-07T00:00:00-06:00</published><updated>2021-12-07T00:00:00-06:00</updated><id>https://ablearn.io/computer%20vision/2021/12/07/CameraCalibration</id><content type="html" xml:base="https://ablearn.io/computer%20vision/2021/12/07/CameraCalibration.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2021-12-07-CameraCalibration.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Pinhole-Camera-Basics&quot;&gt;Pinhole Camera Basics&lt;a class=&quot;anchor-link&quot; href=&quot;#Pinhole-Camera-Basics&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;&lt;img src=&quot;https://abhisheksreesaila.github.io/blog/images/stereo/pinhole.png&quot; alt=&quot;&quot; title=&quot;Pinhole Camera&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Before we go explain this in detail lets clarify the terms&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The object (flower) is said to be &quot;in the &quot;world&quot; and its position is given by &quot;world coordinates&quot;&lt;/li&gt;
&lt;li&gt;Now imagine The light rays from the object pass through a pinhole and gets projected on a wall.  The wall  is the &quot;image plane&quot; and the hole it passes through is called the &quot;pinhole&quot;. The camera is placed at the pinhole. The camera is said to be in the &quot;Camera plane&quot;&lt;/li&gt;
&lt;li&gt;The distance between the camera placed at the pinhole and the wall (image plane) is called &quot;focal length&quot;&lt;/li&gt;
&lt;li&gt;&quot;z&quot; is also called Depth
&lt;div class=&quot;flash&quot;&gt;
    &lt;svg class=&quot;octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info&quot; viewBox=&quot;0 0 14 16&quot; version=&quot;1.1&quot; width=&quot;14&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M6.3 5.69a.942.942 0 01-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 01-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;
    &lt;strong&gt;Note: &lt;/strong&gt;Our goal is to take the 3D coordinates (x,y,z) and TRANSFORM them  to camera plane (3D) and then later PROJECT THEM to the image plane (2D).  
&lt;/div&gt;
We move from WORLD =&amp;gt; CAMERA =&amp;gt; IMAGE. This is called Forward Imaging Model.  Also the projection from Camera to the image plane is called &quot;Perspective Projection&quot;. so we can write the forward imaging model as
&lt;div class=&quot;flash&quot;&gt;
    &lt;svg class=&quot;octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info&quot; viewBox=&quot;0 0 14 16&quot; version=&quot;1.1&quot; width=&quot;14&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M6.3 5.69a.942.942 0 01-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 01-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;
    &lt;strong&gt;Note: &lt;/strong&gt;$ [x_w, y_w, z_w] $ (Coordinate transformation) =&amp;gt;  $ [x_c, y_c, z_c]  $ =&amp;gt; (Perspective Projection) $ =&amp;gt;   [x_i, y_i]   $ 
&lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;h1 id=&quot;Perspective-Imaging-with-Pinhole&quot;&gt;Perspective Imaging with Pinhole&lt;a class=&quot;anchor-link&quot; href=&quot;#Perspective-Imaging-with-Pinhole&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;&lt;img src=&quot;https://abhisheksreesaila.github.io/blog/images/stereo/similartri.png&quot; alt=&quot;&quot; title=&quot;Similar Triangles&quot; /&gt;&lt;/p&gt;
&lt;p&gt;The above diagram is exactly same as &quot;Pinhole&quot; diagram with labels removed for explaining geometry.The yellow and blue triangles are similar.  By the property of similar triangles (see below note), we have&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$r_i/f = r_o/z$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Where $r_o = (x_o, y_o, z_o )$ and $ r_i = (x_i, y_i, f) $&lt;/p&gt;
&lt;p&gt;$ r_i $ and $ r_o $ are &lt;a href=&quot;https://mathinsight.org/vector_introduction&quot;&gt;vectors&lt;/a&gt; describing the point on the image and world respectively. Hence we can split and write the equation as&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$ x_i/f = x_o/z $  ;   $ y_i/f = y_i/z $
&lt;div class=&quot;flash&quot;&gt;
    &lt;svg class=&quot;octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info&quot; viewBox=&quot;0 0 14 16&quot; version=&quot;1.1&quot; width=&quot;14&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M6.3 5.69a.942.942 0 01-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 01-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;
    &lt;strong&gt;Note: &lt;/strong&gt;Triangles are set to be &quot;similar&quot; if they are of the same shape but not necessarily the same size. If 2 triangles have this property then 
&lt;/div&gt;  &lt;em&gt;$AB/DE = BC/EF$&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;https://abhisheksreesaila.github.io/blog/images/stereo/s-triangles-2.png&quot; alt=&quot;&quot; title=&quot;Similar Triangles Concept&quot; /&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;h2 id=&quot;Pinhole-Properties&quot;&gt;Pinhole Properties&lt;a class=&quot;anchor-link&quot; href=&quot;#Pinhole-Properties&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;Straight line remains straight in the image plane&lt;/li&gt;
&lt;li&gt;Magnification is inversely proportional to depth ($z_o$). In other words, The further you are from the camera, smaller you appear. Now this is obvious since we all use cameras today. But take a look at the parallel lines intersects for a railway track. The distance between the parallel tracks becomes so small in the image plane that it looks like intersection though in reality we know its parallel throughout. The point of intersection is called the &quot;Vanishing Point&quot;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&quot;https://abhisheksreesaila.github.io/blog/images/stereo/railway-tracks.png&quot; alt=&quot;&quot; title=&quot;Similar Triangles Concept&quot; /&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Since a pinhole is a tiny hole letting the light inside the camera to capture the images, for a large object it will take a few seconds before the image is captured. Imagine clicking a camera and waiting 5 seconds to see the image!!  To improve this, we substitute pinhole with LENS. &lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;Pinhole-vs-Lens&quot;&gt;Pinhole vs Lens&lt;a class=&quot;anchor-link&quot; href=&quot;#Pinhole-vs-Lens&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;We wont cover LENS and its properties, but just keep in mind, LENS are just a &quot;pinhole&quot; made larger. Thats a simplistic view of it. OBviously with more area to capture light, now the wait time to capture the image is reduced, but light is scattered now, and hence causes distortions in your image.
Apart from the focal length we introduce &quot;distortion coefficients&quot; as one of the Intrinisic Parameters.  All modern cameras use spohisticated lenses.&lt;/p&gt;
&lt;p&gt;See below&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://abhisheksreesaila.github.io/blog/images/stereo/lens-camera.png&quot; alt=&quot;&quot; title=&quot;Pinhole vs Lens&quot; /&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;h1 id=&quot;Camera-Calibration&quot;&gt;Camera Calibration&lt;a class=&quot;anchor-link&quot; href=&quot;#Camera-Calibration&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;&lt;div class=&quot;flash&quot;&gt;
    &lt;svg class=&quot;octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info&quot; viewBox=&quot;0 0 14 16&quot; version=&quot;1.1&quot; width=&quot;14&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M6.3 5.69a.942.942 0 01-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 01-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;
    &lt;strong&gt;Note: &lt;/strong&gt;The goal of calibration is estimate the Extrinsic and Intrinsic parameters. 
&lt;/div&gt;&lt;/p&gt;
&lt;h2 id=&quot;Derive-an-equation-for-camera-calibration&quot;&gt;Derive an equation for camera calibration&lt;a class=&quot;anchor-link&quot; href=&quot;#Derive-an-equation-for-camera-calibration&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;&lt;div class=&quot;flash flash-error&quot;&gt;
    &lt;svg class=&quot;octicon octicon-alert octicon octicon-alert octicon octicon-alert octicon octicon-alert&quot; viewBox=&quot;0 0 16 16&quot; version=&quot;1.1&quot; width=&quot;16&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M8.893 1.5c-.183-.31-.52-.5-.887-.5s-.703.19-.886.5L.138 13.499a.98.98 0 000 1.001c.193.31.53.501.886.501h13.964c.367 0 .704-.19.877-.5a1.03 1.03 0 00.01-1.002L8.893 1.5zm.133 11.497H6.987v-2.003h2.039v2.003zm0-3.004H6.987V5.987h2.039v4.006z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;
    &lt;strong&gt;Warning: &lt;/strong&gt;This is math heavy.
&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;Lets use our knowledge about pinhole, perspective projection and derive a simple linear model for calibration.&lt;/p&gt;
&lt;p&gt;Take a point in world coordinate. Assume you are given &quot;Extrinsic Parameters&quot; and so you have successfully transformed them into camera coordinates
Applying perspective projection formula from above we get, $ x_i/f = x_c/z_c $  ;   $ y_i/f = y_c/z_c $   where *_c denotes coordinates in camera plane.&lt;/p&gt;
&lt;p&gt;Rewriting the equation we get,&lt;/p&gt;
&lt;p&gt;$ x_i = f * x_i/z_c $  ;   $ y_i = f * y_i/z_c $&lt;/p&gt;
&lt;p&gt;Lets take a closer look at the IMAGE PLANE.&lt;/p&gt;
&lt;p&gt;For a sake of understanding the basic pinhole model we assumed the image plane to have &quot;mm&quot; millimetres as the unit of measurement. So we take a world coordinate in &quot;m&quot; and convert to &quot;mm&quot;. In reality, we all know it is measured in &quot;pixels&quot; measured by a image sensor. So there could be &quot;mulitple pixels per mm&quot; and so the alternative measurement to be consisdered is &quot;pixel density&quot; which is nothing but &quot;pixels per mm&quot;. So introduce this to in the equation as follows.&lt;/p&gt;
&lt;p&gt;$ x_i = m * f * x_i/z_c $  ;   $ y_i = m * f * y_i/z_c $&lt;/p&gt;
&lt;p&gt;Why we multiply?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pixel Density = 1 ; 1 mm contains 1px;  &lt;/li&gt;
&lt;li&gt;Pixel Density = 10 ; 1 mm contains 10px;   2 mm contains 20px;  3 mm contains 30px;  X mm contains (PixelDensity * X)px;  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;https://abhisheksreesaila.github.io/blog/images/stereo/ImagePlaneToSensor.png&quot; alt=&quot;&quot; title=&quot;Image Plane To Image Sensor Mapping.png&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Second assumption we made was the centre of the Image plane (measured in mm) is the centre of the Image Sensor which may or may not be true. To avoid this uncertainity, we just place the centre of the image sensor to be at the far left-side corner (for understanding and mathematical convenience) and just add whatever that needs to be added to get the correct centre. so now the equation becomes,&lt;/p&gt;
&lt;p&gt;$ x_i = m_x * f * x_c/z_c + O_x $  ;   $ y_i = m_y * f * y_c/z_c  + O_y $  x_i  and y_i is popularly referred to as u and v.&lt;/p&gt;
&lt;p&gt;$ u = m_x * f * x_c/z_c + O_x $  ;   $ v = m_y * f * y_c/z_c  + O_y $&lt;/p&gt;
&lt;p&gt;$ u = f_x * x_c/z_c + O_x $  ;   $ v = f_y * y_c/z_c  + O_y $  Where $ f_x and f_x $ are focal lengths measured in pixels
&lt;div class=&quot;flash flash-warn&quot;&gt;
    &lt;svg class=&quot;octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap&quot; viewBox=&quot;0 0 10 16&quot; version=&quot;1.1&quot; width=&quot;10&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M10 7H6l3-7-9 9h4l-3 7 9-9z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;
    &lt;strong&gt;Important: &lt;/strong&gt;$ f_x, f_y, O_x, O_y $ represent the camera&amp;#8217;s internal geometry and are called INTRINSIC parameters
&lt;/div&gt;
U and V are non-linear equation, but mathematically its convenient to have a linear equation if we can get one. Homogenous coordinates to the rescue!&lt;/p&gt;
&lt;p&gt;Play &lt;a href=&quot;https://wordsandbuttons.online/interactive_guide_to_homogeneous_coordinates.html&quot;&gt;here&lt;/a&gt; to get a good visual understanding. Basically, the concept is to add &quot;fake&quot; coordinate at the end to scale the values in the same propotion. For eg.  8/4 (=2) is same as 16/8 (=2).  Similarly in the world of coordinates geometry [3,2] will be same as [6,4,2] where 2 is the scale factor (or the fake coordinate added). We can always get the original value back from [6,4,2] by division [6/2,4/2]&lt;/p&gt;
&lt;p&gt;First multiply with $ z_c $&lt;/p&gt;
&lt;p&gt;$ u * z_c = f_x * x_c + O_x * z_c $  ;   $ v * z_c = f_y * y_c  + O_y * z_c $&lt;/p&gt;
&lt;p&gt;Represent them in Homogenous Coordinates
&lt;div class=&quot;flash&quot;&gt;
    &lt;svg class=&quot;octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info&quot; viewBox=&quot;0 0 14 16&quot; version=&quot;1.1&quot; width=&quot;14&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M6.3 5.69a.942.942 0 01-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 01-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;
    &lt;strong&gt;Note: &lt;/strong&gt;(To convince ourselves that it is the same equation we derived moments ago, just divide z_c in the matrix. All these are just math tricks to get the same answer)
&lt;/div&gt;&lt;/p&gt;
$$

\begin{bmatrix} u_h  \\  v_h \\ w_h  \end{bmatrix}
=
\begin{bmatrix} f_x*x_c + O_x*z_c  \\     f_y*y_c + O_y*z_c  \\ z_c  \end{bmatrix}

$$&lt;p&gt;Showing the same equation in Matrix form&lt;/p&gt;
$$

M_{int} = K = 

\begin{bmatrix} f_x &amp;amp; 0 &amp;amp; O_x &amp;amp; 0 \\  0 &amp;amp; f_y &amp;amp; O_y &amp;amp; 0  \\ 0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 0   \end{bmatrix}
\begin{bmatrix} x_c \\ y_c \\ z_c \\ 1 \end{bmatrix}

$$&lt;p&gt;This is also called &quot;Intrinsic Matrix or Camera Matrix&quot; popularly shown as K&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;In the beginning of this section we assumed we are given Camera Coordinates. Lets check out what it consists of.&lt;/p&gt;
&lt;p&gt;Imagine a camera placed somewhere in an empty room other than the corners.  Fix a corner of this empty room as the origin.  Then camera is at a &lt;em&gt;distance&lt;/em&gt; from this corner. Additionally it will be at an &lt;em&gt;angle&lt;/em&gt; to the orgin of the world coordinate, in our case, the corner of the room we have chosen. In computer vision terms, the camera is said to be &quot;translated (geometric translation, not language tranlsation :) )  and &quot;rotated&quot; w.r.t to the world coordinate. Hence, we need some sort of translation and rotation variables that will help us take the world cooridinates of the image and project them to the camera first. Right? We call this &quot;Extrinsic Parameters&quot;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://abhisheksreesaila.github.io/blog/images/stereo/coordsys1.png&quot; alt=&quot;&quot; title=&quot;Co-ordinate system changes&quot; /&gt;
&lt;em&gt;&lt;a href=&quot;https://www.scratchapixel.com/lessons/3d-basic-rendering/computing-pixel-coordinates-of-3d-point/mathematics-computing-2d-coordinates-of-3d-points&quot;&gt;Source&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In the picture above local refers to camera coordinate system. Lets call the &quot;pink point&quot; as P. Also observe that &quot;the pink point&quot; is not 2 different points but the exact same point but represented in 2 different systems. In other words  $ P_{camera} $  is same as $  P_{world} $&lt;/p&gt;
&lt;p&gt;Rotation is provided by the rotation matrix and  is given as follows :-&lt;/p&gt;
$$

R 
=
\begin{bmatrix} r_1 &amp;amp; r_2 &amp;amp; r_3 \\  r_4 &amp;amp; r_5 &amp;amp; r_6  \\  r_7 &amp;amp; r_8 &amp;amp; r_9  \end{bmatrix}

$$&lt;ul&gt;
&lt;li&gt;$  r_1, r_2, r_3  $ represents rotation in the x direction &lt;/li&gt;
&lt;li&gt;$  r_4, r_5, r_6  $ represents rotation in the y direction &lt;/li&gt;
&lt;li&gt;$  r_7, r_8, r_9  $ represents rotation in the z direction &lt;blockquote&gt;&lt;p&gt;R is also taken as &lt;a href=&quot;https://en.wikipedia.org/wiki/Orthogonal_matrix&quot;&gt;Orthonormal Matrix&lt;/a&gt;
Now we have,
$ P_{camera} $ = R *  ($ P_{world} - C_{world}) $  where $ C_{world} $ is the translation. Its negative since &quot;Z&quot; axis of the camera system is exactly opposite to the world system. It has to be done this way so that X and Y axis are in sync i.e if we move left in 1 system, it moves left in the other and so on.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;https://abhisheksreesaila.github.io/blog/images/stereo/extparam.png&quot; alt=&quot;&quot; title=&quot;World to Camera Transformation&quot; /&gt;&lt;a href=&quot;&amp;quot;http://www.cse.psu.edu/~rtc12/CSE486/lecture12.pdf&amp;quot;&quot;&gt;Source&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Changing this to homogenous coordinates we get,&lt;/p&gt;
$$

M_{ext} = 

\begin{bmatrix} X_{c}  \\  Y_{c} \\ Z_{c}  \\ 1 \end{bmatrix}
=

\begin{bmatrix} r_1 &amp;amp; r_2 &amp;amp; r_3 &amp;amp; 0 \\  r_4 &amp;amp; r_5 &amp;amp; r_6 &amp;amp; 0   \\  r_7 &amp;amp; r_8 &amp;amp; r_9 &amp;amp; 0  \\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 \end{bmatrix}

\begin{bmatrix} 1 &amp;amp; 0 &amp;amp; 0 &amp;amp; -c_{x} \\   0 &amp;amp; 1 &amp;amp; 0 &amp;amp; -c_{y}  \\   0 &amp;amp; 0 &amp;amp; 1 &amp;amp; -c_{z} \\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 \end{bmatrix}

\begin{bmatrix} X_{w}  \\  Y_{w} \\ Z_{w}  \\ 1 \end{bmatrix}

$$&lt;p&gt;$ M_{ext} $ is the Extrinisic Matrix&lt;/p&gt;
&lt;h1 id=&quot;Projection-Matrix&quot;&gt;Projection Matrix&lt;a class=&quot;anchor-link&quot; href=&quot;#Projection-Matrix&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;In the begining of the blog post we said We want to move from WORLD =&amp;gt; CAMERA =&amp;gt; IMAGE to take the world coordinates and place them on the image. From the previous section we understood that to move from&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;WORLD =&amp;gt; CAMERA = we need extrinisic matrix&lt;/li&gt;
&lt;li&gt;CAMERA =&amp;gt; IMAGE = we need intrinsic matrix&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can show mathematically as&lt;/p&gt;
&lt;p&gt;$ u_{h} = M_{int}  M_{ext} X_{w} $&lt;/p&gt;
&lt;p&gt;Image points in Homogenous Coordinates =  Intrinsic Matrix &lt;em&gt; Extrinisic Matrix &lt;/em&gt; (point in the world coordinate)&lt;/p&gt;
&lt;p&gt;We can combine these 2 matrices into 1 matrix called &quot;Projection Matrix&quot;&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;$ u_{h} = P X_{w} $  Where  P = $ M_{int} M_{ext} $&lt;/p&gt;
&lt;/blockquote&gt;
$$

\begin{bmatrix} p_1 &amp;amp; p_2 &amp;amp; p_3 &amp;amp; p_4 \\  p_5 &amp;amp; p_6 &amp;amp; p_7 &amp;amp; p_8   \\  p_9 &amp;amp; p_{10} &amp;amp; p_{11} &amp;amp; p_{12}     \end{bmatrix}
 = 
\begin{bmatrix} f_x &amp;amp; 0 &amp;amp; O_x &amp;amp; 0 \\  0 &amp;amp; f_y &amp;amp; O_y &amp;amp; 0  \\ 0 &amp;amp; 0 &amp;amp; 1 &amp;amp; 0   \end{bmatrix}

\begin{bmatrix} r_1 &amp;amp; r_2 &amp;amp; r_3 &amp;amp; t_x \\  r_4 &amp;amp; r_5 &amp;amp; r_6 &amp;amp; t_y   \\  r_7 &amp;amp; r_8 &amp;amp; r_9 &amp;amp; t_z  \\ 0 &amp;amp; 0 &amp;amp; 0 &amp;amp; 1 \end{bmatrix}

$$&lt;h2 id=&quot;Decomposing-Projection-Matrix&quot;&gt;Decomposing Projection Matrix&lt;a class=&quot;anchor-link&quot; href=&quot;#Decomposing-Projection-Matrix&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;From the above Projection Matrix  P = $ M_{int} M_{ext} $  we can deduce that&lt;/p&gt;
$$

\begin{bmatrix} p_1 &amp;amp; p_2 &amp;amp; p_3  \\  p_5 &amp;amp; p_6 &amp;amp; p_7    \\  p_9 &amp;amp; p_{10} &amp;amp; p_{11}   \end{bmatrix}
 = 
\begin{bmatrix} f_x &amp;amp; 0 &amp;amp; O_x \\  0 &amp;amp; f_y &amp;amp; O_y  \\ 0 &amp;amp; 0 &amp;amp; 1    \end{bmatrix}

\begin{bmatrix} r_1 &amp;amp; r_2 &amp;amp; r_3  \\  r_4 &amp;amp; r_5 &amp;amp; r_6 \\  r_7 &amp;amp; r_8 &amp;amp; r_9  \end{bmatrix}

$$&lt;p&gt;We know that Rotation matrix R is a orthonormal matrix. K is the Upper Right Triangular matrix.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://abhisheksreesaila.github.io/blog/images/stereo/KMatrix-UpperTri.png&quot; alt=&quot;&quot; title=&quot;Upper Triangular Matrix&quot; /&gt;
It is possible to decouple them through a process called &lt;strong&gt;&lt;a href=&quot;https://www.emathhelp.net/en/calculators/linear-algebra/qr-factorization-calculator/&quot;&gt;QR Factorization&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;h1 id=&quot;I-understand-Calibration-now.--How-do-we-starting-writing-code?&quot;&gt;I understand Calibration now.  How do we starting writing code?&lt;a class=&quot;anchor-link&quot; href=&quot;#I-understand-Calibration-now.--How-do-we-starting-writing-code?&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;Opencv provides all the functions built-in to estimate parameters. Think of an real world object as an input and the pixel coordinates as the output. The problem at hand is to adjust the middle box in between in such a way that we get the desired output from the input provided. If we do this once, you will get an estimate, but those parameters are likely only to be useful in that angle, direction of the camera. Repeat the steps multiple times and you have a good estimate. It sort of unsupervised machine learning so to speak!&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://abhisheksreesaila.github.io/blog/images/stereo/calib-visual.png&quot; alt=&quot;&quot; title=&quot;Calibration Paramters Estimation&quot; /&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;h1 id=&quot;Code&quot;&gt;Code&lt;a class=&quot;anchor-link&quot; href=&quot;#Code&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;h1 id=&quot;References&quot;&gt;References&lt;a class=&quot;anchor-link&quot; href=&quot;#References&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;&lt;a href=&quot;https://docs.opencv.org/3.4.3/d9/d0c/group__calib3d.html&quot;&gt;OpenCV Calibration&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://learnopencv.com/camera-calibration-using-opencv/&quot;&gt;Geometry of Image Formation&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=_EhY31MSbNM&amp;amp;t=194s&quot;&gt;Pinhole basics&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=qByYk6JggQU&quot;&gt;Calibration Basics&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://personal.math.ubc.ca/~pwalls/math-python/jupyter/latex/&quot;&gt;Latex in Jupyter Notebook&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://www.scratchapixel.com/lessons/3d-basic-rendering/computing-pixel-coordinates-of-3d-point/mathematics-computing-2d-coordinates-of-3d-points&quot;&gt;Conversion of Coordinate Basics&lt;/a&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://ablearn.io/images/post-thumbnails/calibration.jpg" /><media:content medium="image" url="https://ablearn.io/images/post-thumbnails/calibration.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>