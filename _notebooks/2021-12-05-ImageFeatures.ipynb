{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"What's in an image?\"\n",
    "\n",
    "> \"Every photo is beautiful, if you look at the right features!\"\n",
    "\n",
    "- toc: true\n",
    "- branch: master\n",
    "- badges: false\n",
    "- comments: true\n",
    "- categories: [Computer Vision]\n",
    "- hide: false\n",
    "- search_exclude: false\n",
    "- image: images/post-thumbnails/image_features.jpg\n",
    "- metadata_key1: notes\n",
    "- metadata_key2: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# What is a feature in an image?\n",
    "\n",
    "Features : things that will help us identify the image.  The best features are the ones where by looking at them once you are able to detect the image. Sometime 1 feature is not sufficient and you need to collection of them, each of them helping you identify the image.\n",
    "\n",
    "![](https://abhisheksreesaila.github.io/blog/images/stereo/features.png \"Image features\")\n",
    "\n",
    "Look at the following image.  If you pick a patch in the background, it does not tell anything about the image. infact, we can collect hundreds of them and still no use. Lets try something in the middle of the image. again, even If i give you hundreds of them surrounding it, it does not identify the image. Now, lets pick a corner of the image.  IF you closely observe, there can be only 1 corner of that type.  We can pick different corner all of which are unique and help you identify the image.  Hence, you can consider, CORNER make a good collection of features. As such from the early days of computer vision, many algorithms were developed to detect \"corners\" of the image just because they are good features. \n",
    "features\n",
    "\n",
    "---\n",
    "\n",
    "## Why do we need features?\n",
    "\n",
    " NEarly all computer vision algorithms in any domain dont just consume the image as a whole and produce an output. They are always preceeded by \"feature extraction\" and only then peforms their calculation.  (In the world of machine learning, CNN layers are the feature extractors)\n",
    "\n",
    "---\n",
    "\n",
    "## What are feature descriptors?\n",
    "\n",
    "features are just points on the image (referred to as \"keypoints\") We need a \"describe\" them to make it more useful. \n",
    "featrure descriptors - as the name suggest, describe the surroudings of a \"keypoint\" by capturing few vital ingredients around the keypoint. Think of it as vector (a collection) used to describe.  A good analogy would be a persons face.  \"eye\", \"color of the skin\", \"hair\" etc. are features.  color of the eye is \"brown\", color is \"black/brown/white\", \"Grey/Black/Blonde\" hair color are feature descriptors\n",
    "\n",
    "---\n",
    "\n",
    "## What is feature matching?\n",
    "\n",
    "Given 2 images, we want to compare the features between each other. If they are similar images, then we will end up high degree of \"matches\" else we wont. its simple. \n",
    "\n",
    "### Ok why is this useful?\n",
    "\n",
    "By tracking the features between 2 images, we can begin to understand if and how the objects in the images are \"moving\". By doing this over mulltiple images, we can \"track\" the moving objects within an image, movement of a camera w.r.t to the image etc. \n",
    "\n",
    "---\n",
    "\n",
    "The full working code is available [here](https://colab.research.google.com/drive/1rUtldAJRTyVpeB8zJmfWUEO3Bo16qifz#scrollTo=DVvf09RdXh9c) in google colab\n",
    "\n",
    "---\n",
    "\n",
    "## Use OpenCV to detect features and feature description\n",
    "\n",
    "```python\n",
    "\n",
    "#Load Different Image Pairs\n",
    "img1 = cv2.imread(\"images/image0.jpg\")\n",
    "img2 = cv2.imread(\"images/image1.jpg\")\n",
    "\n",
    "```\n",
    "![](https://abhisheksreesaila.github.io/blog/images/stereo/img1.jpg \"Image 1\")\n",
    "\n",
    "![](https://abhisheksreesaila.github.io/blog/images/stereo/img2.jpg \"Image 2\")\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "\n",
    "\n",
    "# INITIALIZE FAST DETECTOR\n",
    "fast = cv2.FastFeatureDetector_create()\n",
    "kp = fast.detect(img1,None)\n",
    "\n",
    "# BRIEF DESCRIPTOR\n",
    "brief = cv2.xfeatures2d.BriefDescriptorExtractor_create()\n",
    "kp1, des1 = brief.compute(img1, kp)\n",
    "\n",
    "# KEYPOINTS DRAWN\n",
    "img1_kp = cv2.drawKeypoints(img1, kp1, None, color=(0,255,0), flags=0)\n",
    "cv2_imshow(img1_kp)\n",
    "\n",
    "```\n",
    "\n",
    "![](https://abhisheksreesaila.github.io/blog/images/stereo/feature1.png \"Image 1 - Features\")\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "\n",
    "#REPEAT for the next image\n",
    "\n",
    "kp = fast.detect(img2,None)\n",
    "kp2, des2 = brief.compute(img2, kp)\n",
    "img2_kp = cv2.drawKeypoints(img2, kp2, None, color=(0,255,0), flags=0)\n",
    "cv2_imshow(img2_kp)\n",
    "\n",
    "```\n",
    "\n",
    "![](https://abhisheksreesaila.github.io/blog/images/stereo/feature2.png \"Image 2 - Features\")\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "\n",
    "#FLANN MATCHING\n",
    "FLANN_INDEX_KDTREE = 0\n",
    "index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "search_params = dict(checks = 50)\n",
    "\n",
    "flann = cv2.DescriptorMatcher_create(cv2.DescriptorMatcher_FLANNBASED)\n",
    "\n",
    "des1 = np.float32(des1)\n",
    "des2 = np.float32(des2)\n",
    "\n",
    "matches = flann.knnMatch(des1, des2, 2)\n",
    "\n",
    "# store all the good matches as per Lowe's ratio test.\n",
    "good = []\n",
    "for m,n in matches:\n",
    "    if m.distance < 0.7*n.distance:\n",
    "        good.append(m)\n",
    "\n",
    "draw_params = dict(matchColor = (0,255,0), # draw matches in green color\n",
    "                    singlePointColor = None,\n",
    "                    flags = 2)\n",
    "\n",
    "img_briefmatch = cv2.drawMatches(img1,kp1,img2,kp2,good,None,**draw_params)\n",
    "cv2_imshow(img_briefmatch)\n",
    "\n",
    "```\n",
    "\n",
    "![](https://abhisheksreesaila.github.io/blog/images/stereo/imatching.png \"Image Matching\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
