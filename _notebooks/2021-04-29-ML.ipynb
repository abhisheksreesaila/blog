{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Basic ML Topics\"\n",
    "\n",
    "> \"Everyday Learnings about ML. Random topics\"\n",
    "\n",
    "- toc: true\n",
    "- branch: master\n",
    "- badges: false\n",
    "- comments: true\n",
    "- categories: [Machine Learning]\n",
    "- hide: false\n",
    "- search_exclude: false\n",
    "- image: images/post-thumbnails/ml.png\n",
    "- metadata_key1: basics\n",
    "- metadata_key2: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding in CNNs\n",
    "\n",
    "Why use padding? In addition to the aforementioned benefit of keeping the spatial sizes constant after CONV, doing **this actually improves performance**. If the CONV layers were to not zero-pad the inputs and only perform valid convolutions, then the size of the volumes would reduce by a small amount after each CONV, **and the information at the borders would be “washed away” too quickly**\n",
    "\n",
    "[Reference](https://www.kaggle.com/c/data-science-bowl-2018/discussion/54426)\n",
    "\n",
    "\n",
    "## Linear vs Logistic Regression\n",
    "\n",
    "- In linear regression, **the outcome (dependent variable) is continuous**. It can have any one of an infinite number of possible values.\n",
    "\n",
    "*If X contains the area in square feet of houses,  (200, 300, 3000) Y contains the corresponding sale price of those houses, ($1000, $2000, $200,000)linear regression to predict selling price as a function of house size Y = f(x)*\n",
    "\n",
    "- In logistic regression, the outcome (dependent variable) has only a **limited number of possible values**. Logistic Regression is used when response variable is *categorical* in nature\n",
    "\n",
    "*If, instead, you wanted to predict, based on size, whether a house would sell for more than 200K, you would use logistic regression. The possible outputs are either Yes, the house will sell for more than 200K, or No, the house will not*\n",
    "\n",
    "## What is data augmentation?\n",
    "\n",
    "Overfitting is caused by having too few samples to learn from, rendering us unable to train a model able to generalize to new data. Given infinite data, our model would be exposed to every possible aspect of the data distribution at hand: we would never overfit. Data augmentation takes the approach of generating more training data from existing training samples, by \"augmenting\" the samples via a number of random transformations that yield believable-looking images. **The goal is that at training time, our model would never see the exact same picture twice**. This helps the model get exposed to more aspects of the data and generalize better. \n",
    "\n",
    "\n",
    "## What is Momentum?\n",
    "Momentum helps to speed the optimization process toward the minimum of the loss and get out of saddle points by adding a running average of previous gradients and use that average instead of the current batch of data. It forces the gradient descent toward the correct direction to the loss by making the convergence faster and by reducing oscillations\n",
    "\n",
    "## What is learning rate?\n",
    "\n",
    "The learning rate is how quickly or how slowly a network updates old parameters for new ones. By default, the learning rate is held constant, however, this way may cause some issues such as:\n",
    "\t• If the learning rate is too small, it will take a long time to reach the optimum, or maybe never reach it.\n",
    "\t• If the learning rate is too big, it will keep bouncing around the optimum.\n",
    "The optimization method may get stuck in shallow valleys!\n",
    "\n",
    "## What are auto-encoders?\n",
    "\n",
    "Autoencoders are a specific type of feedforward neural networks where the input is the same as the output. They compress the input into a lower-dimensional code and then reconstruct the output from this representation. The code is a compact “summary” or “compression” of the input, also called the latent-space representation.\n",
    "An autoencoder consists of 3 components: encoder, code and decoder. The encoder compresses the input and produces the code, the decoder then reconstructs the input only using this code.\n",
    "\n",
    "![](https://abhisheksreesaila.github.io/blog/images/general/autoencoder.png \"Auto Encoder\")\n",
    "\n",
    "To build an autoencoder we need 3 things: an encoding method, decoding method, and a loss function to compare the output with the target. Autoencoders are mainly a dimensionality reduction (or compression) algorithm with a couple of important properties\n",
    "\n",
    "- Data specific\n",
    "- Lossy\n",
    "- Unsupervised\n",
    "\n",
    "## Why batches in training ml model?\n",
    "\n",
    "Ideally, we'd like to use all our data for every step of training because that would give us a better sense of what we should be doing, but that's expensive. So, instead, we use a **different subset every time**. Doing this is cheap and has much of the same benefit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN\n",
    "\n",
    "![](https://abhisheksreesaila.github.io/blog/images/general/cnn-concepts.png \"CNN Concepts\")\n",
    "\n",
    "### Extra Read\n",
    "\n",
    "https://www.jeremyjordan.me/semantic-segmentation/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Epoch/Batch/Iterations\n",
    "\n",
    "Visual of EPOCH vs IERATIONS vs BATCHES \n",
    "\n",
    "![](https://abhisheksreesaila.github.io/blog/images/general/Epoch_Batch_Iter.png \"Basic Concepts\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  FAST AI Notes\n",
    "\n",
    "\n",
    "### Learning Rate\n",
    "\n",
    "![](https://abhisheksreesaila.github.io/blog/images/general/fastai_cycliclearningrate.png \"Cyclic Learning Rate\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
