{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Pytorch Topics\"\n",
    "\n",
    "> \"Everyday Learnings about ML. Random topics\"\n",
    "\n",
    "- toc: true\n",
    "- branch: master\n",
    "- badges: false\n",
    "- comments: true\n",
    "- categories: [Machine Learning]\n",
    "- hide: false\n",
    "- search_exclude: false\n",
    "- image: images/post-thumbnails/pytorch.png\n",
    "- metadata_key1: pytorch\n",
    "- metadata_key2: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch\n",
    "\n",
    "**Model Eval**\n",
    "\n",
    "```python\n",
    "\n",
    "    with torch.no_grad() :            # Switch off automatic differentation\n",
    "    output = learn.model.eval()(x)  # Evaluate models by not considering the batch norm, drop out layers etc.\n",
    "                                        ##Inference mode\n",
    "```\n",
    "\n",
    "**Detach**\n",
    "\n",
    "Remove a tensor (here \"x\") from the computational graph, which reduces memory foot print.\n",
    "\n",
    "Detaches and clones a tensor\n",
    "\n",
    "```python\n",
    "\n",
    "    x.detach()   \n",
    "    \n",
    "    y = x.detach().clone() \n",
    "\n",
    "```\n",
    "\n",
    "**Model**\n",
    "\n",
    "```python\n",
    "\n",
    "   learn.model[0]    #model is made up of \"sequential\" function contaniner.\n",
    "    \n",
    "                     # sequential(sequential()) # neural networks stacked on top of each other\n",
    "    \n",
    "    \n",
    "\n",
    "```\n",
    "\n",
    "**einsum**\n",
    "\n",
    "einsum or Enstein summation is basically short end notation to express actions on tensors (or typically matrices) like transpose, multiplication, sum, dot product etc. \n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "torch.einsum ('ab,bc -> ac', A, B)\n",
    "\n",
    "'ab,bc -> ac'  ==> notation.  a,b,c,d are dimensions. \",\" means multiply \n",
    "\n",
    "You are telling that take 2 inputs of dimensions ab and bc and generate an output that gives ac. \n",
    "\n",
    "A,B ==> matrices\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "**Class Activation Map**\n",
    "\n",
    "**Areas used to determine class = activations of the last layer of conv * weights of the fully connected layer**\n",
    "\n",
    "\n",
    "- Last layer of activation (before the fully connected layer) shows where the model is focusing.  \n",
    "- Needs a global average pooling layer in the network (such as RESNET)\n",
    "\n",
    "- Why before global max pooling layer? \n",
    "The global max poolng layer unlike the other maxpool layers will squash all features into 1 linear vector. \n",
    "The max pooling layer before the fully connected layer will squash all local activations, normalize them and feed them to FC. Until then you will have localized features that model is looking at. In other words, you will have location of the image where the model is focused on.  \n",
    "\n",
    "- How does dot product help?\n",
    "\n",
    "![](https://abhisheksreesaila.github.io/blog/images/general/cnn1.png \"CNN - Training Phase\")\n",
    "\n",
    "Learns Features. Stores Weights\n",
    "\n",
    "![](https://abhisheksreesaila.github.io/blog/images/general/cnn2.png \"CNN - Inference Phase\")\n",
    "\n",
    "Use those weights with the activations and figure which one to focus on, which one to omit. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drawbacks\n",
    "\n",
    "- The architecture needs to have global max pooling layer. Only then can we take the layer before that.\n",
    "- The method can only look at the final layer of the CNN and show why the model predicted what it did.  It cannot show any later prior.\n",
    "- These drawbacks are addressed by the GRAD GAM described below\n",
    "\n",
    "\n",
    "- Calculate the gradients by running .backward() function. (Pytorch does not store them, hence need to calc again during inference)\n",
    "- Average the GRADIENTS of the feature maps of the last conv layer (= weights)\n",
    "- Multiply WEIGHTS vs ACTICATIONS (as in CAM) to get the CAM Map to display.\n",
    "\n",
    "### Pros\n",
    " - over comes all the issues of vanilla CAM\n",
    " - works for any images tasks (classification, segmentation, vQA)\n",
    " \n",
    "### Cons\n",
    " - cannot locate mulitple objects within the images.\n",
    " \n",
    " \n",
    "![](https://abhisheksreesaila.github.io/blog/images/general/cnn3.png \"CNN - GRAD CAM\")\n",
    "\n",
    "- Why gradients equal same size as activation maps? \n",
    "\n",
    "The gradient is calculated for each pixel in the feature map. For example, if the activation map is 512 x 7 x 7, then then the number of graidents are also 512 x 7 x 7\n",
    "\n",
    "- Why averaging gradients yields weights?\n",
    "\n",
    "CAM uses WEIGHTS at the Fully Connected layer to choose the \"feature maps\" that is more relevant and squash the ones which are not.  So it is highly dependent of (CONV Layer => Global Average Pooling Layer ==> FC Layer) network.  GRAD-CAM uses this concept by make its more general.\n",
    "  It uses GRADIENTS to provide the weights. We use the GRADIENTS in the last conv layer, do the global average pooling   ourselves, and now we have our weights!  we dont have to depend on specific GAP layer nor the weights of the fully connected layer. GRADIENTS provide a good enough \"weighting mechanism\" to pick the feature map that is relevant and squash the one which we dont.\n",
    "<font size=\"3\">  \n",
    "Deep neural networks as well act as information distillation pipeline where the input image is being converted to a domain which is visually less interpretable (by removing irrelevant information) but mathematically useful for convnet to make a choice from the output classes in its last layer\n",
    "</font>\n",
    " \n",
    " \n",
    "# References\n",
    " \n",
    " https://glassboxmedicine.com/2020/05/29/grad-cam-visual-explanations-from-deep-networks/\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matplot Lib Basics\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "fig, ax = plt.subplot(3,2,figsize=(5, 5)) \n",
    "\n",
    "# Rows = 3; Columns=2; Total = 5 plots === Set the figure size to 5 inches to 5 inches\n",
    "# Note that the size is defined in inches, not pixels\n",
    "\n",
    "axs[0, 0].hist(data[0])   #1st axis\n",
    "axs[1, 0].scatter(data[0], data[1])  #2nd axis\n",
    "axs[0, 1].plot(data[0], data[1])  #3rd axis\n",
    "axs[1, 1].hist2d(data[0], data[1])  #4th axis\n",
    "\n",
    "plt.show()  # show the plot\n",
    "\n",
    "ax.imshow() # show the image\n",
    "\n",
    "#interpolation = use known data at unknown places (like extrapolate, interpolate)\n",
    " \n",
    "```\n",
    "[Check out various types here of interpolation here](https://matplotlib.org/stable/gallery/images_contours_and_fields/interpolation_methods.html)\n",
    "\n",
    "\n",
    "## Python Decode Function\n",
    "\n",
    "When you encode using a class (string class, data loader class etc.). you can use decode to undo it. Useful in bring back the image to its original form to display while \"intrepreting\" the test results.\n",
    "\n",
    "- IMAGE ==> RESIZE ==> ENCODE (normalize to image net stats or something similar) ==> Output\n",
    "\n",
    "- Output ===> DECODE ==> Original image (but still includes the resize)  ==> Display (-able)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
