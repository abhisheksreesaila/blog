{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Optical Flow\"\n",
    "\n",
    "> \"Understand the concept of optical flow\"\n",
    "\n",
    "- toc: true\n",
    "- branch: master\n",
    "- badges: false\n",
    "- comments: true\n",
    "- categories: [Computer Vision]\n",
    "- hide: false\n",
    "- search_exclude: false\n",
    "- image: images/post-thumbnails/of.png\n",
    "- metadata_key1: notes\n",
    "- metadata_key2: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous blog post [here](https://ablearn.io/computer%20vision/2021/12/05/Image-Features-Understanding.html) we understand basics of \"features\" in an image. One of the most popular usecases for such tracking of features between 2 images would be detection motion of objects within the image.\n",
    "\n",
    "\n",
    "# Optical Flow vs Motion Field\n",
    "\n",
    "Let's try to understand the terms first. \n",
    "\n",
    "We take a object in the world (represented in the world coordinates) and project them onto a image plane. As the object moves in the world, we want to track it in the image plane.  This tracking is called *MOTION FIELD*\n",
    "\n",
    "It is not possible to directly measure motion field since all we have in the image are brightness patterns. \n",
    "\n",
    "Using the variation in brightness patterns between consecutives images can help you track the object(s). This is called *OPTICAL FLOW*. \n",
    "\n",
    "OPTICAL FLOW indicates MOTION of objects in most of the cases. But there could be images with \"illusions\" that changes brightness patterns but does not correspond to \"object movement\" in them\". For eg. see below. \n",
    "\n",
    "---\n",
    "The object is moving but there is no change in brightness pattern. So very hard to detect motion\n",
    "\n",
    "![](https://abhisheksreesaila.github.io/blog/images/stereo/op.png \"Motion Field without Optical Flow\")\n",
    "\n",
    "Here the light source is moving and hence the brightness pattern is changing, but there is no motion. \n",
    "\n",
    "![](https://abhisheksreesaila.github.io/blog/images/stereo/mf.png \"Optical Flow without Motion Field\")\n",
    "\n",
    "\n",
    "\n",
    "# Optical Flow Assumptions\n",
    "\n",
    "Before we attempt to compute the optical flow. Lets assume 2 things\n",
    "\n",
    "1. Brightness of a image point remains constant over time. Since we will be dealing with images taken in quick succession (in the order of milliseconds) this is a safe assumption. \n",
    "\n",
    "![](https://abhisheksreesaila.github.io/blog/images/stereo/opticalfloweq.png \"Object moving in an image\")\n",
    "\n",
    "Mathemtically written as \n",
    "\n",
    "> $I(x+ \\delta x, y+ \\delta y, t+ \\delta t) = I(x, y, t)$\n",
    "\n",
    "2. Displacement ($ \\delta x, \\delta y $) is very small\n",
    "\n",
    "\n",
    "# Optical Flow Estimation\n",
    "\n",
    "We will look at the \"classic\" methods of optical flow estimation. OPENCV provides a lot of inbuilt methods to compute them\n",
    "\n",
    "## Sparse Optical Flow\n",
    "\n",
    "We compute optical flow by extracting important features only. We will use the \"feature extraction\" techniques from the [earlier blog post](https://ablearn.io/computer%20vision/2021/12/05/Image-Features-Understanding.html) and determine the optical flow. Sparse Optical Flow is fast but not very accurate.\n",
    "\n",
    "The code is available here.\n",
    "\n",
    "For this exercise we use the dataset from [KITTI Vision Car Dataset](http://www.cvlibs.net/datasets/kitti/) available here.\n",
    "\n",
    "### Load Images\n",
    "\n",
    "```python\n",
    "\n",
    " #Load two consecutive images\n",
    "img1 = cv2.imread(\"images/000199_10.png\")\n",
    "img2 = cv2.imread(\"images/000199_11.png\")\n",
    "\n",
    "#Convert both images to grayscale\n",
    "## We need the gray scale images for the functions below\n",
    "gray1 = cv2.cvtColor(im1, cv2.COLOR_BGR2GRAY)\n",
    "gray2 = cv2.cvtColor(im2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "### Feature Extraction\n",
    "\n",
    "We will use the Shi-Tomasi corner detector from OPENCV. To understand how to define the parameters, you can use [this link](https://docs.opencv.org/master/dd/d1a/group__imgproc__feature.html#ga1d6bb77486c8f92d79c8793ad995d541) \n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "feature_params = dict(maxCorners = 500, qualityLevel = 0.01, minDistance = 10,blocksize = 3)\n",
    "prev = cv2.goodFeaturesToTrack(gray1,feature_params['maxCorners'], feature_params[\"qualityLevel\"], feature_params[\"minDistance\"], feature_params[\"blocksize\"])\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "By obtaining the features and drawing them, we obtain the following similar to below.\n",
    "\n",
    "\n",
    "![](https://abhisheksreesaila.github.io/blog/images/stereo/sparse-optical-flow-fe.png \"Corners detected (features) extracted from the image\")\n",
    "\n",
    "\n",
    "\n",
    "### Run Optical Flow\n",
    "\n",
    "Launch the Lucas Kanade Optical Flow algorithm (*cv2.calcOpticalFlowPyrLK*) with the features and both grayscale images.\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "\n",
    "lk_params = dict(winSize = (15, 15) , maxLevel = 2 , criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
    "\n",
    "# img1 and img2 = images\n",
    "# prev = features of the first image\n",
    "next, status, error = cv2.calcOpticalFlowPyrLK(img1, img2, prev, None, winSize=lk_params[\"winSize\"], maxLevel=lk_params[\"maxLevel\"],criteria=lk_params[\"criteria\"])\n",
    "\n",
    "\n",
    "#Store the Matches (status=1 means a match)\n",
    "good_old = prev[status ==1]\n",
    "good_new = next[status ==1]\n",
    "\n",
    "```\n",
    "\n",
    "### Visualize \n",
    "\n",
    "Go through each matched feature, and draw it on the second image\n",
    "\n",
    "![](https://abhisheksreesaila.github.io/blog/images/stereo/opticalflowop1.png \"Optical Flow Output\")\n",
    "\n",
    "Here is a video for a more intuitive show of the optical flow\n",
    "\n",
    "> youtube: https://youtu.be/mNGbiAWzRSw\n",
    "\n",
    "\n",
    "## Deep Optical Flow\n",
    "\n",
    "In Deep Optical Flow we compute optical flow for every pixel. Just to clarify, we dont use deep learning (yet!) \n",
    "\n",
    "The full code is available here\n",
    "\n",
    "1. Load the images using openv functions as before\n",
    "\n",
    "###  Calculate Optical Flow\n",
    "\n",
    "Wait! no feature extraction? Yes thats correct. Since we are runnning optical flow on every pixel, we can directly use the corresponding opencv function. If you'd like to understand how to tweak the parameters, you can visit this [link](https://docs.opencv.org/3.0-beta/modules/video/doc/motion_analysis_and_object_tracking.html#calcopticalflowfarneback)\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "flow = cv2.calcOpticalFlowFarneback(gray1, gray2, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "\n",
    "print(\"Shapes Gray & Flow\")\n",
    "print(gray1.shape)\n",
    "print(flow.shape)\n",
    "print(\" \")\n",
    "\n",
    "\"\"\"\n",
    "Shapes Gray & Flow\n",
    "(376, 1241)\n",
    "(376, 1241, 2)\n",
    "\"\"\"\n",
    "\n",
    "```\n",
    "> Important: Here flow is a 2d matrix which has the same shape as input but contains the \"$ \\delta x $\" (represented by U) and \"$ \\delta y$\" (represented by V) for each point (X,Y).  \n",
    "\n",
    "![](https://abhisheksreesaila.github.io/blog/images/stereo/flow-shown.png \"Flow Matrix Shown Visually\")\n",
    "\n",
    "Accorinding to OPENCV, we get U and V in cartesian coordinates. To show this visually it will be good if we can find \"magnitude\" and \"direction\".  then we will intuitively know how much it moved and in what angle. Catesian system does not provide that.  Luckily for us, \"polar system coordinates\" gives us what we want. \n",
    "\n",
    "```python\n",
    "\n",
    "magnitude, angle = cv2.cartToPolar(flow[...,0],flow[...,1])\n",
    "\n",
    "```\n",
    "\n",
    "### Setting the HUE Value\n",
    "\n",
    "\n",
    "### Converting HUE to RGB\n",
    "\n",
    "\n",
    "### Video\n",
    "\n",
    "> youtube: https://youtu.be/QkuOY7AObh8\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[Optical Flow Definition](https://www.youtube.com/watch?v=lnXFcmLB7sM&t=92s)\n",
    "\n",
    "[Workshops from Think Autonomous Course](https://courses.thinkautonomous.ai/optical-flow)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
