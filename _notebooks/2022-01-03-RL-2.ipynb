{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Reinforcement Learning - Part 2\"\n",
    "\n",
    "> \"RL concepts in 1 place\"\n",
    "\n",
    "- toc: true\n",
    "- branch: master\n",
    "- badges: false\n",
    "- comments: true\n",
    "- categories: [Machine Learning]\n",
    "- hide: false\n",
    "- search_exclude: false\n",
    "- image: images/post-thumbnails/rl.png\n",
    "- metadata_key1: notes\n",
    "- metadata_key2: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q Learning\n",
    "\n",
    "The purpose of this blog post is to learn Q learning using code using the theory from the last blog post. \n",
    "- You can find the previous one here.  [Reinforcement Learning - Basics](https://ablearn.io/machine%20learning/2021/08/02/RL.html)\n",
    "\n",
    "We will use openAI gym to apply Q learning for a FrozenLakeNoSlip environment I will explain the important parts of the code but the full code can be downloaded [here](https://colab.research.google.com/drive/1PbJnJonr8VWaOMnbosgxHbDYX4kwntMp?usp=sharing)\n",
    "\n",
    "---\n",
    "\n",
    "*Initialize the OPENAI GYM. Print the action and states (observation space) just to get an idea as to how many actions and states we are dealing with*\n",
    "\n",
    "```python\n",
    "\n",
    "try:\n",
    "    register(\n",
    "        id='FrozenLakeNoSlip-v0',\n",
    "        entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "        kwargs={'map_name' : '4x4', 'is_slippery':False},\n",
    "        max_episode_steps=100,\n",
    "        reward_threshold=0.78, # optimum = .8196\n",
    "    )\n",
    "except:\n",
    "    pass\n",
    "\n",
    "env_name = \"FrozenLakeNoSlip-v0\"\n",
    "env = gym.make(env_name)\n",
    "print(\"Observation space:\", env.observation_space)\n",
    "print(\"Action space:\", env.action_space)\n",
    "\n",
    "\"\"\" Output\n",
    "#Observation space: Discrete(16)\n",
    "#Action space: Discrete(4)\n",
    "\"\"\"\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Initialize\n",
    "\n",
    "Initialize parameters such as \"discount factor\", \"learning rate\" and \"Q table\"\n",
    "\n",
    "```python\n",
    "\n",
    "# Create a class\n",
    "class QAgent():\n",
    "    def __init__(self, env, discount_rate=0.97,learning_rate=0.01):\n",
    "          super().__init__(env)\n",
    "            self.state_size = env.observation_space.n  # init state size\n",
    "            self.eps = 1.0\n",
    "            self.discount_rate = discount_rate  #  discount factor\n",
    "            self.learning_rate = learning_rate  #  learning rate\n",
    "            self.build_model()     # Init Q-Table. Just random values for begin with      \n",
    "\n",
    "    def build_model(self):    # random Q-Table with rows(state size) X columns (action size)\n",
    "         self.q_table = 1e-4*np.random.random([self.state_size, self.action_size])\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Choosing Action\n",
    "\n",
    "```python  \n",
    "def get_action(self, state):\n",
    "    \n",
    "# get the Q-value for a state from the \"Q table\". You will get 4 possible values , since there 4 actions.  (q1, q2, q3, q4)\n",
    "    q_state = self.q_table[state]  \n",
    "    \n",
    "# get the max of the  (q1, q2, q3, q4). That represents the \"BEST ACTION\". Its called \"greedy approach\" since we are always trying to priortize \"MAX\". however there is a inherent problem with choosing \"MAX\". See below explanation. \n",
    "    action_greedy = np.argmax(q_state)\n",
    " \n",
    "# Choose \"random\" action based on a arbritary criteria (epsilon. you can call it anything. its totally arbritary. we are just trying to avoid max all the time. Thats all)\n",
    "# If the random.random() < epsilon, choose random, else choose greedy\n",
    "    action_random = super().get_action(state) \n",
    "    return action_random  if random.random() < self.eps else action_greedy\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## TR Update\n",
    "\n",
    "This is the main function which updates the Q table based on the bellman equation. \n",
    "\n",
    "From the last blog post\n",
    "\n",
    "> Q(s,a) =  Q(s,a) + $ \\alpha $ [$ R(s,a,s^1)  + \\gamma  max_{a'} Q(s',a')$ - Q(s,a)]\n",
    "\n",
    "One can read this as \n",
    "- **Qvalue = Current Qvalue +  learning rate (expected future reward - current Q value)**\n",
    "- **Qvalue = Current Qvalue +  learning rate (BELLMAN ERROR)**\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "def train(self, experience):\n",
    "    state, action, next_state, reward, done = experience\n",
    "    q_next = self.q_table[next_state]\n",
    "    q_next = np.zeros([self.action_size]) if done else q_next\n",
    "     \n",
    "    #sample = [R + Discount * max(q values)] \n",
    "    sample = reward + self.discount_rate * np.max(q_next) \n",
    "    \n",
    "     # our update function for q(s,a) = q(s,a) + learning rate(bellman error)\n",
    "                                                             # bellman error : sample - q(s,a))\n",
    "    self.q_table[state, action] =  self.q_table[state, action] + self.learning_rate * (sample - self.q_table[state, action])\n",
    "    \n",
    "    if done:\n",
    "    self.eps = self.eps * 0.99\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "## Bringing it all together\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "total_reward = 0\n",
    "for ep in range(100):  # no of episodes\n",
    "  state = env.reset()  # reset, start from the beginning\n",
    "  done = False\n",
    "  while not done:\n",
    "    action = agent.get_action(state)   # pass the \"current state\" and get a \"random\" or \"greedy action\"\n",
    "    next_state, reward, done, info =  env.step(action) # step through the env. the openaigym will do the rest. it will give you the reward, tell you if its complete and give other meta info\n",
    "   \n",
    "    agent.train((state, action, next_state, reward, done))\n",
    "    state = next_state        # set the current state to the next state\n",
    "    total_reward += reward   # add the rewards\n",
    "    print(\"s:\", state, \"a:\", action)\n",
    "    print(\"Episode: {}, Total reward: {}, eps: {}\".format(ep,total_reward,agent.eps))\n",
    "    env.render()\n",
    "    print(agent.q_table)\n",
    "    time.sleep(0.05)\n",
    "    clear_output(wait=True)\n",
    "\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  $V_{i+1}(s)$ = argmax $ \\sum \\limits_{a\\in A} $ P($s^{1}|s,a$) * [ $ R(s,a,s^1)  + \\gamma   V_{i}(s^1)$] \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
