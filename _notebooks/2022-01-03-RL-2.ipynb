{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"What is Q Learning?\"\n",
    "\n",
    "> \"Understanding Q learning using theory and code\"\n",
    "\n",
    "- toc: true\n",
    "- branch: master\n",
    "- badges: false\n",
    "- comments: true\n",
    "- categories: [Reinforcement Learning]\n",
    "- hide: false\n",
    "- search_exclude: false\n",
    "- image: images/post-thumbnails/rl.png\n",
    "- metadata_key1: notes\n",
    "- metadata_key2: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q Learning\n",
    "\n",
    "The purpose of this blog post is to learn Q learning using code using the theory from the last blog post. \n",
    "- You can find the previous one here.  [Reinforcement Learning - Basics](https://ablearn.io/machine%20learning/2021/08/02/RL.html)\n",
    "\n",
    "We will use openAI gym to apply Q learning for a FrozenLakeNoSlip environment I will explain the important parts of the code but the full code can be downloaded [here](https://colab.research.google.com/drive/1PbJnJonr8VWaOMnbosgxHbDYX4kwntMp?usp=sharing)\n",
    "\n",
    "---\n",
    "\n",
    "*Initialize the OPENAI GYM. Print the action and states (observation space) just to get an idea as to how many actions and states we are dealing with*\n",
    "\n",
    "```python\n",
    "\n",
    "try:\n",
    "    register(\n",
    "        id='FrozenLakeNoSlip-v0',\n",
    "        entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "        kwargs={'map_name' : '4x4', 'is_slippery':False},\n",
    "        max_episode_steps=100,\n",
    "        reward_threshold=0.78, # optimum = .8196\n",
    "    )\n",
    "except:\n",
    "    pass\n",
    "\n",
    "env_name = \"FrozenLakeNoSlip-v0\"\n",
    "env = gym.make(env_name)\n",
    "print(\"Observation space:\", env.observation_space)\n",
    "print(\"Action space:\", env.action_space)\n",
    "\n",
    "\"\"\" Output\n",
    "#Observation space: Discrete(16)\n",
    "#Action space: Discrete(4)\n",
    "\"\"\"\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Initialize\n",
    "\n",
    "Initialize parameters such as \"discount factor\", \"learning rate\" and \"Q table\"\n",
    "\n",
    "```python\n",
    "\n",
    "# Create a class\n",
    "class QAgent():\n",
    "    def __init__(self, env, discount_rate=0.97,learning_rate=0.01):\n",
    "          super().__init__(env)\n",
    "            self.state_size = env.observation_space.n  # init state size\n",
    "            self.eps = 1.0\n",
    "            self.discount_rate = discount_rate  #  discount factor\n",
    "            self.learning_rate = learning_rate  #  learning rate\n",
    "            self.build_model()     # Init Q-Table. Just random values for begin with      \n",
    "\n",
    "    def build_model(self):    # random Q-Table with rows(state size) X columns (action size)\n",
    "         self.q_table = 1e-4*np.random.random([self.state_size, self.action_size])\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Choosing Action\n",
    "\n",
    "```python  \n",
    "def get_action(self, state):\n",
    "    \n",
    "# get the Q-value for a state from the \"Q table\". You will get 4 possible values , since there 4 actions.  (q1, q2, q3, q4)\n",
    "    q_state = self.q_table[state]  \n",
    "    \n",
    "# get the max of the  (q1, q2, q3, q4). That represents the \"BEST ACTION\". Its called \"greedy approach\" since we are always trying to priortize \"MAX\". however there is a inherent problem with choosing \"MAX\". See below explanation. \n",
    "    action_greedy = np.argmax(q_state)\n",
    " \n",
    "# Choose \"random\" action based on a arbritary criteria (epsilon. you can call it anything. its totally arbritary. we are just trying to avoid max all the time. Thats all)\n",
    "# If the random.random() < epsilon, choose random, else choose greedy\n",
    "    action_random = super().get_action(state) \n",
    "    return action_random  if random.random() < self.eps else action_greedy\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## TR Update\n",
    "\n",
    "This is the main function which updates the Q table based on the bellman equation. \n",
    "\n",
    "From the last blog post\n",
    "\n",
    "> Q(s,a) =  Q(s,a) + $ \\alpha $ [$ R(s,a,s^1)  + \\gamma  max_{a'} Q(s',a')$ - Q(s,a)]\n",
    "\n",
    "One can read this as \n",
    "- **Qvalue = Current Qvalue +  learning rate (expected future reward - current Q value)**\n",
    "- **Qvalue = Current Qvalue +  learning rate (BELLMAN ERROR)**\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "def train(self, experience):\n",
    "    state, action, next_state, reward, done = experience\n",
    "    q_next = self.q_table[next_state]\n",
    "    q_next = np.zeros([self.action_size]) if done else q_next\n",
    "     \n",
    "    #sample = [R + Discount * max(q values)] \n",
    "    sample = reward + self.discount_rate * np.max(q_next) \n",
    "    \n",
    "     # our update function for q(s,a) = q(s,a) + learning rate(bellman error)\n",
    "                                                             # bellman error : sample - q(s,a))\n",
    "    self.q_table[state, action] =  self.q_table[state, action] + self.learning_rate * (sample - self.q_table[state, action])\n",
    "    \n",
    "    if done:\n",
    "    self.eps = self.eps * 0.99\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "## Bringing it all together\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "total_reward = 0\n",
    "for ep in range(100):  # no of episodes\n",
    "  state = env.reset()  # reset, start from the beginning\n",
    "  done = False\n",
    "  while not done:\n",
    "    action = agent.get_action(state)   # pass the \"current state\" and get a \"random\" or \"greedy action\"\n",
    "    next_state, reward, done, info =  env.step(action) # step through the env. the openaigym will do the rest. it will give you the reward, tell you if its complete and give other meta info\n",
    "   \n",
    "    agent.train((state, action, next_state, reward, done))\n",
    "    state = next_state        # set the current state to the next state\n",
    "    total_reward += reward   # add the rewards\n",
    "    print(\"s:\", state, \"a:\", action)\n",
    "    print(\"Episode: {}, Total reward: {}, eps: {}\".format(ep,total_reward,agent.eps))\n",
    "    env.render()\n",
    "    print(agent.q_table)\n",
    "    time.sleep(0.05)\n",
    "    clear_output(wait=True)\n",
    "\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limitations of Q Learning\n",
    "\n",
    "Q learning is good where the finite number of states and actions AND there are relatively small.  If there are too many states and actions, the number of possibilities are very high and there wont be sufficient memory to hold the table. But the more important concern is that some problems cannot be easily expressed as distinct states and actions.\n",
    "\n",
    "In the example from FROZEN LAKE, the states were SAFE, FROZEN, HOLE, GOAL. The actions were up, down, left and right.\n",
    "\n",
    "Lets say in the game pac-man such as these, the states are very \"similar\" and yet by using a q table it will be shown as 2 different states, and hence we can conclude that it cannot be easily translated. Here state is not a \"single unique distinct\" entity rather it is more characterized as a \"collection\" of features.  \n",
    "\n",
    "![](https://abhisheksreesaila.github.io/blog/images/rl/pac1.png \"Pacman State 1\")\n",
    "\n",
    "![](https://abhisheksreesaila.github.io/blog/images/rl/pac2.png \"Pacman State 2\")\n",
    "\n",
    "\n",
    "Useful way to think about it a set of odd numbers.  S={1,3,5}  S1={7,9,11}. Both represent odd numbers. They are similar and represented as a collection. Both \"S\" and \"S1\" are states represented by \"ODD\" numbers but they contain different values in them. Just like the pac-man example, we need a \"same/similar\" action to be performed on them but looking at them individually value by value we fail to capture the \"pattern\" or the \"essence\"\n",
    "\n",
    "\n",
    "The point here is we need to represent \"state\" as a collection and capture the essence rather than treating it as a single distinct entity. It is in cases like this, we take the help of a function which take in a bunch of features, \"understand their essence/patterns\" and spits out a Q-values for each action. See below.\n",
    "\n",
    "![](https://abhisheksreesaila.github.io/blog/images/rl/univeral_function_approx.png \"Univeral Function Approximator\")\n",
    "\n",
    "\n",
    "The best known universal function approximators today are none other than \"neural networks\". We can employ neural networks to learn the Q-values for different states. Neural Networks need some basic components to work effectively.\n",
    "Lets see what they are and how we get can them.\n",
    "\n",
    " - **Input** : Bunch of features\n",
    " - **Output** : Q-Values for each action (of course, like before, we will chose the one with the highest value)\n",
    " - **Loss Function** : We can take the Bellman Error and Minimize them. However, the loss function we chose has to be \"differentiable\". In other words, we should be apply backpropogation to transfer the loss from the \"output layer\" all the way back to the first hidden layer to update the \"weights or parameters $ \\phi $\" in the network. Lets see how we can convert \"bellman error\" to a differentiable function after which we are set to use the \"neural networks\" in our world of \"Q learning\".\n",
    " \n",
    ">Note: Since we combined \"deep learning\" with \"Q learning\" - we call it. \"Deep Q Learning\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our incremental update formula is \n",
    "\n",
    "> Q(s,a) =  Q(s,a) + $ \\alpha $ [$ R(s,a,s^1)  + \\gamma  max_{a'} Q(s',a')$ - Q(s,a)]\n",
    "\n",
    "Consider y = $ R(s,a,s^1)  + \\gamma  max_{a'} Q(s',a')$\n",
    "\n",
    "> Q(s,a) =  Q(s,a) - $ \\alpha $[Q(s,a) - y] \n",
    "\n",
    "Just like the mean squared loss, we can square the bellman error to punish the large variances.\n",
    "\n",
    "> Q(s,a) =  Q(s,a) - $ \\alpha [Q(s,a) - y]^2 $\n",
    "\n",
    "Lets apply differential function w.r.t to $ \\phi $ (parameters) to both sides of the equation\n",
    "\n",
    "$ \\phi  = \\phi - \\alpha \\frac{\\partial (Q(s,a) - y)^2}{\\partial \\phi} $\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problems with DQN\n",
    "\n",
    "1. Corelated samples\n",
    " - Unlike the SGD, the samples chosen during \"Q learning\" are close to each other and can overfit and does not converge to the global minima\n",
    " - Solution : maintain a queue of past experiences;  use them to update the gradients. Breaks the close corelations\n",
    " \n",
    "2. Moving Targets\n",
    "  - Use 2 NN\n",
    "  - Compute the labels using the TARGET Network. fix it for a while\n",
    "  - Q Network will work as normal\n",
    "  - Stablizes the training for a certain duration\n",
    " \n",
    "- Mentioned in the paper Mink et al.  \n",
    "- shown to solve a lot of problems\n",
    "\n",
    " \n",
    " \n",
    " \n",
    " \n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.youtube.com/watch?v=RaIcFiNqP-0&list=PLYgyoWurxA_8ePNUuTLDtMvzyf-YW7im2&index=10\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
