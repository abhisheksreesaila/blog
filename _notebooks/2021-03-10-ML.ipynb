{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"rand machine topics\"\n",
    "\n",
    "> \"Everyday Learnings about ML. Random topics\"\n",
    "\n",
    "- toc: true\n",
    "- branch: master\n",
    "- badges: false\n",
    "- comments: true\n",
    "- categories: [machine learning]\n",
    "- hide: false\n",
    "- search_exclude: false\n",
    "- image: images/post-thumbnails/pytorch.png\n",
    "- metadata_key1: machine learning\n",
    "- metadata_key2: pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch\n",
    "\n",
    "**Model Eval**\n",
    "\n",
    "```python\n",
    "\n",
    "    with torch.no_grad() :            # Switch off automatic differentation\n",
    "    output = learn.model.eval()(x)  # Evaluate models by not considering the batch norm, drop out layers etc.\n",
    "                                        ##Inference mode\n",
    "```\n",
    "\n",
    "**Detach**\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "    x.detach()   #remove a tensor (here \"x\") from the computational graph, which reduces memory foot print\n",
    "    \n",
    "    y = x.detach().clone() #detaches and clones a tensor\n",
    "\n",
    "```\n",
    "\n",
    "**Model**\n",
    "\n",
    "```python\n",
    "\n",
    "   learn.model[0]    #model is made up of \"sequential\" function contaniner.\n",
    "    \n",
    "                     # sequential(sequential()) # neural networks stacked on top of each other\n",
    "    \n",
    "    \n",
    "\n",
    "```\n",
    "\n",
    "**einsum**\n",
    "\n",
    "einsum or Enstein summation is basically short end notation to express actions on tensors (or typically matrices) like transpose, multiplication, sum, dot product etc. \n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "torch.einsum ('ab,bc -> ac', A, B)\n",
    "\n",
    "'ab,bc -> ac'  ==> notation.  a,b,c,d are dimensions. \",\" means multiply \n",
    "\n",
    "You are telling that take 2 inputs of dimensions ab and bc and generate an output that gives ac. \n",
    "\n",
    "A,B ==> matrices\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "**Class Activation Map**\n",
    "\n",
    "**Areas used to determine class = activations of the last layer of conv * weights of the fully connected layer**\n",
    "\n",
    "\n",
    "- Last layer of activation (before the fully connected layer) shows where the model is focusing.  \n",
    "- Needs a global average pooling layer in the network (such as RESNET)\n",
    "\n",
    "- Why before global max pooling layer? \n",
    "The global max poolng layer unlike the other maxpool layers will squash all features into 1 linear vector. \n",
    "The max pooling layer before the fully connected layer will squash all local activations, normalize them and feed them to FC. Until then you will have localized features that model is looking at. In other words, you will have location of the image where the model is focused on.  \n",
    "\n",
    "- How does dot product help?\n",
    "\n",
    "![](https://abhisheksreesaila.github.io/blog/images/general/cnn1.png \"CNN - Training Phase\")\n",
    "\n",
    "Learns Features. Stores Weights\n",
    "\n",
    "![](https://abhisheksreesaila.github.io/blog/images/general/cnn2.png \"CNN - Inference Phase\")\n",
    "\n",
    "Use those weights with the activations and figure which one to focus on, which one to omit. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drawbacks\n",
    "\n",
    "- The architecture needs to have global max pooling layer. Only then can we take the layer before that.\n",
    "- The method can only look at the final layer of the CNN and show why the model predicted what it did.  It cannot show any later prior.\n",
    "- These drawbacks are addressed by the GRAD GAM described below\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
