{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Reinforcement Learning\"\n",
    "\n",
    "> \"RL concepts in 1 place\"\n",
    "\n",
    "- toc: true\n",
    "- branch: master\n",
    "- badges: false\n",
    "- comments: true\n",
    "- categories: [Machine Learning]\n",
    "- hide: false\n",
    "- search_exclude: false\n",
    "- image: images/post-thumbnails/rl.png\n",
    "- metadata_key1: notes\n",
    "- metadata_key2: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Terms\n",
    "\n",
    "- Unlike the supervised or unsuperivsed learning algorithms, it is used in dynamic environment\n",
    "\n",
    "- There are 2 entities involved: \n",
    "  - AGENT : Software involved in getting input and executing actions.\n",
    "  - Environment : House that the agent is trying to execute the actions. \n",
    "\n",
    "-  Input\n",
    "    - Take a set of observations (or states)\n",
    "    \n",
    "- Process (or policy)\n",
    "    - Figures out the right set of actions\n",
    "    - If we used \"deep learning\" for this, then it becomes **Deep Reinforcement Learning**\n",
    "    - Takes an input \"oldSTATE\" and outputs an \"ACTION\" that goes to a \"newSTATE\"\n",
    "    - Think of it as a \"plan\". If you \"plan\" to take the \"freeway\" today to avoid internal city roads - thats a plan\n",
    "    - if you are planning to city roads to avoid \"congestion\" on the freeway - thats a plan (or policy). you are optmizing \"time\" in both but following different policy\n",
    "    \n",
    " \n",
    "- Output\n",
    "    - Right set of commands to follow\n",
    "    \n",
    "- Feedback\n",
    "    - It the output received positive feedback, great, remember and repeat when possible\n",
    "    - If not, change to another action. \n",
    " \n",
    "![](https://abhisheksreesaila.github.io/blog/images/rl/rl-flow.png \"Reinforcement Learning\")\n",
    "\n",
    "\n",
    "# Process\n",
    "\n",
    "Consider a grid world below\n",
    "\n",
    "If we have to move from. A to E, we travel in a direction. Consider the squares to be STATES and the direction you are moving as ACTIONS. One possible way to reach is  1=>2=>3=>4=>End by executing actions such as \" N => N=> E=> E\n",
    "\n",
    "\n",
    "    \n",
    " \n",
    "# Markov Property\n",
    "\n",
    "For any given process, given a set of states, actions, transition probabilities, reward function, it is said to folloq the markov property if\n",
    "\n",
    "> The future state is independent of the past i.e all the information needed to predict the future is contained in the present and there is no need to know the history. In other words, the transition probability function follows the markov property.  \n",
    "\n",
    "Given S $ \\in $ {s1, s2, s3}\n",
    "      A $ \\in $ {a1, a2, a3}\n",
    "      P $ \\in $ {p1, p2, p3} \n",
    "      R $ \\in $ {r1, r2, r3} \n",
    "      \n",
    "      \n",
    " <<mathematical def of markov>>     \n",
    "\n",
    "In reinforcement learning, we dont know the transition functions and reward function, infact we have to learn it. We start of with small samples and establish some connections, and as we get more samples, we either strengthen the old connections or let go of them. and if enough votes exists between the 2 states, then we can confidently states the transition probabilities.  <font color=\"red\"> TODO : Explain with Diagram </font>\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal of the any Reinforcement Learning\n",
    "\n",
    "> The expected long term reward should be maximum. In other words. find a \"policy\" that is optimal.\"Optimal\" meaning you can get a maximum reward over time (not just immediate)\n",
    "\n",
    "R = $[r_{t}, r_{t+1}, r_{t+2},.....r_{∞}]$\n",
    "\n",
    "There are infinite sequences and hence infinite rewards.  how to solve this?\n",
    "\n",
    "- Restrict to finite sequences. Some problems are inherently finite (robot catching a ball)\n",
    "- Make sure we reach the terminal states for every possible policy\n",
    "- **Discounted rewards** - exponentially reduce the future rewards. \n",
    "\n",
    "> Future (unexpected) rewards are valued exponentially less than the immediate ones.\n",
    "\n",
    "So mathematically we can write it as \n",
    "\n",
    "R = $r_{t} + \\gamma^{2} r_{t+1} + \\gamma^{3} r_{t+2} + ....+ \\gamma^{∞-1} r_{∞}$\n",
    "\n",
    "Where $\\gamma$ = discount factor and is between 0 and 1 \n",
    "\n",
    "If $\\gamma$ is low, then it means we only care about the immediate rewards, since all the future becomes tiny and eventually becomes 0\n",
    "\n",
    "If $\\gamma$ is high, then it means we care a great deal about our future rewards. \n",
    "\n",
    "If $\\gamma$ is 1, we equally care about the rewards in all the timesteps\n",
    "\n",
    "> In essence, $\\gamma$ tell us about the horizon of rewards we are interested in\n",
    "\n",
    "It is shown visually below\n",
    "\n",
    "![](https://abhisheksreesaila.github.io/blog/images/rl/gamma_horizon.png \"Discount Factors vs Horizon Length\")\n",
    "\n",
    "\n",
    "### State Value and Action Value Function\n",
    "\n",
    "From the above equation for rewards we can quantify few things such as \n",
    "\n",
    "1. **What is the value of an agent being in state S?**\n",
    "  - From the reward function above we can conclude that it should be all the rewards in every time step from the state S to the terminal state by following a policy $ \\pi $ \n",
    "  > V $_\\pi(s)$ = $ E_{\\pi} \\sum \\limits _{t=0} ^{t-1} \\gamma^{t}r_{t} |s_{t} = s| $ \n",
    " \n",
    "  - It might look daunting, but its the exact same equation as above (rewards equation) computed for any given state s for its value V.\n",
    "  - If you want, you can simply think of its as **V(s) = sum of all rewards following a certain plan (or policy)** Now, let's say I come and tell you that the policy is the best possible one (\"optimal\") then immediately you can conclude the value is also the best possible one for that state. In other words, the policy (or plan) is optimal it should yield optimal value.\n",
    "    >  $V_\\pi(s)^{*}$ = $ E_{\\pi}^{*} \\sum \\limits _{t=0} ^{t-1} \\gamma^{t}r_{t} |s_{t} = s| $ \n",
    "\n",
    " - \"*\" indicates its optimal. Just an convention. you can use whatever you want.   \n",
    " \n",
    "2. **What is the value of an agent being in state S by choosing action A?**\n",
    "  -  This is exact same question as above, but a bit more granular.  We know that from a given state you can pick many actions (a1, a2, a3) and so on.  \n",
    "     - What is the value of a state if you pick action a1 - then follow policy  $ \\pi $\n",
    "     - What is the value of a state if you pick action a2 - then follow policy  $ \\pi $\n",
    "  - The reason for breaking this down is that we know an action is picked based on a certain probability. Depending on the probability value, a1 or a2 or a3 is picked and based on that you go to a certain state and hence you might get a different overall value V $_\\pi(s)$ Written mathematically, \n",
    "\n",
    "  > Q $_\\pi(s,a)$ = $ E_{\\pi} \\sum \\limits _{t=0} ^{t-1} \\gamma^{t}r_{t} |s_{t} = s, a_{t} = a| $ \n",
    "\n",
    "  > The \"Q value\" is the value of an agent in state S by chosing action A then following optimal policy\n",
    "  \n",
    "To get a \"optimal\" Q value, we have to make sure 2 things\n",
    "   -  the action we have taken has to be very good, infact the best one!\n",
    "   -  the policy after that has to be optimal!\n",
    " \n",
    " We do the above things, then we get $Q^{*}$  (optimal Q value) \n",
    " \n",
    " Combining these 2 above concepts,\n",
    " \n",
    " - first in plain english\n",
    "  \n",
    "    > -  Value of a state V = probability of executing an action A in the first place * Then the Q Value\n",
    "    > -  Optimal value of state value V = For all possible actions take the MAX of (probability of executing an action A in the first place * Q value)\n",
    " \n",
    "   $V_\\pi(s)$ = $ \\sum \\limits_{a\\in A} $ P($s^{1}|s,a$) *  Q $_\\pi(s,a)$\n",
    " \n",
    "\n",
    "### Squared Bellman Error\n",
    "\n",
    "\n",
    "### model free vs model based\n",
    "\n",
    "\n",
    "### temporal difference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glossary\n",
    "\n",
    "### Value vs Reward\n",
    " \n",
    " - Value = long term benefits\n",
    " - Reward = Immediate feedback\n",
    " - If its too long, environment has changed, your algorithm or function is obsolete\n",
    " - If its too short, its not a visionary and missed crucial opportunities\n",
    "   \n",
    "### Explore vs Exploit\n",
    " \n",
    " - Agent explores new environment and learn ==> updates policy\n",
    " - Agent exploits exising environment, fine tunes and gets good at it! \n",
    " - Our problem : Balance Exploit vs Explore\n",
    " \n",
    "### Dynamic Programming\n",
    "\n",
    "- Divides a problem into multiple sub problems\n",
    "- Solves the simplest sub problem first and then does this recursively until all of them are solved\n",
    "- Solving all the sub-problems will ultimately solve the bigger problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[RL Basics](https://www.youtube.com/watch?v=wN3rxIKmMgE)\n",
    "\n",
    "[RL Solving MDP and Reward Functions](https://www.youtube.com/watch?v=meywaLPitZ4&list=PLYgyoWurxA_8ePNUuTLDtMvzyf-YW7im2&index=3)\n",
    "\n",
    "[Markov Property](https://www.youtube.com/watch?v=GJEL-QkT2yk&list=PLYgyoWurxA_8ePNUuTLDtMvzyf-YW7im2&index=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
