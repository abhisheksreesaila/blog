{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Reinforcement Learning - Basics\"\n",
    "\n",
    "> \"Trying to explain the basics of reinforcement learning\"\n",
    "\n",
    "- toc: true\n",
    "- branch: master\n",
    "- badges: false\n",
    "- comments: true\n",
    "- categories: [Reinforcement Learning]\n",
    "- hide: false\n",
    "- search_exclude: false\n",
    "- image: images/post-thumbnails/rl.png\n",
    "- metadata_key1: notes\n",
    "- metadata_key2: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "\n",
    "In this blog post, I try to understand the intution behind the math for reinforcement learning. If you understand the basics and how the math is derived, then all the algorithms following this blogpost are just a simple variation which can be easily understood. I will try to provide a \"english\" explanation to all the math and hide the math, but for the mathematically inclined people, you can expand and check out. Finally, after the blog post, for all the algorithms the corresponding code will be provided.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# Basic Terms\n",
    "\n",
    "- Unlike the supervised or unsuperivsed learning algorithms, it is used in constantly changing or dynamic environment\n",
    "\n",
    "- There are 2 entities involved: \n",
    "  - AGENT : Software involved in getting input and executing actions.\n",
    "  - Environment : House that the agent is trying to execute the actions. \n",
    "\n",
    "-  Input\n",
    "    - Take a set of observations (or states)\n",
    "    \n",
    "- Process (or policy)\n",
    "    - Figures out the right set of actions\n",
    "    - If we used \"deep learning\" for this, then it becomes **Deep Reinforcement Learning**\n",
    "    - Takes an input \"oldSTATE\" and outputs an \"ACTION\" that goes to a \"newSTATE\"\n",
    "    - Think of it as a \"plan\". If you \"plan\" to take the \"freeway\" today to avoid internal city roads - thats a plan\n",
    "    - if you are planning to city roads to avoid \"congestion\" on the freeway - thats a plan (or policy). you are optmizing \"time\" in both but following different policy\n",
    "    \n",
    "- Output\n",
    "    - Right set of commands to follow\n",
    "    \n",
    "- Feedback\n",
    "    - It the output received positive feedback, great, remember and repeat when possible\n",
    "    - If not, change to another action. \n",
    " \n",
    "![](https://abhisheksreesaila.github.io/blog/images/rl/rl-flow.png \"Reinforcement Learning\")\n",
    "\n",
    "---\n",
    "\n",
    "# Process\n",
    "\n",
    "There are 2 types of process in the real world.  In the visual below, A => B and B=>C and its the only path of transportation. Most often we counter a scenario that A has 80% probability go to B and 20% probability to go to C\n",
    "\n",
    "- A, B, C are the states.  \n",
    "- RIGHT and DOWN are actions\n",
    "- 0.8, 0.2, 1.0 are transition probabilities\n",
    "- The decision to go from A=>B=>C is a policy ; A=>B is a policy ; A=>C is a policy\n",
    "\n",
    "![](https://abhisheksreesaila.github.io/blog/images/rl/process_with_certainity.png \"Process with Certainity\")\n",
    "\n",
    "![](https://abhisheksreesaila.github.io/blog/images/rl/process_with_probs.png \"Process with Transition Probabilities\")\n",
    "\n",
    "---\n",
    "    \n",
    "# Markov Property\n",
    "\n",
    "For any given process, given a set of states, actions, transition probabilities, reward function, it is said to folloq the markov property if\n",
    "\n",
    "> The future state is independent of the past i.e all the information needed to predict the future is contained in the present and there is no need to know the history. In other words, the transition probability function follows the markov property.  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal of the any Reinforcement Learning\n",
    "\n",
    "> The expected long term reward should be maximum. In other words. find a \"policy\" that is optimal.\"Optimal\" meaning you can get a maximum reward over time (not just immediate)\n",
    "\n",
    "R = $[r_{t}, r_{t+1}, r_{t+2},.....r_{∞}]$\n",
    "\n",
    "There are infinite sequences and hence infinite rewards.  how to solve this?\n",
    "\n",
    "- Restrict to finite sequences. Some problems are inherently finite (robot catching a ball)\n",
    "- Make sure we reach the terminal states for every possible policy\n",
    "- **Discounted rewards** - exponentially reduce the future rewards. \n",
    "\n",
    "> Future (unexpected) rewards are valued exponentially less than the immediate ones.\n",
    "\n",
    "So mathematically we can write it as \n",
    "\n",
    "R = $r_{t} + \\gamma^{2} r_{t+1} + \\gamma^{3} r_{t+2} + ....+ \\gamma^{∞-1} r_{∞}$\n",
    "\n",
    "Where $\\gamma$ = discount factor and is between 0 and 1 \n",
    "\n",
    "If $\\gamma$ is low, then it means we only care about the immediate rewards, since all the future becomes tiny and eventually becomes 0\n",
    "\n",
    "If $\\gamma$ is high, then it means we care a great deal about our future rewards. \n",
    "\n",
    "If $\\gamma$ is 1, we equally care about the rewards in all the timesteps\n",
    "\n",
    "> In essence, $\\gamma$ tell us about the horizon of rewards we are interested in\n",
    "\n",
    "It is shown visually below\n",
    "\n",
    "![](https://abhisheksreesaila.github.io/blog/images/rl/gamma_horizon.png \"Discount Factors vs Horizon Length\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### State Value and Action Value Function\n",
    "\n",
    "From the above equation for rewards we can quantify few things such as \n",
    "\n",
    "1. **What is the value of an agent being in state S?**\n",
    "  - From the reward function above we can conclude that it should be all the rewards in every time step from the state S to the terminal state by following a policy $ \\pi $ \n",
    "  > V $_\\pi(s)$ = $ E_{\\pi} \\sum \\limits _{t=0} ^{t-1} \\gamma^{t}r_{t} |s_{t} = s| $ \n",
    " \n",
    "  - It might look daunting, but its the exact same equation as above (rewards equation) computed for any given state s for its value V.\n",
    "  - If you want, you can simply think of its as **V(s) = sum of all rewards following a certain plan (or policy)** Now, let's say I come and tell you that the policy is the best possible one (\"optimal\") then immediately you can conclude the value is also the best possible one for that state. In other words, the policy (or plan) is optimal it should yield optimal value.\n",
    "    >  $V_\\pi(s)^{*}$ = $ E_{\\pi}^{*} \\sum \\limits _{t=0} ^{t-1} \\gamma^{t}r_{t} |s_{t} = s| $ \n",
    "\n",
    " - \"*\" indicates its optimal. Just an convention. you can use whatever you want.   \n",
    " \n",
    "2. **What is the value of an agent being in state S by choosing action A? What is this action worth?**\n",
    "  -  This is exact same question as above, but a bit more granular.  We know that from a given state you can pick many actions (a1, a2, a3) and so on.  \n",
    "     - What is the value of a state if you pick action a1 - then follow policy  $ \\pi $\n",
    "     - What is the value of a state if you pick action a2 - then follow policy  $ \\pi $\n",
    "  - The reason for breaking this down is that we know an action is picked based on a certain probability. Depending on the probability value, a1 or a2 or a3 is picked and based on that you go to a certain state and hence you might get a different overall value V $_\\pi(s)$ Written mathematically, \n",
    "\n",
    "  > Q $_\\pi(s,a)$ = $ E_{\\pi} \\sum \\limits _{t=0} ^{t-1} \\gamma^{t}r_{t} |s_{t} = s, a_{t} = a| $ \n",
    "\n",
    "  > The \"Q value\" is the value of an agent in state S by chosing action A then following optimal policy\n",
    "  \n",
    "To get a \"optimal\" Q value, we have to make sure 2 things\n",
    "   -  the action we have taken has to be very good, infact the best one!\n",
    "   -  the policy after that has to be optimal!\n",
    " \n",
    " We do the above things, then we get $Q^{*}$  (optimal Q value) \n",
    " \n",
    " Combining these 2 above concepts,\n",
    " \n",
    "\n",
    "Q value = probability of executing an action A in the first place * [Current reward + (discounted future rewards)]\n",
    "    \n",
    "$Q^{*}(s,a)$ = argmax $ \\sum \\limits_{a\\in A} $ P($s^{1}|s,a$) * [ $ R(s,a,s^1)  + \\gamma   V^{*}(s^1)$]  ........(1)\n",
    "\n",
    "Optimal value of state value V = For all possible actions take the MAX of (probability of executing an action A in the first place * Q value)\n",
    " \n",
    "$V^{*}(s) $ = argmax $ \\sum \\limits_{a\\in A}$ $Q^{*}(s,a)$   ........(2)\n",
    "\n",
    "Substitute (2) in (1), we get bellman equation in terms of Q values\n",
    "\n",
    "\n",
    "$Q^{*}(s,a)$ =  $ \\sum \\limits_{a\\in A} $ P($s^{1}|s,a) $ * [$ R(s,a,s^1)  + \\gamma  max Q^{*}(s',a)$]\n",
    "\n",
    "This is the BELLMAN EQUATION for the Q values and forms a corner stone in RL\n",
    "\n",
    "You can substitute the other way and get it in terms of \"V\" values. In this and future blog posts, I intend to explore Q learning and its variants, so i am comfortable looking at it from the \"Q Value\" perspective. \n",
    "\n",
    " ---\n",
    " \n",
    "### Solving Bellman Equation\n",
    "\n",
    "Lets recap, \n",
    "\n",
    " - By using the concept of DISOUNTED REWARDS, we arrived at a equation for calculate expected rewards\n",
    " - Using the above concept, we were able to quantify 2 things in our processess.\n",
    "   - State value (or \"V\" Value)  - value of a state given a policy. From a state, you have multiple actions to follow. \n",
    "   - Action value(or \"Q\" Value)  - value of a state after following a particular action, then a given policy. This is a more specific version of the \"state value\".\n",
    "  \n",
    " ![](https://abhisheksreesaila.github.io/blog/images/rl/qvalues.png \"State and Action Values\")\n",
    "\n",
    "If i asked to pick the best action, it means i am asking to choose an action with the best Q value, which inturns leads to the best \"State\" value. In other words, you put your best foot forward, what else did you expect :) ?\n",
    "\n",
    "Note that the bellman's equation is recursive, meaning Q(s,a) is dependent of Q(s', a) and so on. Lets simplify and understand the intuition first.\n",
    "\n",
    "Q(s, a) = something + Q(s', a)\n",
    "\n",
    "Q(s', a) = something + Q(s\", a)\n",
    "\n",
    "Q(s\", a) = something + Q(s\"', a)\n",
    "\n",
    "Assume Q(s\"', a) is the final state, then for \n",
    "\n",
    "Q(s\"', a) = we get the value since its a terminal state\n",
    "   \n",
    "now go back and substitute, to get  Q(s\"', a), then substitute again to get Q(s', a) and finally we get Q(s, a)\n",
    "\n",
    "This concept of solving subproblems which ultimately leads to the final answer being calculated is called dynamic programming. \n",
    "\n",
    "So we can modify the equation ever-so-slightly to show the updates. (Bellman equation writtten to show iterative updates)\n",
    "\n",
    "$Q_{i+1}(s,a)$ =  $ \\sum \\limits_{a\\in A} $ P($s^{1}|s,a) $ * [$ R(s,a,s^1)  + \\gamma  max Q_{i}(s',a)$]\n",
    "\n",
    "\n",
    "In our example,  v(s\"') = iteration 1, v(s\") = iteration 2 etc. \n",
    "\n",
    "So, given the TRANSITION PROBABILITIES and REWARD FUNCTIONS, we can easily apply the dynamic programming to the bellman equation and solve it!!  Take an example here.\n",
    "\n",
    "You are starting from random values, and then fixing them to show correct ones. Then finally from start to finish pick the box with the least cost.  This is called \"value iteration\".\n",
    "\n",
    "\n",
    "![](https://abhisheksreesaila.github.io/blog/images/rl/iteration1.png \"Iteration 1\")\n",
    "\n",
    "![](https://abhisheksreesaila.github.io/blog/images/rl/iteration2.png \"Iteration 2\")\n",
    " \n",
    "![](https://abhisheksreesaila.github.io/blog/images/rl/iteration3.png \"Iteration 3\")\n",
    "\n",
    "\n",
    "There are couple of other variants called \"Policy iteration\" and \"Hybrid Valuation\" which updates the policy. It will explained in another blog post.\n",
    "\n",
    "---\n",
    "\n",
    "### Temporal Differencing\n",
    "\n",
    "In the world of reinforcement learning, we are not given \"transition probabilities\" and Rewards functions to different states are not known to begin with. The agent interacts multiple times (called episodes) and only after that \"figures\" out. how do we update the bellman equation in each iteration if the values are unknown? We update the equation for each interaction with the environment. Initially, we have zero knowledge, so we will pick next states randomly. Our sample size is just 1.  Just with 1 interaction its impossible to say the random state we jumped to is optimal or not. So we adjust the equation by adding a \"learning rate\" parameter that is usually very low (eg. 0.001). Next we repeat the steps, each time our sample is increasing, and obviosuly our confidence in knowing the entire env is also increasing. Only after a certain # of episodes we establish a full knowledge of the env. \n",
    "\n",
    "> Every time you take an action. A from state S, you get a sample of **$P(s^{1}|s,a$)**  and a corresponding **$R(s,a,s^1)$** \n",
    "\n",
    "Also, it wont be broken into 2 parts, rather it will fused into 1 value. *say sample 1* at the end, you will get S = {sample1, sample2, sample3...sampleEnd} Basically, its a probability distribution given by\n",
    "\n",
    "sample = $R(s,a,s^1)$  +  $\\gamma Q_{i}(s^1)$\n",
    "\n",
    "TD update equation is written as \n",
    "\n",
    "> Q(s,a) =  Q(s,a) + $ \\alpha $ [$ R(s,a,s^1)  + \\gamma  max_{a'} Q(s',a')$ - Q(s,a)]\n",
    "\n",
    "One can read this as \n",
    "Qvalue = Current Qvalue +  learning rate (expected future reward - current Q value)\n",
    "Qvalue = Current Qvalue +  learning rate (BELLMAN ERROR)\n",
    "\n",
    "You are learning the Q values as you go, and hence the equation represents a algorithm called \"Q learning\"\n",
    "\n",
    "The intuition behind this is : i dont trust the sample too much but just a little. (like a good stranger) I started with a random value, but i am not sure whether i \"over quoted\" the Q value or \"under quoted\". Suppose the ideal Q value is 50. But the random value you started is 80. After getting the first sample, Qvalue = Qvalue +  learning rate (BELLMAN ERROR) where BELLMAN ERROR is negative, its tell you reduce 80. but not too much just by a small factor (lets say 0.001). Then try to get a sample from env, again try the same transition from s -> s', you get negative again. you reduce 80 again. and so on... The point here is that rate of change of Q value tell you which direction to move (postive or negative in our example), learning rate tell you the TRUST factor based on which you keep updating Q values.\n",
    "\n",
    "> Formally stated, treat a single sample as a representative of the distribution and apply an \"incremental\" update to reduce the \"bellman error\"\n",
    "\n",
    "> Note:  For those people familiar with deep learning, this is similar to the \"stochastic gradient learning\" where you updates for each input, and since you are looking at it piece by piece, you want to adjust the \"update\" to the model by a learning rate parameter. \n",
    "\n",
    "From here onwards, all we will focus how to learn Q accurately. If there is magic wand, and we could know the optimal Q values between states, thats your algorithm 1. Suppose someone hands over a table, thats an option too! Life is not easy and so we will end up starting randomly and use the above equation\n",
    "\n",
    "---\n",
    "\n",
    "## Glossary\n",
    "\n",
    "### Value vs Reward\n",
    " \n",
    " - Value = long term benefits\n",
    " - Reward = Immediate feedback\n",
    " - If its too long, environment has changed, your algorithm or function is obsolete\n",
    " - If its too short, its not a visionary and missed crucial opportunities\n",
    "   \n",
    "### Explore vs Exploit\n",
    " \n",
    " - Agent explores new environment and learn ==> updates policy\n",
    " - Agent exploits exising environment, fine tunes and gets good at it! \n",
    " - Our problem : Balance Exploit vs Explore\n",
    " \n",
    "### Dynamic Programming\n",
    "\n",
    "- Divides a problem into multiple sub problems\n",
    "- Solves the simplest sub problem first and then does this recursively until all of them are solved\n",
    "- Solving all the sub-problems will ultimately solve the bigger problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[RL Basics](https://www.youtube.com/watch?v=wN3rxIKmMgE)\n",
    "\n",
    "[RL Solving MDP and Reward Functions](https://www.youtube.com/watch?v=meywaLPitZ4&list=PLYgyoWurxA_8ePNUuTLDtMvzyf-YW7im2&index=3)\n",
    "\n",
    "[RL-Solving MDP](https://www.youtube.com/watch?v=l87rgLg90HI&list=PLYgyoWurxA_8ePNUuTLDtMvzyf-YW7im2&index=5)\n",
    "\n",
    "[Markov Property](https://www.youtube.com/watch?v=GJEL-QkT2yk&list=PLYgyoWurxA_8ePNUuTLDtMvzyf-YW7im2&index=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
