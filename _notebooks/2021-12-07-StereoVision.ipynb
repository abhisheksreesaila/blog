{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Stereo Vision\"\n",
    "\n",
    "> \"Two eyes are always better than one!\"\n",
    "\n",
    "- toc: true\n",
    "- branch: master\n",
    "- badges: false\n",
    "- comments: true\n",
    "- categories: [Computer Vision]\n",
    "- hide: false\n",
    "- search_exclude: false\n",
    "- image: images/post-thumbnails/sv2.png\n",
    "- metadata_key1: notes\n",
    "- metadata_key2: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# The loss of depth\n",
    "\n",
    "Given a point in the image plane (U, V), can we find a corresponding point in the world system? The answer is NO.  When we move capture a image we know that 3d world coordinate gets transformed to camera cooridinate and then a 2D image plane. Refer to this blog post for indepth details. Since we loose information, more specifically \"Z\" depth information, it is impossible to get it back.  In other words, given an image point you cannot reverse engineer WORLD coordinate since you have lost a crucial \"Z\" coordinate in the translation process. however, you havent lost \"X\", \"Y\".  There is hope to get it back, but we need additional help.\n",
    " \n",
    " ![](https://abhisheksreesaila.github.io/blog/images/stereo/monocular2.png \"Loss of Depth\")\n",
    "\n",
    " \n",
    " Since we know \"X\", \"Y\", we know it exists somewhere along the line shown as dotted green lines. Why? This was the same line which was used in the projection of object onto the image plane to derive all the math. So, it can also assist in the reverse direction as well. \n",
    "\n",
    "![](https://abhisheksreesaila.github.io/blog/images/stereo/projection2.png \"Projection of the object on image plane\")\n",
    "\n",
    "\n",
    "# How to recover?\n",
    "\n",
    "The trick is to understand how nature does this. We all have 2 eyes, and we perceive depth (3d) in all objects. Don't we? The \"second eye\" provides its view of the world  in addition to the first, and both together work together perceive depth.\n",
    "\n",
    "Lets apply the same concept here. lets bring in another camera, place it horizontally along the same axis at a distance, find the exact same spot (U, V) on it, guess the point on the \"dotted green line\".  The intersection of these 2 dotted green lines gives you the depth Z.  See below for the visual\n",
    "\n",
    "![](https://abhisheksreesaila.github.io/blog/images/stereo/stereo.png \"Stereo Vision\")\n",
    "\n",
    "\n",
    "- $ u_r v_r $ and  $ u_l v_l $ are the exact same point of the image as seen in the right and left camera respectively\n",
    "- the distance between the cameras is called baseline denoted by b\n",
    "- the camera plane is placed at the pinhole with origin (0,0,0) on the left and (b,0,0) on the right\n",
    "- P (x, y, z) is the scene point that we are trying to compute using the 2 cameras.\n",
    "\n",
    "\n",
    "This concept of using 2 cameras to perceive depth in the real world is called Simple Stereo Vision.  In this blog post lets understand the mechanics of such a system. \n",
    "\n",
    "# Finding Depth\n",
    "\n",
    "Lets start with the basics. For a pinhole based camera system we know the following equations\n",
    "\n",
    "$ u_l = f_x * x/z + O_x $  ;   $ v_l = f_y * y/z  + O_y $ \n",
    "\n",
    "\n",
    "For the right camera, its the same but the camera axis is shifted by \"b\"\n",
    "\n",
    "$ u_r = f_x * x/z + O_x $  ;   $ v_r = f_y * y/z  + O_y $ \n",
    "\n",
    "\n",
    "Using the 4 equations, solving for x, y, z  we get\n",
    "\n",
    "$ x = \\frac {b (u_l - O_x)}{ u_l - u_r } $\n",
    "$ y = \\frac {b f_x (v_l - O_y)}{f_y (u_l - u_r)} $\n",
    "$ z = \\frac {b f_x}{ u_l - u_r } $\n",
    "\n",
    "\n",
    "Where $ u_l - u_r $ is called disparity and its inversely propotional to \"z\" \n",
    "\n",
    "\n",
    "> Important: If we know the internal parameters fx, fy, ox, oy and compute disparity, we compute Z and hence the depth. \n",
    "\n",
    "\n",
    "If the object is closer to the camera, you will see a large disparity. for example, U value in the left camera will be 100, whereas the right camera it will be 75.  This is exact same pixel in the image but having 2 different values. The opposite is also true i.e the object is far, there will be very less difference between the U values (Say 100 and 95).  At infinite distance, U values will exactly be the same.\n",
    "\n",
    "Disparity is propotional to baseline meaning if the distance between camera increase, disparity will increase. \n",
    "\n",
    "I keep mentioning only \"U\" because there is no $v_l - v_r $ in the equation. which means only the horizontal component between the 2 cameras vary not the \"vertical component\".  This proporty shows that  $ u_r, v_r $ and $ u_l $ and $ v_l $ lie along the same line (show by the yellow line). when we are computing DISPARITY to solve for X, Y, Z  in the real world, we can pick a point in the left camera $ (u_l, v_l) $ and  ONLY search along the same line in the right camera (and not wander aimlessly and search the whole image) to get the $ u_r, v_r $ .i.e  its a \"1D\" search problem. See image below for an example. This is often called the **\"correspondence problem\"**\n",
    "\n",
    "\n",
    "\n",
    "## Problems with stereo matching\n",
    "\n",
    "Did we solve of finding depth just by solving 2 cameras? yes for the most part, but if the images have the repetitive texture, its impossible to compute disparity and therefore can't compute depth.  see image below. \n",
    "\n",
    "\n",
    "![](https://abhisheksreesaila.github.io/blog/images/stereo/texture.png \"Stereo Vision CAN be computed\")\n",
    "\n",
    "![](https://abhisheksreesaila.github.io/blog/images/stereo/no-texture.png \"Stereo Vision CANNOT be computed\")\n",
    "\n",
    "\n",
    "\n",
    "# Calibration of the Stereo\n",
    "\n",
    "In the section above we assumed the stereo is calibrated that means we know the how they are aligned with respect to each other. \n",
    "\n",
    "Suppose we take a photo of effiel tower on a iphone. Then another person take a same photo with a slight different angle from samsung android phone. Is it possible to compute Z depth information and hence reocover the 3d structure of the image?  The answer turns out to be yes. \n",
    "\n",
    "![](https://abhisheksreesaila.github.io/blog/images/stereo/Uncalibrated-stereo.png \"Uncalibrated Stereo\")\n",
    "\n",
    "Every digital camera embeds certain metadata within the image such as the focal length etc. which can be read as internal parameters. All we need to compute are the external parameters.\n",
    "\n",
    "\n",
    "In practice if there are 2 camera taking a shot at the same picture at 2 different angles, if we know the internal parameters of each camera, then we can calculate the alignment ourselves and hence compute the depth.  that is what we will explore in this section\n",
    "\n",
    "\n",
    "\n",
    "Consider the above picture.  It is identical to the one in the earlier section except that left and right cameras have their own coordinate system $(x_l,y_l, z_l)$ and $ (x_r, y_r, z_r)$ respectively. \n",
    "\n",
    "Our goal is to compute the \"translation\" and \"rotation\" of one camera w.r.t the other.\n",
    "\n",
    "## Epipolar Geometry\n",
    "\n",
    "![](https://abhisheksreesaila.github.io/blog/images/stereo/epipolar_geo.png \"Epipolar Plane\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plane formed by the scene point (P) and camera origins $ o_l $  and $ o_r $ is called epipolar plane\n",
    "\n",
    "The projection of the center of left camera on the right one and vice versa $ e_l $ and $ e_r $ are called epipoles\n",
    "\n",
    "Every scene point will have it own epipolar plane.\n",
    "\n",
    "## Now why do we care about epipolar geometry?\n",
    "\n",
    "> Our goal is to find a equation such that we can calculate t, R (translation, Rotation)\n",
    "\n",
    "![](https://abhisheksreesaila.github.io/blog/images/stereo/epipolar_cons.png \"Epipolar Constraint\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epipolar Constraint\n",
    "\n",
    "Consider a vector perpendicular to $X_l$ (highlighted in pink). Lets call it N\n",
    "\n",
    "From linear algebra,\n",
    " - N = Cross Product between t and $X_l$\n",
    " - N = t X $X_l$....(1)\n",
    " \n",
    "Also, \n",
    " \n",
    "- $X_l$ * N = 0 (dot product of N and $X_l$ is 0).....(2)\n",
    "\n",
    "Hence from (1) and (2)\n",
    "\n",
    "(t X $X_l$) * $X_l$ = 0\n",
    "\n",
    "Expanding the vector $X_l$ and performing the CROSS PRODUCT\n",
    "\n",
    "\n",
    "$$\n",
    "\n",
    "R \n",
    "=\n",
    "\\begin{bmatrix} x_l & y_l & z_l  \\end{bmatrix}\n",
    "\n",
    "\\begin{bmatrix}  t_yz_l - t_zy_l & t_zx_l - t_xz_l & t_xy_l - t_yx_l    \\end{bmatrix}\n",
    "= 0\n",
    "\n",
    "$$\n",
    "\n",
    "Writing in Matrix Form\n",
    "\n",
    "$$\n",
    "\n",
    "R \n",
    "=\n",
    "\\begin{bmatrix} x_l & y_l & z_l  \\end{bmatrix}\n",
    "\n",
    "\\begin{bmatrix} 0 & -t_z & t_y \\\\ t_z & 0 & -t_x \\\\ -t_y & t_x & 0     \\end{bmatrix}\n",
    "\n",
    "\\begin{bmatrix} x_l & y_l & z_l  \\end{bmatrix}\n",
    "\n",
    "= 0\n",
    "\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "We all know that $ x_l = R x_r + t $\n",
    "\n",
    "Where t = position of right camera w.r.t to left\n",
    "R = orientation of right camera w.r.t to left\n",
    "\n",
    "Writing this equation in Matrix form\n",
    "\n",
    "\n",
    "$$\n",
    "\n",
    "\\begin{bmatrix} x_l & y_l & z_l  \\end{bmatrix}\n",
    "\n",
    "\\begin{bmatrix} r11 & r12 & r13 \\\\ r21 & r22 & r23 \\\\  r31 & r32 & r33   \\end{bmatrix}\n",
    "\n",
    "\\begin{bmatrix} x_r & y_r & z_r  \\end{bmatrix}\n",
    "\n",
    "+\n",
    "\n",
    "\\begin{bmatrix} t_x & t_y & t_z  \\end{bmatrix}\n",
    "\n",
    "$$\n",
    "\n",
    "\n",
    "Substituting for $x_l y_l z_l$\n",
    "\n",
    "E = TxR\n",
    "\n",
    "T= skew sysmmetric. \n",
    "R = ortho normal matrix\n",
    "\n",
    "decouple them \"singular value decomposition\n",
    "\n",
    "Unforutnaly. XL is still found in teh equation\n",
    "\n",
    "from perspective projection substitute,\n",
    "\n",
    "[ul vl 1]F [ur vr 1] = 0\n",
    "\n",
    "F = fundamental matrix\n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "\n",
    "You get F\n",
    "\n",
    "decompose into E\n",
    "\n",
    "decompose E into T and R\n",
    "\n",
    "then we can calibrated the stereo\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[Stereo Vision](https://www.youtube.com/watch?v=hUVyDabn1Mg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
