{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Stereo Vision - PART 1\"\n",
    "\n",
    "> \"Why do we need stereo? Two eyes are always better than one!\"\n",
    "\n",
    "- toc: true\n",
    "- branch: master\n",
    "- badges: false\n",
    "- comments: true\n",
    "- categories: [Computer Vision]\n",
    "- hide: false\n",
    "- search_exclude: false\n",
    "- image: images/post-thumbnails/sv2.png\n",
    "- metadata_key1: notes\n",
    "- metadata_key2: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "\n",
    "In this multi-part blog post, we try to understand \n",
    "\n",
    "- Why we need stereo in computer vision?\n",
    "- Given any stereo cameras, how can we calibrate and use them to find depth?\n",
    "- Summary of steps\n",
    "\n",
    "\n",
    "# Why do we need Stereo?\n",
    "\n",
    "\n",
    "## The loss of depth\n",
    "\n",
    "Given a point in the image plane (U, V), can we find a corresponding point in the world system? The answer is NO.  When we move capture a image we know that 3d world coordinate gets transformed to camera cooridinate and then a 2D image plane. Refer to this blog post for indepth details. Since we loose information, more specifically \"Z\" depth information, it is impossible to get it back.  In other words, given an image point you cannot reverse engineer WORLD coordinate since you have lost a crucial \"Z\" coordinate in the translation process. however, you havent lost \"X\", \"Y\".  There is hope to get it back, but we need additional help.\n",
    " \n",
    " ![](https://abhisheksreesaila.github.io/blog/images/stereo/monocular2.png \"Loss of Depth\")\n",
    "\n",
    " \n",
    "Since we know \"X\", \"Y\", we know it exists somewhere along the line shown as dotted green lines. Why? This was the same line which was used in the projection of object onto the image plane to derive all the math. So, it can also assist in the reverse direction as well. \n",
    "\n",
    "![](https://abhisheksreesaila.github.io/blog/images/stereo/projection2.png \"Projection of the object on image plane\")\n",
    "\n",
    "\n",
    "### How to recover?\n",
    "\n",
    "The trick is to understand how nature does this. We all have 2 eyes, and we perceive depth (3d) in all objects. Don't we? The \"second eye\" provides its view of the world  in addition to the first, and both together work together perceive depth.\n",
    "\n",
    "Lets apply the same concept here. lets bring in another camera, place it horizontally along the same axis at a distance, find the exact same spot (U, V) on it, guess the point on the \"dotted green line\".  The intersection of these 2 dotted green lines gives you the depth Z.  See below for the visual\n",
    "\n",
    "![](https://abhisheksreesaila.github.io/blog/images/stereo/stereo.png \"Stereo Vision\")\n",
    "\n",
    "\n",
    "- $ u_r v_r $ and  $ u_l v_l $ are the exact same point of the image as seen in the right and left camera respectively\n",
    "- the distance between the cameras is called baseline denoted by b\n",
    "- the camera plane is placed at the pinhole with origin (0,0,0) on the left and (b,0,0) on the right\n",
    "- P (x, y, z) is the scene point that we are trying to compute using the 2 cameras.\n",
    "\n",
    "\n",
    "This concept of using 2 cameras to perceive depth in the real world is called Simple Stereo Vision.  In this blog post lets understand the mechanics of such a system. \n",
    "\n",
    "\n",
    "## Finding Depth\n",
    "\n",
    "Lets start with the basics. For a pinhole based camera system we know the following equations\n",
    "\n",
    "$ u_l = f_x * x/z + O_x $  ;   $ v_l = f_y * y/z  + O_y $ \n",
    "\n",
    "For the right camera, its the same but the camera axis is shifted by \"b\"\n",
    "\n",
    "$ u_r = f_x * x/z + O_x $  ;   $ v_r = f_y * y/z  + O_y $ \n",
    "\n",
    "Using the 4 equations, solving for x, y, z  we get\n",
    "\n",
    "$ x = \\frac {b (u_l - O_x)}{ u_l - u_r } $\n",
    "$ y = \\frac {b f_x (v_l - O_y)}{f_y (u_l - u_r)} $\n",
    "\n",
    "**$ z = \\frac {b f_x}{ u_l - u_r } $**\n",
    "\n",
    "\n",
    "> Note: $ u_l - u_r $ is called Disparity and its inversely propotional to \"z\". \n",
    "\n",
    "> Important: If we know the internal parameters fx, fy, ox, oy and compute disparity, we compute Z and hence the depth. \n",
    "\n",
    "If the object is closer to the camera, you will see a large disparity. for example, U value in the left camera will be 100, whereas the right camera it will be 75.  This is exact same pixel in the image but having 2 different values. The opposite is also true i.e the object is far, there will be very less difference between the U values (Say 100 and 95).  At infinite distance, U values will exactly be the same.\n",
    "\n",
    "> Disparity is propotional to baseline meaning if the distance between camera increase, disparity will increase. \n",
    "\n",
    "I keep mentioning only \"U\" because there is no $v_l - v_r $ in the equation. which means only the horizontal component between the 2 cameras vary not the \"vertical component\".  This proporty shows that  $ u_r, v_r $ and $ u_l $ and $ v_l $ lie along the same line (show by the yellow line). when we are computing DISPARITY to solve for X, Y, Z  in the real world, we can pick a point in the left camera $ (u_l, v_l) $ and  ONLY search along the same line in the right camera (and not wander aimlessly and search the whole image) to get the $ u_r, v_r $ .i.e  its a \"1D\" search problem. See image below for an example. This is often called the **\"correspondence problem\"**\n",
    "\n",
    "![](https://abhisheksreesaila.github.io/blog/images/stereo/texture.png \"Finding Correspondence\")\n",
    "\n",
    "The white patch in the picture is called the \"scan line\".\n",
    "\n",
    "## Problems with stereo matching\n",
    "\n",
    "Did we solve of finding depth just by solving 2 cameras? yes for the most part, but if the images have the repetitive texture, its impossible to compute disparity and therefore can't compute depth.  see image below. \n",
    "\n",
    "![](https://abhisheksreesaila.github.io/blog/images/stereo/no-texture.png \"Stereo Vision CANNOT be computed\")\n",
    "\n",
    "# Next Steps\n",
    "\n",
    "In the next [part](https://ablearn.io/computer%20vision/2021/12/08/StereoVision-2.html) we will understand how to calibrate stereo cameras and use them to compute depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[Stereo Vision](https://www.youtube.com/watch?v=hUVyDabn1Mg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
