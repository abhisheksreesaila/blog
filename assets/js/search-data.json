{
  
    
        "post0": {
            "title": "What is Q-Learning?",
            "content": "Q-Learning . The purpose of this blog post is to learn Q learning using code using the theory from the last blog post. . You can find the previous one here. Reinforcement Learning - Basics | . We will use openAI gym to apply Q learning for a FrozenLakeNoSlip environment I will explain the important parts of the code but the full code can be downloaded here . . Initialize the OPENAI GYM. Print the action and states (observation space) just to get an idea as to how many actions and states we are dealing with . try: register( id=&#39;FrozenLakeNoSlip-v0&#39;, entry_point=&#39;gym.envs.toy_text:FrozenLakeEnv&#39;, kwargs={&#39;map_name&#39; : &#39;4x4&#39;, &#39;is_slippery&#39;:False}, max_episode_steps=100, reward_threshold=0.78, # optimum = .8196 ) except: pass env_name = &quot;FrozenLakeNoSlip-v0&quot; env = gym.make(env_name) print(&quot;Observation space:&quot;, env.observation_space) print(&quot;Action space:&quot;, env.action_space) &quot;&quot;&quot; Output #Observation space: Discrete(16) #Action space: Discrete(4) &quot;&quot;&quot; . . Initialize . Initialize parameters such as &quot;discount factor&quot;, &quot;learning rate&quot; and &quot;Q table&quot; . # Create a class class QAgent(): def __init__(self, env, discount_rate=0.97,learning_rate=0.01): super().__init__(env) self.state_size = env.observation_space.n # init state size self.eps = 1.0 self.discount_rate = discount_rate # discount factor self.learning_rate = learning_rate # learning rate self.build_model() # Init Q-Table. Just random values for begin with def build_model(self): # random Q-Table with rows(state size) X columns (action size) self.q_table = 1e-4*np.random.random([self.state_size, self.action_size]) . . Choosing Action . def get_action(self, state): # get the Q-value for a state from the &quot;Q table&quot;. You will get 4 possible values , since there 4 actions. (q1, q2, q3, q4) q_state = self.q_table[state] # get the max of the (q1, q2, q3, q4). That represents the &quot;BEST ACTION&quot;. Its called &quot;greedy approach&quot; since we are always trying to priortize &quot;MAX&quot;. however there is a inherent problem with choosing &quot;MAX&quot;. See below explanation. action_greedy = np.argmax(q_state) # Choose &quot;random&quot; action based on a arbritary criteria (epsilon. you can call it anything. its totally arbritary. we are just trying to avoid max all the time. Thats all) # If the random.random() &lt; epsilon, choose random, else choose greedy action_random = super().get_action(state) return action_random if random.random() &lt; self.eps else action_greedy . . TR Update . This is the main function which updates the Q table based on the bellman equation. . From the last blog post . Q(s,a) = Q(s,a) + $ alpha $ [$ R(s,a,s^1) + gamma max_{a&#39;} Q(s&#39;,a&#39;)$ - Q(s,a)] . One can read this as . Qvalue = Current Qvalue + learning rate (expected future reward - current Q value) | Qvalue = Current Qvalue + learning rate (BELLMAN ERROR) | . def train(self, experience):state, action, next_state, reward, done = experience q_next = self.q_table[next_state] q_next = np.zeros([self.action_size]) if done else q_next #sample = [R + Discount * max(q values)] sample = reward + self.discount_rate * np.max(q_next) # our update function for q(s,a) = q(s,a) + learning rate(bellman error) # bellman error : sample - q(s,a)) self.q_table[state, action] = self.q_table[state, action] + self.learning_rate * (sample - self.q_table[state, action]) if done: self.eps = self.eps * 0.99 . . Bringing it all together . total_reward = 0 for ep in range(100): # no of episodes state = env.reset() # reset, start from the beginning done = False while not done: action = agent.get_action(state) # pass the &quot;current state&quot; and get a &quot;random&quot; or &quot;greedy action&quot; next_state, reward, done, info = env.step(action) # step through the env. the openaigym will do the rest. it will give you the reward, tell you if its complete and give other meta info agent.train((state, action, next_state, reward, done)) state = next_state # set the current state to the next state total_reward += reward # add the rewards print(&quot;s:&quot;, state, &quot;a:&quot;, action) print(&quot;Episode: {}, Total reward: {}, eps: {}&quot;.format(ep,total_reward,agent.eps)) env.render() print(agent.q_table) time.sleep(0.05) clear_output(wait=True) . Limitations of Q Learning . Q learning is good where the finite number of states and actions AND there are relatively small. If there are too many states and actions, the number of possibilities are very high and there wont be sufficient memory to hold the table. But the more important concern is that some problems cannot be easily expressed as distinct states and actions. . In the example from FROZEN LAKE, the states were SAFE, FROZEN, HOLE, GOAL. The actions were up, down, left and right. . Lets say in the game pac-man such as these, the states are very &quot;similar&quot; and yet by using a q table it will be shown as 2 different states, and hence we can conclude that it cannot be easily translated. Here state is not a &quot;single unique distinct&quot; entity rather it is more characterized as a &quot;collection&quot; of features. . . . Useful way to think about it a set of odd numbers. S={1,3,5} S1={7,9,11}. Both represent odd numbers. They are similar and represented as a collection. Both &quot;S&quot; and &quot;S1&quot; are states represented by &quot;ODD&quot; numbers but they contain different values in them. Just like the pac-man example, we need a &quot;same/similar&quot; action to be performed on them but looking at them individually value by value we fail to capture the &quot;pattern&quot; or the &quot;essence&quot; . The point here is we need to represent &quot;state&quot; as a collection and capture the essence rather than treating it as a single distinct entity. It is in cases like this, we take the help of a function which take in a bunch of features, &quot;understand their essence/patterns&quot; and spits out a Q-values for each action. See below. . . The best known universal function approximators today are none other than &quot;neural networks&quot;. We can employ neural networks to learn the Q-values for different states. Neural Networks need some basic components to work effectively. Lets see what they are and how we get can them. . Input : Bunch of features | Output : Q-Values for each action (of course, like before, we will chose the one with the highest value) | Loss Function : We can take the Bellman Error and Minimize them. However, the loss function we chose has to be &quot;differentiable&quot;. In other words, we should be apply backpropogation to transfer the loss from the &quot;output layer&quot; all the way back to the first hidden layer to update the &quot;weights or parameters $ phi $&quot; in the network. Lets see how we can convert &quot;bellman error&quot; to a differentiable function after which we are set to use the &quot;neural networks&quot; in our world of &quot;Q learning&quot;. . Note: Since we combined &quot;deep learning&quot; with &quot;Q learning&quot; - we call it. &quot;Deep Q Learning&quot; | . Our incremental update formula is . Q(s,a) = Q(s,a) + $ alpha $ [$ R(s,a,s^1) + gamma max_{a&#39;} Q(s&#39;,a&#39;)$ - Q(s,a)] . Consider y = $ R(s,a,s^1) + gamma max_{a&#39;} Q(s&#39;,a&#39;)$ . Q(s,a) = Q(s,a) - $ alpha $[Q(s,a) - y] . Just like the mean squared loss, we can square the bellman error to punish the large variances. . Q(s,a) = Q(s,a) - $ alpha [Q(s,a) - y]^2 $ . Lets apply differential function w.r.t to $ phi $ (parameters) to both sides of the equation . $ phi = phi - alpha frac{ partial (Q(s,a) - y)^2}{ partial phi} $ . Problems with DQN . Corelated samples . Unlike the SGD, the samples chosen during &quot;Q learning&quot; are close to each other and can overfit and does not converge to the global minima | Solution : maintain a queue of past experiences; use them to update the gradients. Breaks the close corelations | . | Moving Targets . Use 2 NN | Compute the labels using the TARGET Network. fix it for a while | Q Network will work as normal | Stablizes the training for a certain duration | . | Mentioned in the paper Mink et al. | shown to solve a lot of problems | . https://www.youtube.com/watch?v=RaIcFiNqP-0&amp;list=PLYgyoWurxA_8ePNUuTLDtMvzyf-YW7im2&amp;index=10 .",
            "url": "https://ablearn.io/reinforcement%20learning/2022/01/03/QLearning.html",
            "relUrl": "/reinforcement%20learning/2022/01/03/QLearning.html",
            "date": " • Jan 3, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Pytorch 101",
            "content": "Tensors . AutoGrad . Creating a (simple) model . class my_neural_network(nn.Module): def __init__(self): super(my_neural_network, self).__init__() self.flatten = nn.Flatten() self.linear_relu_stack = nn.Sequential( nn.Linear(28*28, 512), nn.ReLU(), nn.Linear(512, 512), nn.ReLU(), nn.Linear(512, 10) ) def forward(self, x): x = self.flatten(x) logits = self.linear_relu_stack(x) return logits input = torch.rand(1, 28, 28) model = my_neural_network() logits = model(input) pred_prob = nn.Softmax (dim=1) (logits) pred_prob.argmax(1) . Dataloaders . Datasets .",
            "url": "https://ablearn.io/machine%20learning/2021/12/11/Pytorch.html",
            "relUrl": "/machine%20learning/2021/12/11/Pytorch.html",
            "date": " • Dec 11, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Stereo Vision",
            "content": "Pinhole Camera Basics . Before we go explain this in detail lets clarify the terms . The object (flower) is said to be &quot;in the &quot;world&quot; and its position is given by &quot;world coordinates&quot; | Now imagine The light rays from the object pass through a pinhole and gets projected on a wall. The wall is the &quot;image plane&quot; and the hole it passes through is called the &quot;pinhole&quot;. The camera is placed at the pinhole. The camera is said to be in the &quot;Camera plane&quot; | The distance between the camera placed at the pinhole and the wall (image plane) is called &quot;focal length&quot; . Note: Our goal is to take the 3D coordinates (x,y,z) and project them camera plane (3D) and then later to the image plane (2D). We move from WORLD =&gt; CAMERA =&gt; IMAGE. This is called Forward Imaging Model. | . . Intrinsic and Extrinsic Parameters . Lets first move from WORLD =&gt; CAMERA . Imagine a camera placed somewhere in an empty room other than the corners. Fix a corner of this empty room as the origin. Then camera is at a distance from this corner. Additionally it will be at an angle to the orgin of the world coordinate, in our case, the corner of the room we have chosen. In computer vision terms, the camera is said to be &quot;translated (geometric translation, not language tranlsation :) ) and &quot;rotated&quot; w.r.t to the world coordinate. Hence, we need some sort of translation and rotation variables that will help us take the world cooridinates of the image and project them to the camera first. Right? We call this &quot;Extrinsic Parameters&quot; . Now, CAMERA =&gt; IMAGE. This is easy. We just have focal length (shown and described above). This is called Intrinsic parameters. . . Perspective Imaging with Pinhole . The above diagram is exactly same as &quot;Pinhole&quot; diagram with labels removed for explaining geometry.The yellow and blue triangles are similar. By the property of similar triangles (see below note), we have . $r_i/f = r_o/z$ . $ r_i $ and $ r_o $ are vectors describing the point on the image and world respectively. Hence we can split and write the equation as $ . Note: Triangles are set to be &quot;similar&quot; if they are of the same shape but not necessarily the same size. If 2 triangles have this property then *$AB/DE = BC/EF$* . . . Simple Stereo . Calibrated Stereo . Epipolar Geometry . Stereo Algorithms . References . OpenCV Calibration . Geometry of Image Formation .",
            "url": "https://ablearn.io/computer%20vision/2021/12/07/StereoVision.html",
            "relUrl": "/computer%20vision/2021/12/07/StereoVision.html",
            "date": " • Dec 7, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Optical Flow",
            "content": "In the previous blog post here we understand basics of &quot;features&quot; in an image. One of the most popular usecases for such tracking of features between 2 images would be detection motion of objects within the image. . Optical Flow vs Motion Field . Let&#39;s try to understand the terms first. . We take a object in the world (represented in the world coordinates) and project them onto a image plane. As the object moves in the world, we want to track it in the image plane. This tracking is called MOTION FIELD . It is not possible to directly measure motion field since all we have in the image are brightness patterns. . Using the variation in brightness patterns between consecutives images can help you track the object(s). This is called OPTICAL FLOW. . OPTICAL FLOW indicates MOTION of objects in most of the cases. But there could be images with &quot;illusions&quot; that changes brightness patterns but does not correspond to &quot;object movement&quot; in them&quot;. For eg. see below. . . The object is moving but there is no change in brightness pattern. So very hard to detect motion . . Here the light source is moving and hence the brightness pattern is changing, but there is no motion. . . Optical Flow Assumptions . Before we attempt to compute the optical flow. Lets assume 2 things . Brightness of a image point remains constant over time. Since we will be dealing with images taken in quick succession (in the order of milliseconds) this is a safe assumption. | . Mathemtically written as . $I(x+ delta x, y+ delta y, t+ delta t) = I(x, y, t)$ . Displacement ($ delta x, delta y $) is very small | Optical Flow Estimation . We will look at the &quot;classic&quot; methods of optical flow estimation. OPENCV provides a lot of inbuilt methods to compute them . Sparse Optical Flow . We compute optical flow by extracting important features only. We will use the &quot;feature extraction&quot; techniques from the earlier blog post and determine the optical flow. Sparse Optical Flow is fast but not very accurate. The code is available here. . For this exercise we use the dataset from KITTI Vision Car Dataset available here. . Load Images . #Load two consecutive images img1 = cv2.imread(&quot;images/000199_10.png&quot;) img2 = cv2.imread(&quot;images/000199_11.png&quot;) #Convert both images to grayscale ## We need the gray scale images for the functions below gray1 = cv2.cvtColor(im1, cv2.COLOR_BGR2GRAY) gray2 = cv2.cvtColor(im2, cv2.COLOR_BGR2GRAY) . Feature Extraction . We will use the Shi-Tomasi corner detector from OPENCV. To understand how to define the parameters, you can use this link . feature_params = dict(maxCorners = 500, qualityLevel = 0.01, minDistance = 10,blocksize = 3) prev = cv2.goodFeaturesToTrack(gray1,feature_params[&#39;maxCorners&#39;], feature_params[&quot;qualityLevel&quot;], feature_params[&quot;minDistance&quot;], feature_params[&quot;blocksize&quot;]) . By obtaining the features and drawing them, we obtain the following similar to below. . . Run Optical Flow . Launch the Lucas Kanade Optical Flow algorithm (cv2.calcOpticalFlowPyrLK) with the features and both grayscale images. . lk_params = dict(winSize = (15, 15) , maxLevel = 2 , criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03)) # img1 and img2 = images # prev = features of the first image next, status, error = cv2.calcOpticalFlowPyrLK(img1, img2, prev, None, winSize=lk_params[&quot;winSize&quot;], maxLevel=lk_params[&quot;maxLevel&quot;],criteria=lk_params[&quot;criteria&quot;]) #Store the Matches (status=1 means a match) good_old = prev[status ==1] good_new = next[status ==1] . Visualize . Go through each matched feature, and draw it on the second image . . Here is a video for a more intuitive show of the optical flow . Deep Optical Flow . In Deep Optical Flow we compute optical flow for every pixel. Just to clarify, we dont use deep learning (yet!) . The full code is available here . Load the images using openv functions as before | Calculate Optical Flow . Wait! no feature extraction? Yes thats correct. Since we are runnning optical flow on every pixel, we can directly use the corresponding opencv function. If you&#39;d like to understand how to tweak the parameters, you can visit this link . flow = cv2.calcOpticalFlowFarneback(gray1, gray2, None, 0.5, 3, 15, 3, 5, 1.2, 0) print(&quot;Shapes Gray &amp; Flow&quot;) print(gray1.shape) print(flow.shape) print(&quot; &quot;) &quot;&quot;&quot; Shapes Gray &amp; Flow (376, 1241) (376, 1241, 2) &quot;&quot;&quot; . . Important: Here flow is a 2d matrix which has the same shape as input but contains the &quot;$ delta x $&quot; (represented by U) and &quot;$ delta y$&quot; (represented by V) for each point (X,Y). . Accorinding to OPENCV, we get U and V in cartesian coordinates. To show this visually it will be good if we can find &quot;magnitude&quot; and &quot;direction&quot;. then we will intuitively know how much it moved and in what angle. Catesian system does not provide that. Luckily for us, &quot;polar system coordinates&quot; gives us what we want. . magnitude, angle = cv2.cartToPolar(flow[...,0],flow[...,1]) . Setting the HUE Value . Converting HUE to RGB . Video . . References . Optical Flow Definition . Workshops from Think Autonomous Course .",
            "url": "https://ablearn.io/computer%20vision/2021/12/06/OpticalFlow.html",
            "relUrl": "/computer%20vision/2021/12/06/OpticalFlow.html",
            "date": " • Dec 6, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "What's in an image?",
            "content": "What is a feature in an image? . Features : things that will help us identify the image. The best features are the ones where by looking at them once you are able to detect the image. Sometime 1 feature is not sufficient and you need to collection of them, each of them helping you identify the image. . . Look at the following image. If you pick a patch in the background, it does not tell anything about the image. infact, we can collect hundreds of them and still no use. Lets try something in the middle of the image. again, even If i give you hundreds of them surrounding it, it does not identify the image. Now, lets pick a corner of the image. IF you closely observe, there can be only 1 corner of that type. We can pick different corner all of which are unique and help you identify the image. Hence, you can consider, CORNER make a good collection of features. As such from the early days of computer vision, many algorithms were developed to detect &quot;corners&quot; of the image just because they are good features. features . . Why do we need features? . NEarly all computer vision algorithms in any domain dont just consume the image as a whole and produce an output. They are always preceeded by &quot;feature extraction&quot; and only then peforms their calculation. (In the world of machine learning, CNN layers are the feature extractors) . . What are feature descriptors? . features are just points on the image (referred to as &quot;keypoints&quot;) We need a &quot;describe&quot; them to make it more useful. featrure descriptors - as the name suggest, describe the surroudings of a &quot;keypoint&quot; by capturing few vital ingredients around the keypoint. Think of it as vector (a collection) used to describe. A good analogy would be a persons face. &quot;eye&quot;, &quot;color of the skin&quot;, &quot;hair&quot; etc. are features. color of the eye is &quot;brown&quot;, color is &quot;black/brown/white&quot;, &quot;Grey/Black/Blonde&quot; hair color are feature descriptors . . What is feature matching? . Given 2 images, we want to compare the features between each other. If they are similar images, then we will end up high degree of &quot;matches&quot; else we wont. its simple. . Ok why is this useful? . By tracking the features between 2 images, we can begin to understand if and how the objects in the images are &quot;moving&quot;. By doing this over mulltiple images, we can &quot;track&quot; the moving objects within an image, movement of a camera w.r.t to the image etc. . . The full working code is available here in google colab . . Use OpenCV to detect features and feature description . #Load Different Image Pairs img1 = cv2.imread(&quot;images/image0.jpg&quot;) img2 = cv2.imread(&quot;images/image1.jpg&quot;) . . . . # INITIALIZE FAST DETECTOR fast = cv2.FastFeatureDetector_create() kp = fast.detect(img1,None) # BRIEF DESCRIPTOR brief = cv2.xfeatures2d.BriefDescriptorExtractor_create() kp1, des1 = brief.compute(img1, kp) # KEYPOINTS DRAWN img1_kp = cv2.drawKeypoints(img1, kp1, None, color=(0,255,0), flags=0) cv2_imshow(img1_kp) . . . #REPEAT for the next image kp = fast.detect(img2,None) kp2, des2 = brief.compute(img2, kp) img2_kp = cv2.drawKeypoints(img2, kp2, None, color=(0,255,0), flags=0) cv2_imshow(img2_kp) . . . #FLANN MATCHING FLANN_INDEX_KDTREE = 0 index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5) search_params = dict(checks = 50) flann = cv2.DescriptorMatcher_create(cv2.DescriptorMatcher_FLANNBASED) des1 = np.float32(des1) des2 = np.float32(des2) matches = flann.knnMatch(des1, des2, 2) # store all the good matches as per Lowe&#39;s ratio test. good = [] for m,n in matches: if m.distance &lt; 0.7*n.distance: good.append(m) draw_params = dict(matchColor = (0,255,0), # draw matches in green color singlePointColor = None, flags = 2) img_briefmatch = cv2.drawMatches(img1,kp1,img2,kp2,good,None,**draw_params) cv2_imshow(img_briefmatch) . .",
            "url": "https://ablearn.io/computer%20vision/2021/12/05/ImageFeatures.html",
            "relUrl": "/computer%20vision/2021/12/05/ImageFeatures.html",
            "date": " • Dec 5, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "C++ Basics",
            "content": "Templates . Lambdas . Pointers . References . Cmake Tutorial .",
            "url": "https://ablearn.io/programming%20languages/2021/12/04/CPlusPlus.html",
            "relUrl": "/programming%20languages/2021/12/04/CPlusPlus.html",
            "date": " • Dec 4, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "fastcore - a must for every python dev",
            "content": "The purpose is to learn few snippets from FASTCORE so that we can improve our daily python skills and understand fastai better . from fastcore.all import * from nbdev.showdoc import * from fastcore.test import * from fastcore.nb_imports import * . Store attributes of a class easily . ################################ normal ############################################# class something: def __init__(self,a,b,c): self.a = a self.b = b self.c = c x = something(2,3,4) print(x.a) print(x.b) print(x.c) #output : 2 3 4 . ################################ using fastcore ############################## class something: def __init__(self,a,b,c): store_attr() #&lt;-- x = something(2,3,4) print(x.a) print(x.b) print(x.c) #output : 2 3 4 . Avoid boiler plate during Inheritance . ################################ normal ############################################################ class parent: def __init__(self): self.message = &quot;I am from parent class&quot; class child(parent): def __init__(self, e,f,g): store_attr() super().__init__() #&lt; need this to init parent class cc = child(2,3,4) print(cc.message) #output : I am from parent class ############################## using fastcore ############################################# # Creating a wrapper to the parent. call it BetterParent :) &quot; class BetterParent(parent, metaclass=PrePostInitMeta): #&lt; inherits both Parent + metaclass = PrePostInitMeta def __pre_init__(self, *args, **kargs): # PrePostInitMeta has &quot;pre&quot;, &quot;post&quot; init methods. # It creates the &quot;Parent&quot; everytime it is invoked by any child super().__init__() class child2(BetterParent): def __init__(self): pass #&lt; Just child init. BetterParent runs pre-init and creates parent. class child3(BetterParent): def __init__(self): pass #&lt; Just child init. no parent class init ever again! cc2 = child2() cc3 = child3() print(cc3.message) #output : I am from parent class # use cases : 1 to many relationships can be easily created with much boiler plate with this code . Alternative to lambda . import numpy as np arr = np.array([1,2,3,4,5]) y = lambda x : x.sum() y(arr) #output : 15 . import numpy as np arr = np.array([1,2,3,4,5]) y = Self.sum() #&lt; notice capital &quot;S&quot; (instead of s). Avoids lambda x: x y(arr) #output : 15 . Transforms . class A(Transform): #&lt;- pass the class def encodes(self, x): return x+100 #&lt;- implements the encode func def decodes(self, x) : return x-100 #&lt;- implements the decode func f=A() f(1) #output : 101 f.decode(1) # without the &quot;s&quot; #output : -99 #convert any func into a transform using the decorator. adds implementation to the encodes func. @Transform def specialfunction(x): return x+100 specialfunction(100) # same as specialfunction.encode(100) specialfunction.decode(100) # decode not implemented. just returns input #output : 100 . Pipeline . Executing transforms in a certain order. It can take a normal function and make it transform internally. see below . def add2(x) : return x+2 def mul100(y): return y*100 pipe = Pipeline([add2, mul100]) pipe(1) #. (1+2) * 300 = 300 #output : 300 # create a transform, add to pipeline class addsomemore(Transform): order =-1 #&lt;- decreased the order by 1. in the below examples it shift from 3rd to 2nd position def encodes(self,x): return x +1000 def decodes(self,y): return y -1000 adds = addsomemore() pipe = Pipeline([add2, mul100, adds]) pipe(1) #. (1+2) * 300 + 1000 = 1300! ##uncomment &quot;order&quot; and run. Result : (1+2) |||| + 1000 = 1003||| * 1000 = 100300 #output : 100300 . Better Partial functions . # normal def func100(x, y): &#39;&#39;&#39;this is func100&#39;&#39;&#39; return x + y func100.__doc__ #output &#39;this is func100&#39; # add2sonly = partial(func100, 2) add2sonly(4) # 6 add2sonly(10) # 12 add2sonly.__doc__ #output &#39;partial(func, *args, **keywords) - new function with partial application n of the given arguments and keywords. n&#39; #lost the doc string :( # # fastcore def func100(x, y): &#39;&#39;&#39;this is func100&#39;&#39;&#39; return x + y add2sonly = partialler(func100, 2) # partialler (and not partial) add2sonly(4) # 6 add2sonly(10) # 12 add2sonly.__doc__ #&lt;-- still has the doc string :) #output &#39;this is func100&#39; # . Monkey Patch-ing . ################################### normal ################################# class A: def somefunc(self): print(&quot;hello this is somefunc&quot;) def mk_patch_func(self): print(&quot;monkey patch&quot;) A.somfunc = mk_patch_func # replace the add of somfunc with mk_patch_func obj = A() obj.somfunc() #&lt;- calls monkey patch function (and not somefunc) #output : monkey patch ################################# fastcore ################################# class B: def funcb(self): print(&quot;hello this is somefunc&quot;) @patch #&lt;- decorator def funcb(self:B): #&lt;- pass the class name print(&quot;monkey patch&quot;) obj1 = B() obj1.funcb() #&lt;- calls monkey patch function (and not somefunc) #output : monkey patch . Type Dispatch . By def, When you have multiple implementation of the same function name, depending on the input, appropriate func is called. fastcore enables it for python . @typedispatch #&lt; add this decorator def callme(x, y:int): return x+y @typedispatch #&lt; add this decorator def callme(x, y:str): return f&quot;{x} {y}&quot; callme(1,2) #output : 3 callme(&quot;hello&quot;, &quot;world&quot;) #output : &#39;hello world&#39; . References . Blog post . fastcore lib .",
            "url": "https://ablearn.io/machine%20learning/2021/09/27/fastcore.html",
            "relUrl": "/machine%20learning/2021/09/27/fastcore.html",
            "date": " • Sep 27, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "CMake Basics",
            "content": "Typical CMAKE File . Follow the comments for the explanation . # boiler plate # cmake installed or version cmake_minimum_requred(VERSION 3.10) #project name project(&lt;&lt;your project name&gt;&gt;) # set the c++ standard in your project set(cmake_cxx_standard 11) set(cmake_cxx_standard_required True) # like print message(&quot;any message you want to print&quot;) #create executable add_executable(&lt;&lt;exec name&gt;&gt; program.cpp) /* - Create a library (SHARED or static). - The difference between them is that when its declared as STATIC, all the code is inserted into the main program, hence the main program is indepedent,SHARED = opposite, hence smaller in size */ add_library(&lt;&lt;lib exec name&gt;&gt; SHARED (or STATIC) /path/to/lib.cpp) /* - force the executable and library to build into a path you can look at it later(and not guess) - CMAKE_BINARY_PATH = default variable given by CMAKE - where ever you rum the CMAKE command. Usually in the BUILD folder */ - set(EXECUTABLE_OUTPUT_PATH ${CMAKE_BINARY_PATH}/bin) - set(LIBRARY_OUTPUT_PATH ${CMAKE_BINARY_PATH}/lib) # now that we build and forced them to go into a path, lets link both the MAIN &lt;-&gt; LIB target_link_libraries(&lt;&lt;main exec name&gt;&gt; &lt;&lt;library exec name&gt;&gt;) #in case you want to find popular package find_package(&lt;&lt;package name&gt;&gt;) if(&lt;&lt;package name&gt;&gt;_FOUND) message (&quot;Found!&quot;) include_directories(${&lt;&lt;packagen name&gt;&gt;_INCLUDE_DIRS}) target_link_libraries(&lt;&lt;main exec name&gt;&gt; ${&lt;&lt;package name&gt;&gt;_LIBRARIES}) else if (NOT &lt;&lt;package name&gt;&gt;_FOUND) message(&quot;not found!&quot;) endif() . CMAKE EXEC . cmake . //means find CMakeLists.txt in the current dir. . it is often best practice to create a BUILD dir and then run cmake in it . mkdir buid cd build . cmake .. // this will look for CMakeLists.txt in the &quot;parent&quot; directory BUT will build files in the current dir //CMAKE will create some files that will enable the build in the next step cmake --build . /// build the project from the CURR DIR and generate the executables . References . Cmake Tutorial .",
            "url": "https://ablearn.io/programming%20languages/2021/09/22/CMake.html",
            "relUrl": "/programming%20languages/2021/09/22/CMake.html",
            "date": " • Sep 22, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "Deep Learning Basics",
            "content": "Pytorch . Model Eval . with torch.no_grad() : # Switch off automatic differentation output = learn.model.eval()(x) # Evaluate models by not considering the batch norm, drop out layers etc. ##Inference mode . Detach . Remove a tensor (here &quot;x&quot;) from the computational graph, which reduces memory foot print. . Detaches and clones a tensor . x.detach() y = x.detach().clone() . Model . learn.model[0] #model is made up of &quot;sequential&quot; function contaniner. # sequential(sequential()) # neural networks stacked on top of each other . einsum . einsum or Enstein summation is basically short end notation to express actions on tensors (or typically matrices) like transpose, multiplication, sum, dot product etc. . torch.einsum (&#39;ab,bc -&gt; ac&#39;, A, B) &#39;ab,bc -&gt; ac&#39; ==&gt; notation. a,b,c,d are dimensions. &quot;,&quot; means multiply You are telling that take 2 inputs of dimensions ab and bc and generate an output that gives ac. A,B ==&gt; matrices . Class Activation Map . Areas used to determine class = activations of the last layer of conv * weights of the fully connected layer . Last layer of activation (before the fully connected layer) shows where the model is focusing. | Needs a global average pooling layer in the network (such as RESNET) . | Why before global max pooling layer? The global max poolng layer unlike the other maxpool layers will squash all features into 1 linear vector. The max pooling layer before the fully connected layer will squash all local activations, normalize them and feed them to FC. Until then you will have localized features that model is looking at. In other words, you will have location of the image where the model is focused on. . | How does dot product help? . | . . Learns Features. Stores Weights . . Use those weights with the activations and figure which one to focus on, which one to omit. . Drawbacks . The architecture needs to have global max pooling layer. Only then can we take the layer before that. | The method can only look at the final layer of the CNN and show why the model predicted what it did. It cannot show any later prior. | These drawbacks are addressed by the GRAD GAM described below | . Calculate the gradients by running .backward() function. (Pytorch does not store them, hence need to calc again during inference) | Average the GRADIENTS of the feature maps of the last conv layer (= weights) | Multiply WEIGHTS vs ACTICATIONS (as in CAM) to get the CAM Map to display. | . Pros . over comes all the issues of vanilla CAM | works for any images tasks (classification, segmentation, vQA) | . Cons . cannot locate mulitple objects within the images. | . . Why gradients equal same size as activation maps? | . The gradient is calculated for each pixel in the feature map. For example, if the activation map is 512 x 7 x 7, then then the number of graidents are also 512 x 7 x 7 . Why averaging gradients yields weights? | . CAM uses WEIGHTS at the Fully Connected layer to choose the &quot;feature maps&quot; that is more relevant and squash the ones which are not. So it is highly dependent of (CONV Layer =&gt; Global Average Pooling Layer ==&gt; FC Layer) network. GRAD-CAM uses this concept by make its more general. It uses GRADIENTS to provide the weights. We use the GRADIENTS in the last conv layer, do the global average pooling ourselves, and now we have our weights! we dont have to depend on specific GAP layer nor the weights of the fully connected layer. GRADIENTS provide a good enough &quot;weighting mechanism&quot; to pick the feature map that is relevant and squash the one which we dont. **Deep neural networks as well act as information distillation pipeline where the input image is being converted to a domain which is visually less interpretable (by removing irrelevant information) but mathematically useful for convnet to make a choice from the output classes in its last layer** . References . https://glassboxmedicine.com/2020/05/29/grad-cam-visual-explanations-from-deep-networks/ . Matplot Lib Basics . fig, ax = plt.subplot(3,2,figsize=(5, 5)) # Rows = 3; Columns=2; Total = 5 plots === Set the figure size to 5 inches to 5 inches # Note that the size is defined in inches, not pixels axs[0, 0].hist(data[0]) #1st axis axs[1, 0].scatter(data[0], data[1]) #2nd axis axs[0, 1].plot(data[0], data[1]) #3rd axis axs[1, 1].hist2d(data[0], data[1]) #4th axis plt.show() # show the plot ax.imshow() # show the image #interpolation = use known data at unknown places (like extrapolate, interpolate) . Check out various types here of interpolation here . Python Decode Function . When you encode using a class (string class, data loader class etc.). you can use decode to undo it. Useful in bring back the image to its original form to display while &quot;intrepreting&quot; the test results. . IMAGE ==&gt; RESIZE ==&gt; ENCODE (normalize to image net stats or something similar) ==&gt; Output . | Output ===&gt; DECODE ==&gt; Original image (but still includes the resize) ==&gt; Display (-able) . | .",
            "url": "https://ablearn.io/machine%20learning/2021/08/11/DeepLearning.html",
            "relUrl": "/machine%20learning/2021/08/11/DeepLearning.html",
            "date": " • Aug 11, 2021"
        }
        
    
  
    
        ,"post9": {
            "title": "OpenCV Essentials",
            "content": "Upload a file to the google colab . Image Reading . Read the image using opencv&#39;s imread . image = cv2.imread(&quot;image.jpg&quot;) h = image.shape [0] # first dim of the shape is HEIGHT w = image.shape [1] # second dim of the shape is WEIGHT . Image Resize . Here we are resizing the width to 50 pixels. We have to make sure the height is proportionally reduced. . # find out the ratio by dividing the &quot;new&quot; width / &quot;old width&quot; r = 50.0 / image.shape[1] # multiple the ratio with the height # creating a dimension tuple; (weight, height) dim = (50, int(image.shape[0] * r)) # resizing of the image resized = cv2.resize(image, dim, interpolation=cv2.INTER_AREA) plt.imshow(&quot;Resized (Width)&quot;, resized) . Image interpolation occurs when you resize or distort your image from one pixel grid to another. Zooming refers to increase the quantity of pixels, so that when you zoom an image, you will see more detail. Interpolation works by using known data to estimate values at unknown points . There are many interpolation methods in opencv. Common ones include . cv2.INTER_NEAREST | cv2.INTER_LINEAR | cv2.INTER_AREA | cv2.INTER_CUBIC | cv2.INTER_LANCZOS4 | . ColorSpaces . Convert the image from original to the HSV color space. . hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV) plt_imshow(&quot;HSV&quot;, hsv) . . . Here is a list of all opencv color spaces to use . Translation . Translation means &quot;geometrical translation&quot; and achieved through providing a special matrix called the Translation Matrix to the opencv translation function. . The transformation matrix [[ 1, 0, X] [0, 1, Y]] #X ==&gt; left and right** | #Y ==&gt; up and down** | . | . Use the positive values for X and Y for &quot;shifting right&quot; and &quot;shifting down&quot; Use the negative values for X and Y for &quot;shifting left&quot; and &quot;shifting up&quot; . # Shift the image 25 pixels to the right and 50 pixels down Mn = np.float32([[1,0,25], [0,1,50]]) # maintains colinearity - maintains parallelism, ratio is maintained shifted_ing = cv2.warpAffine(image, Mn, (image.shape[1], image.shape[0])) #show the image plt.imshow(shifted_ing) . . Rotation . image = cv2.imread(args[&quot;image&quot;]) # get a height and width of the image (h, w) = image.shape [:2] # get the rotation which is usually the center of the image rotation_point = (h/2, w/2) # get the rotation matrix M = cv2.getRotationMatrix2D(rotation_point,45,1) #center, angle (# clock wise = negative; # counter clock wise = positive) , scaling . . If you do not want the side to cropped off we have to update the rotation matrix values. Refer the link here . Note: imutils is a thin wrapper around opencv function and can do the &quot;rotate bound&quot; in 1 line rather than computing the rotation matrices ourselves using the sin/cosine values . Drawing . canvas = np.zeros((300, 300, 3), dtype=&quot;uint8&quot;) # create a blank image green = (0, 255, 0) # set the color (B,G,R) cv2.line (canvas, (0,0), (300,300), green, 2) # draw a green line from 0,0 to 300,300 with thickness = 2px wide plt.imshow(canvas) ################################# red = (255, 0, 0) cv2.line (canvas, (300, 0), (0,300), red, 2)# draw a red line from 0,0 to 300,300 with thickness = 2px wide plt.imshow(canvas) ################################# blue = (0, 0, 255) cv2.rectangle(canvas, (200,100), (100,200), blue, 2) # draw a rectange ; provide the 2 corners (top right, left bottom) of the rectangle plt.imshow(canvas) ################################# pink = [255, 192, 203] cv2.circle(canvas,(150, 150), 25, pink, 2) # draw a pink circle with a center at (150, 150) and radius of 25px wide plt.imshow(canvas) ################################# random_color = np.random.randint(0, 255, (3,)).tolist() # random color cv2.line (canvas, (0, 150), (300, 150), random_color, 2) # draw a line plt.imshow(canvas) . . Image Arthimetic . images in opencv are opencv ar numpy arrays from values 0-255. If you add and go over 255, it will max out to 255 and vice versa at 0 . added = cv2.add(np.uint8([200]), np.uint8([100])) subtracted = cv2.subtract(np.uint8([50]), np.uint8([100])) print(&quot;max of 255: {}&quot;.format(added)) print(&quot;min of 0: {}&quot;.format(subtracted)) . . Suppose you use numpy operations, it does not max out rather performs a &quot;modulo&quot; operation i.e If you exceed 255, then its goes around and starts from 0 (like a clock starting from 1 after 12). Similarly if you go below 0, then it starts going to values less than or equal to 255. . added = np.uint8([200]) + np.uint8([100]) subtracted = np.uint8([50]) - np.uint8([100]) print(&quot;wrap around: {}&quot;.format(added)) print(&quot;wrap around: {}&quot;.format(subtracted)) . . We can use the arthimetic to darken or lighten the image. . To lighten we have to . Create a matrix with the exact same dim as the input image filled with ones | Multiply by some arbritary high number (say 100) | ADD the input image and matrix | . . M = np.ones(image.shape, dtype=&quot;uint8&quot;) * 100 added = cv2.add(image, M) plt_imshow(&quot;Lighter&quot;, added) . . To darken we have to . Create a matrix with the exact same dim as the input image filled with ones | SUBTRACT the input image and matrix | . . M = np.ones(image.shape, dtype=&quot;uint8&quot;) * 50 subtracted = cv2.subtract(image, M) plt_imshow(&quot;Darker&quot;, subtracted) . . Important: The data type (dtype) has to be &quot;uint8&quot; or else these opencv operations of add or subtract does not work as intended. .",
            "url": "https://ablearn.io/computer%20vision/2021/08/02/opencv.html",
            "relUrl": "/computer%20vision/2021/08/02/opencv.html",
            "date": " • Aug 2, 2021"
        }
        
    
  
    
        ,"post10": {
            "title": "Reinforcement Learning - Basics",
            "content": "Purpose . In this blog post, I try to understand the intution behind the math for reinforcement learning. If you understand the basics and how the math is derived, then all the algorithms following this blogpost are just a simple variation which can be easily understood. I will try to provide a &quot;english&quot; explanation to all the math and hide the math, but for the mathematically inclined people, you can expand and check out. Finally, after the blog post, for all the algorithms the corresponding code will be provided. . . Basic Terms . Unlike the supervised or unsuperivsed learning algorithms, it is used in constantly changing or dynamic environment . | There are 2 entities involved: . AGENT : Software involved in getting input and executing actions. | Environment : House that the agent is trying to execute the actions. | . | Input . Take a set of observations (or states) | . | Process (or policy) . Figures out the right set of actions | If we used &quot;deep learning&quot; for this, then it becomes Deep Reinforcement Learning | Takes an input &quot;oldSTATE&quot; and outputs an &quot;ACTION&quot; that goes to a &quot;newSTATE&quot; | Think of it as a &quot;plan&quot;. If you &quot;plan&quot; to take the &quot;freeway&quot; today to avoid internal city roads - thats a plan | if you are planning to city roads to avoid &quot;congestion&quot; on the freeway - thats a plan (or policy). you are optmizing &quot;time&quot; in both but following different policy | . | Output . Right set of commands to follow | . | Feedback . It the output received positive feedback, great, remember and repeat when possible | If not, change to another action. | . | . . . Process . There are 2 types of process in the real world. In the visual below, A =&gt; B and B=&gt;C and its the only path of transportation. Most often we counter a scenario that A has 80% probability go to B and 20% probability to go to C . A, B, C are the states. | RIGHT and DOWN are actions | 0.8, 0.2, 1.0 are transition probabilities | The decision to go from A=&gt;B=&gt;C is a policy ; A=&gt;B is a policy ; A=&gt;C is a policy | . . . . Markov Property . For any given process, given a set of states, actions, transition probabilities, reward function, it is said to folloq the markov property if . The future state is independent of the past i.e all the information needed to predict the future is contained in the present and there is no need to know the history. In other words, the transition probability function follows the markov property. . . Goal of the any Reinforcement Learning . The expected long term reward should be maximum. In other words. find a &quot;policy&quot; that is optimal.&quot;Optimal&quot; meaning you can get a maximum reward over time (not just immediate) . R = $[r_{t}, r_{t+1}, r_{t+2},.....r_{∞}]$ . There are infinite sequences and hence infinite rewards. how to solve this? . Restrict to finite sequences. Some problems are inherently finite (robot catching a ball) | Make sure we reach the terminal states for every possible policy | Discounted rewards - exponentially reduce the future rewards. | . Future (unexpected) rewards are valued exponentially less than the immediate ones. . So mathematically we can write it as . R = $r_{t} + gamma^{2} r_{t+1} + gamma^{3} r_{t+2} + ....+ gamma^{∞-1} r_{∞}$ . Where $ gamma$ = discount factor and is between 0 and 1 . If $ gamma$ is low, then it means we only care about the immediate rewards, since all the future becomes tiny and eventually becomes 0 . If $ gamma$ is high, then it means we care a great deal about our future rewards. . If $ gamma$ is 1, we equally care about the rewards in all the timesteps . In essence, $ gamma$ tell us about the horizon of rewards we are interested in . It is shown visually below . . . State Value and Action Value Function . From the above equation for rewards we can quantify few things such as . What is the value of an agent being in state S? . From the reward function above we can conclude that it should be all the rewards in every time step from the state S to the terminal state by following a policy $ pi $ . V $_ pi(s)$ = $ E_{ pi} sum limits _{t=0} ^{t-1} gamma^{t}r_{t} |s_{t} = s| $ . | It might look daunting, but its the exact same equation as above (rewards equation) computed for any given state s for its value V. . | If you want, you can simply think of its as V(s) = sum of all rewards following a certain plan (or policy) Now, let&#39;s say I come and tell you that the policy is the best possible one (&quot;optimal&quot;) then immediately you can conclude the value is also the best possible one for that state. In other words, the policy (or plan) is optimal it should yield optimal value. . $V_ pi(s)^{*}$ = $ E_{ pi}^{*} sum limits _{t=0} ^{t-1} gamma^{t}r_{t} |s_{t} = s| $ . | &quot;*&quot; indicates its optimal. Just an convention. you can use whatever you want. . | . | What is the value of an agent being in state S by choosing action A? What is this action worth? . This is exact same question as above, but a bit more granular. We know that from a given state you can pick many actions (a1, a2, a3) and so on. What is the value of a state if you pick action a1 - then follow policy $ pi $ | What is the value of a state if you pick action a2 - then follow policy $ pi $ | . | The reason for breaking this down is that we know an action is picked based on a certain probability. Depending on the probability value, a1 or a2 or a3 is picked and based on that you go to a certain state and hence you might get a different overall value V $_ pi(s)$ Written mathematically, | . Q $_ pi(s,a)$ = $ E_{ pi} sum limits _{t=0} ^{t-1} gamma^{t}r_{t} |s_{t} = s, a_{t} = a| $ . The &quot;Q value&quot; is the value of an agent in state S by chosing action A then following optimal policy . | To get a &quot;optimal&quot; Q value, we have to make sure 2 things . the action we have taken has to be very good, infact the best one! | the policy after that has to be optimal! . We do the above things, then we get $Q^{*}$ (optimal Q value) . Combining these 2 above concepts, . | . Q value = probability of executing an action A in the first place * [Current reward + (discounted future rewards)] . $Q^{*}(s,a)$ = argmax $ sum limits_{a in A} $ P($s^{1}|s,a$) [ $ R(s,a,s^1) + gamma V^{}(s^1)$] ........(1) . Optimal value of state value V = For all possible actions take the MAX of (probability of executing an action A in the first place * Q value) . $V^{*}(s) $ = argmax $ sum limits_{a in A}$ $Q^{*}(s,a)$ ........(2) . Substitute (2) in (1), we get bellman equation in terms of Q values . $Q^{*}(s,a)$ = $ sum limits_{a in A} $ P($s^{1}|s,a) $ [$ R(s,a,s^1) + gamma max Q^{}(s&#39;,a)$] . This is the BELLMAN EQUATION for the Q values and forms a corner stone in RL . You can substitute the other way and get it in terms of &quot;V&quot; values. In this and future blog posts, I intend to explore Q learning and its variants, so i am comfortable looking at it from the &quot;Q Value&quot; perspective. . . Solving Bellman Equation . Lets recap, . By using the concept of DISOUNTED REWARDS, we arrived at a equation for calculate expected rewards | Using the above concept, we were able to quantify 2 things in our processess. . State value (or &quot;V&quot; Value) - value of a state given a policy. From a state, you have multiple actions to follow. | Action value(or &quot;Q&quot; Value) - value of a state after following a particular action, then a given policy. This is a more specific version of the &quot;state value&quot;. | . If i asked to pick the best action, it means i am asking to choose an action with the best Q value, which inturns leads to the best &quot;State&quot; value. In other words, you put your best foot forward, what else did you expect :) ? . | . Note that the bellman&#39;s equation is recursive, meaning Q(s,a) is dependent of Q(s&#39;, a) and so on. Lets simplify and understand the intuition first. . Q(s, a) = something + Q(s&#39;, a) . Q(s&#39;, a) = something + Q(s&quot;, a) . Q(s&quot;, a) = something + Q(s&quot;&#39;, a) . Assume Q(s&quot;&#39;, a) is the final state, then for . Q(s&quot;&#39;, a) = we get the value since its a terminal state . now go back and substitute, to get Q(s&quot;&#39;, a), then substitute again to get Q(s&#39;, a) and finally we get Q(s, a) . This concept of solving subproblems which ultimately leads to the final answer being calculated is called dynamic programming. . So we can modify the equation ever-so-slightly to show the updates. (Bellman equation writtten to show iterative updates) . $Q_{i+1}(s,a)$ = $ sum limits_{a in A} $ P($s^{1}|s,a) $ * [$ R(s,a,s^1) + gamma max Q_{i}(s&#39;,a)$] . In our example, v(s&quot;&#39;) = iteration 1, v(s&quot;) = iteration 2 etc. . So, given the TRANSITION PROBABILITIES and REWARD FUNCTIONS, we can easily apply the dynamic programming to the bellman equation and solve it!! Take an example here. . You are starting from random values, and then fixing them to show correct ones. Then finally from start to finish pick the box with the least cost. This is called &quot;value iteration&quot;. . . . . There are couple of other variants called &quot;Policy iteration&quot; and &quot;Hybrid Valuation&quot; which updates the policy. It will explained in another blog post. . . Temporal Differencing . In the world of reinforcement learning, we are not given &quot;transition probabilities&quot; and Rewards functions to different states are not known to begin with. The agent interacts multiple times (called episodes) and only after that &quot;figures&quot; out. how do we update the bellman equation in each iteration if the values are unknown? We update the equation for each interaction with the environment. Initially, we have zero knowledge, so we will pick next states randomly. Our sample size is just 1. Just with 1 interaction its impossible to say the random state we jumped to is optimal or not. So we adjust the equation by adding a &quot;learning rate&quot; parameter that is usually very low (eg. 0.001). Next we repeat the steps, each time our sample is increasing, and obviosuly our confidence in knowing the entire env is also increasing. Only after a certain # of episodes we establish a full knowledge of the env. . Every time you take an action. A from state S, you get a sample of $P(s^{1}|s,a$) and a corresponding $R(s,a,s^1)$ . Also, it wont be broken into 2 parts, rather it will fused into 1 value. say sample 1 at the end, you will get S = {sample1, sample2, sample3...sampleEnd} Basically, its a probability distribution given by . sample = $R(s,a,s^1)$ + $ gamma Q_{i}(s^1)$ . TD update equation is written as . Q(s,a) = Q(s,a) + $ alpha $ [$ R(s,a,s^1) + gamma max_{a&#39;} Q(s&#39;,a&#39;)$ - Q(s,a)] . One can read this as Qvalue = Current Qvalue + learning rate (expected future reward - current Q value) Qvalue = Current Qvalue + learning rate (BELLMAN ERROR) . You are learning the Q values as you go, and hence the equation represents a algorithm called &quot;Q learning&quot; . The intuition behind this is :i dont trust the sample too much but just a little. (like a good stranger) I started with a random value, but i am not sure whether i &quot;over quoted&quot; the Q value or &quot;under quoted&quot;. Suppose the ideal Q value is 50. But the random value you started is 80. After getting the first sample, Qvalue = Qvalue + learning rate (BELLMAN ERROR) where BELLMAN ERROR is negative, its tell you reduce 80. but not too much just by a small factor (lets say 0.001). Then try to get a sample from env, again try the same transition from s -&gt; s&#39;, you get negative again. you reduce 80 again. and so on... The point here is that rate of change of Q value tell you which direction to move (postive or negative in our example), learning rate tell you the TRUST factor based on which you keep updating Q values.&gt; Formally stated, treat a single sample as a representative of the distribution and apply an &quot;incremental&quot; update to reduce the &quot;bellman error&quot; . Note:For those people familiar with deep learning, this is similar to the &quot;stochastic gradient learning&quot; where you updates for each input, and since you are looking at it piece by piece, you want to adjust the &quot;update&quot; to the model by a learning rate parameter. From here onwards, all we will focus how to learn Q accurately. If there is magic wand, and we could know the optimal Q values between states, thats your algorithm 1. Suppose someone hands over a table, thats an option too! Life is not easy and so we will end up starting randomly and use the above equation . . Glossary . Value vs Reward . Value = long term benefits | Reward = Immediate feedback | If its too long, environment has changed, your algorithm or function is obsolete | If its too short, its not a visionary and missed crucial opportunities | . Explore vs Exploit . Agent explores new environment and learn ==&gt; updates policy | Agent exploits exising environment, fine tunes and gets good at it! | Our problem : Balance Exploit vs Explore | . Dynamic Programming . Divides a problem into multiple sub problems | Solves the simplest sub problem first and then does this recursively until all of them are solved | Solving all the sub-problems will ultimately solve the bigger problem. | . References . RL Basics . RL Solving MDP and Reward Functions . RL-Solving MDP . Markov Property .",
            "url": "https://ablearn.io/reinforcement%20learning/2021/08/02/RL.html",
            "relUrl": "/reinforcement%20learning/2021/08/02/RL.html",
            "date": " • Aug 2, 2021"
        }
        
    
  
    
        ,"post11": {
            "title": "Pandas vs SQL",
            "content": "JOIN . SELECT * FROM TABLE a join TABLE_B b on a.column1 = b.column2 . pd.merge( left = left_dataframe, right = right_dataframe, how =&#39;left&#39; # what kind of join? right/left/outer suffixes=(&#39;_fa&#39;, &#39;_ag&#39;), # by default, it will _x, _y to the common columns between the tables. This will add meaningful suffixes left_on=[&#39;CustomField19&#39;], # left table columns to join right_on=[&#39;AssetId&#39;]) # right table column to join pd.merge( left = left_dataframe, right = right_dataframe, how =&#39;left&#39; # what kind of join? right/left/outer suffixes=(&#39;_fa&#39;, &#39;_ag&#39;), # by default, it will _x, _y to the common columns between the tables. This will add meaningful suffixes on=[&#39;CustomField19&#39;], # common columns to join. it will intersect for values which are diff . RENAME . sp_rename(table.colA, table.ColB) . df.rename(columns = {&quot;colA&quot;: &quot;colB&quot;}, inplace=True) . WHERE . SELECT * FROM table WHERE colName = Value . df[[df.ColName == Value]] . DISTINCT . SELECT DISTINCT Col1, Col2 FROM table WHERE colName = Value . df[[&#39;col1&#39;, &#39;col2&#39;]].drop_duplicates() . UDPATE column from another column . UPDATE tbl Set ColA = ColB Where ColC = &#39;somevalue&#39; . df[&#39;ColA&#39;] = df.apply(lambda x: x[&#39;ColB&#39;] if x[&#39;ColC&#39;]=&quot;somevalue&quot; else x[&#39;ColA&#39;], axis = 1] . RANK . select row_number() OVER (order by col) from TableA . df[&quot;ColA&quot;].rank(method=&quot;max&quot;) default = average possible : max, min, first, dense . References . updates in pandas .",
            "url": "https://ablearn.io/data%20engineering/2021/07/14/pandas.html",
            "relUrl": "/data%20engineering/2021/07/14/pandas.html",
            "date": " • Jul 14, 2021"
        }
        
    
  
    
        ,"post12": {
            "title": "Setup Notebooks Remotely",
            "content": "Setup Remote Jupyter Notebook . Useful in cases where you want to access jupyter notebooks from MAC or WINDOWS but want to setup UBUNTU servers with jupyter notebooks. . CREATE EXEC SCRIPT . CREATE jupyter-startup.sh | . source ~/miniconda3/etc/profile.d/conda.sh conda activate YOUR_CONDA_VIRTUAL_ENVIRONMENT jupyter notebook . CREATE JUPYTER SERVICE . CREATE a following jupyter service in your home directory | COPY the file to this destination ==&gt; /etc/systemd/system/jupyter.service | . [Unit] Description=Jupyter Notebook Service Description [Service] Type=simple PIDFile=/run/jupyter.pid ExecStart=/home/YOUR_USER_NAME/jupyter-startup.sh User=YOUR_USER_NAME Group=YOUR_USER_NAME Restart=always RestartSec=10 [Install] WantedBy=multi-user.target . - sudo systemctl enable jupyter.service - sudo systemctl daemon-reload - sudo systemctl restart jupyter.service . Important :Pick either option 1 or 2 (but not both). . SSH Tunnel (option 1) . 1) Setup a password to the notebook server so that we dont have to provide token every single time. On the remote server, do the following . jupyter notebook password #Enter password: **** #Verify password: **** #[NotebookPasswordApp] Wrote hashed password to /Users/&lt;&lt;you&gt;&gt;/.jupyter/jupyter_notebook_config.json . 2) Run SSH tunnel to setup a link on the local host . Pick any port number for LOCAL_PORT. You may need to enter a passphrase. . ssh -N -f -L localhost:&lt;LOCAL_PORT&gt;:localhost:8080 username@remotemachine -p &lt;&lt;PORT ID&gt;&gt; -v . 3) Type http://localhost:8888/tree in your local browser to access the jupyter notebook setup remotely. . . PUBLIC URL (option 2) . CONFIG JUPYTER NOTEBOOK . Create Jupyter Notebook Configuration | . jupyter notebook –generate-config . CREATE PASSWORD and CERTIFICATES . First generate a new password to login | . from notebook.auth import passwd import IPython passwd() . Enter password: ········ Verify password: ········ . &#39;sha1:6ed7a3ec3487:57c304ac1ba182fa7c160d2fcac788a2c376cb5f&#39; . Generate the certificate file. It will be used in the configuration step | . openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout mykey.key -out mycert.pem . . EDIT NOTEBOOK CONFIGURATION FILES . cd .jupyter nano jupyter_notebook_config.py . Open the configuration file and edit as follows . c.NotebookApp.allow_origin = &#39;*&#39; | c.NotebookApp.allow_remote_access = True | c.NotebookApp.certfile = &#39;YOUR_CERTIFICATE_FILE&#39; | c.NotebookApp.ip = &#39;0.0.0.0&#39; | c.NotebookApp.open_browser = False | c.NotebookApp.password = u&#39;&#39;&lt;/li&gt; c.NotebookApp.port = ANY_PORT_YOU_LIKE | &lt;/ul&gt; &lt;/li&gt; &lt;/ul&gt; EDIT FIREWALL RULES (if needed) . sudo ufw allow from &lt;&lt;IP ADDRESS&gt;&gt; . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; References . Config Jupyter Notebook | Config Jupyter Notebook 2 | Create Jupyter Service | Create Jupyter Service 2 | Configure Linux firewall | . &lt;/div&gt; | . | .",
            "url": "https://ablearn.io/machine%20learning/2021/05/22/SetupJupNotebook.html",
            "relUrl": "/machine%20learning/2021/05/22/SetupJupNotebook.html",
            "date": " • May 22, 2021"
        }
        
    
  
    
        ,"post13": {
            "title": "Bite Sized ML Topics",
            "content": "Padding in CNNs . Why use padding? In addition to the aforementioned benefit of keeping the spatial sizes constant after CONV, doing this actually improves performance. If the CONV layers were to not zero-pad the inputs and only perform valid convolutions, then the size of the volumes would reduce by a small amount after each CONV, and the information at the borders would be “washed away” too quickly . Reference . Linear vs Logistic Regression . In linear regression, the outcome (dependent variable) is continuous. It can have any one of an infinite number of possible values. | . If X contains the area in square feet of houses, (200, 300, 3000) Y contains the corresponding sale price of those houses, ($1000, $2000, $200,000)linear regression to predict selling price as a function of house size Y = f(x) . In logistic regression, the outcome (dependent variable) has only a limited number of possible values. Logistic Regression is used when response variable is categorical in nature | . If, instead, you wanted to predict, based on size, whether a house would sell for more than 200K, you would use logistic regression. The possible outputs are either Yes, the house will sell for more than 200K, or No, the house will not . What is data augmentation? . Overfitting is caused by having too few samples to learn from, rendering us unable to train a model able to generalize to new data. Given infinite data, our model would be exposed to every possible aspect of the data distribution at hand: we would never overfit. Data augmentation takes the approach of generating more training data from existing training samples, by &quot;augmenting&quot; the samples via a number of random transformations that yield believable-looking images. The goal is that at training time, our model would never see the exact same picture twice. This helps the model get exposed to more aspects of the data and generalize better. . What is Momentum? . Momentum helps to speed the optimization process toward the minimum of the loss and get out of saddle points by adding a running average of previous gradients and use that average instead of the current batch of data. It forces the gradient descent toward the correct direction to the loss by making the convergence faster and by reducing oscillations . What is learning rate? . The learning rate is how quickly or how slowly a network updates old parameters for new ones. By default, the learning rate is held constant, however, this way may cause some issues such as: • If the learning rate is too small, it will take a long time to reach the optimum, or maybe never reach it. • If the learning rate is too big, it will keep bouncing around the optimum. The optimization method may get stuck in shallow valleys! . What are auto-encoders? . Autoencoders are a specific type of feedforward neural networks where the input is the same as the output. They compress the input into a lower-dimensional code and then reconstruct the output from this representation. The code is a compact “summary” or “compression” of the input, also called the latent-space representation. An autoencoder consists of 3 components: encoder, code and decoder. The encoder compresses the input and produces the code, the decoder then reconstructs the input only using this code. . . To build an autoencoder we need 3 things: an encoding method, decoding method, and a loss function to compare the output with the target. Autoencoders are mainly a dimensionality reduction (or compression) algorithm with a couple of important properties . Data specific | Lossy | Unsupervised | . Why batches in training ml model? . Ideally, we&#39;d like to use all our data for every step of training because that would give us a better sense of what we should be doing, but that&#39;s expensive. So, instead, we use a different subset every time. Doing this is cheap and has much of the same benefit. . . CNN . CNN Concepts . CNN Shapes Understanding . Pooling . Extra Read . https://www.jeremyjordan.me/semantic-segmentation/ . Epoch/Batch/Iterations . Visual of EPOCH vs IERATIONS vs BATCHES . . Learning Rate . Natural Language Processing . . . References . Convolutions .",
            "url": "https://ablearn.io/machine%20learning/2021/04/29/ML.html",
            "relUrl": "/machine%20learning/2021/04/29/ML.html",
            "date": " • Apr 29, 2021"
        }
        
    
  
    
        ,"post14": {
            "title": "Understanding Azure AD and OAuth 2.0",
            "content": "Purpose . Understand Azure AD OAuth 2.0 Token Generation and Validation . Terms . Azure AD = Azure Active Directory | OAuth 2.0 = Protocol | OpenIDConnect = Library that makes it easy to setup custom OAuth and Setup Azure | . Steps . Registration . Register your app within AzureAD You will get an APP ID and TENANT ID | . | . Create a secret . Create a client secret OR | Create a certificate in Azure Key Vault | Upload the .cer | . Get the OAuth JWT Token . Azure AD will provide you with endpoints . | Request Template . Provide the APPID and secret | grant_type = client_credentials | scope : api://&lt;&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; Response will be JWT token . | &lt;/ul&gt; Access Control . App level permission - set roles works with token endpoints | works at the app level | . | Delegate permissions - set scopes works with authorize endpoints | works at a more granular level | . | . Add a authorized client application . for any app withih the same subscription (or another subscription in cases for mulit-tenant apps) Go to &quot;expose an API -&gt; add a authorized client application&quot; | . | this helps to use the same app registration and its corresponding configuration but all related apps. - In other words, 1 app registration can accomodate multiple apps . | . Add roles and permission . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; | . | .",
            "url": "https://ablearn.io/azure/2021/04/14/AzureAD.html",
            "relUrl": "/azure/2021/04/14/AzureAD.html",
            "date": " • Apr 14, 2021"
        }
        
    
  
    
        ,"post15": {
            "title": "MindMap - Become more creative and productive",
            "content": "",
            "url": "https://ablearn.io/others/2021/03/30/MindMap.html",
            "relUrl": "/others/2021/03/30/MindMap.html",
            "date": " • Mar 30, 2021"
        }
        
    
  
    
        ,"post16": {
            "title": "Robotics",
            "content": "Kalman Filter Intuition . PURPOSE . Kalman Filter are useful tools to measure something that cannot be measured directly! You gather all the attributes to measure it indirectly. Example : you cannot measure the moving vehicle position w.r.t to its surroundings, so use LIDAR to throw light at them and measure the reflections back to create a map of the objects around it. Its not going to be perfect, but its a good approx. Another example is SONAR - Measure the speed of sound reflection to measure ocean&#39;s depth. . Kalman filter has the these high level components:- . PREDICTION - Predict the next state of the object. | MEASUREMENT - Measure the current state of the object. | CONFIDENCE - How good (or bad) the PREDICTED or MEASUREMENT values are | . Kalman filter (or variants) have the following intuition. . CURRENT STATE . Suppose current time is 3:00pm | . PREDICTION over MEASUREMENT . After few mins, we look at the watch again and we PREDICT its 3:04pm, but its shows 3:55pm!! | We know the MEASUREMENT is wrong and have every reason to believe the PREDICTION is more acccurate and set the watch to be 3:04pm. Our CONFIDENCE in the MEASUREMENT is shaken, we believe it is so noisy to show 3:55pm | . MEASUREMENT over PREDICTION . On the other hand, if the measurement shows 3:05 pm, we have greater confidence that MEASUREMENT is correct, since its more or less in line with our expectations and since its actually measured, thats should be more accurate. Here we trust or more CONFIDENCE in the MEASUREMENT. | . Visual . Limitations . Kalman filters assumes the measurements and noise to have an gaussian distrbution. Does not suport multi-modal? . Particle Filter . Advantages . Concept . Path Planning . Djikistra algorithm . Assumptions . We have a map | We know the starting location and ending location | We know the cost to reach any node in the map i.e we have a weighted graph | . | Algorithm . Start at the source node. We maintain a tracker variable for registering its cost from source. | Since its origin, it is 0 Add this to your path list | . | Find neighbouring nodes Update the tracker variable from &quot;infinity&quot; to the cost from (source -&gt; neighbouring node) | If the node is already reached, then update the value of tracker only if its lesser than earlier value. | The &quot;value&quot; calculated from the source node. | . | Stop, when you reach the destination node | . | . It will give the shortest node from any node to all other nodes provided they are reachable from source. . As always, lets look at it visually! . Assume the cost between each cell is 1. . Visit each cell until you reach the TARGET node . . Working of A# (subset of Djikistra) . Uses the heuristic appraoch combined the elements of Djikistra use g_cost = distance from start node | . | keep track of the directions the goal use h_cost = distance towards the goal this could be &quot;euclidiean distnace (shortest distance between 2 points in space) (with or without collision) | Manhattan distance = grid distance between 2 points in space (avoiding collisions) | . | . | optimal distance In each iteration, always select the node with the lowest f_cost = sum(g_cost, h_cost) | . | . Working of RRT (Rapid Exploring Random Tree) . Use random probabilities to determine | Completely differet a* and djikstras algo | algorithm From the start position, generate random point | if its too far, reduce to max allowable threshold | connect the point to its nearest node. if you starting out it will be the root node, but subsquently there could other closer points | while connecting, if there is a collision, then ignore the point, then repeat | keep doing this until you generate a point close to the goal (closeness distance set by you) | . | . References . ConstructSIM . Matlab Tech Talk By Brian .",
            "url": "https://ablearn.io/robotics/2021/03/23/robotics.html",
            "relUrl": "/robotics/2021/03/23/robotics.html",
            "date": " • Mar 23, 2021"
        }
        
    
  
    
        ,"post17": {
            "title": "Review of the book - Atomic Habits",
            "content": "Purpose . Summarize the book &quot;Atomic Habits&quot; . Start Small (like really really really small) . If you want to develop a habit, start small. But people tend to start big even if they think its small. | The book talks about how a person wanting to get to GYM, started going to GYM, stay for 5 mins and come back home, for 4 weeks. | Eventually he developed a habit to go to GYM and become better. The principle is &quot;Standardize, then optimize&quot; | The book recommends the 2 minute rule : The first version of the should be so small that it can be started and completed within 2 minutes. (just like the line i am typing right now**) | . It&#39;s ok to miss a day. but not 2. . Often we going to strict routine and follow them to the tee, and after a day of disruption, give up telling ourselves that its worthless. | Missing 1 day is OK, dont miss 2 in a row. | Getting back in line is still waaaaaay better than giving up all the way. | . Connecting developing one to an already developed habit . If you want to floss, keep next to the tooth brush | . Remove friction towards good ones. . If you are going to GYM, then think of the smallest of things like direction of driving, getting water, clothes in your closet etc. because each one of them are contributing to your resistance. When you remove them or smoothen them out, it will easy not to miss GYM and before you know you are on your way!! | . Increase friction towards bad ones . Just place your remote in the other room and you wont watch hours of TV. | Just remove favorites in your browser and you wont watch youtube | Just move the junk good to a upper shelf, you will stop eating junk | Place the phone in the other room, you will stop addicted to your phone | . Avoiding Procastination . We always value present than future. unless the present is rewarding we wont start. we will look to the dream of achieving instead of hard work. Make the present rewarding, pleasant. . Break the future goal into different steps. | Ask yourself how you can enjoy the present. Series of such experiences should be connected to a goal you are seeking. | GOAL : Getting fit is the goal, but going to GYM is bad soln 1: Go to Gym and then you can get watch netflix for 30 mins soln 2 : Better, go to treadmill and watch netflix in the gym | . Develop an identity based habit . When we develop a habit, the best way to stick to it is cultivate an identity around it. For example, the goal is not be read 100 books, but the goal should be phrased to be say &quot;the goal is to become a READER&quot;. the goal is not to run a marathon, but become a RUNNER&quot;. . Habit : Daily GITHUB check-ins. lot of work today. so added a 2 min rule (which took 2 mins to complete) . Habits can get boring . When we develop a new habit, pretty soon the novelty dies and we can slip into complancency. Most habits we seek and develop require constant improvement. Instead of aiming to drastic improvements and giving up, we should seek organic small improvements. In others words, aim for constant, small enough to make improvements with little effort and but LARGE enough to feel satisified At first, it can become a trial-and-error but you can reach local optimum pretty quickly and continue the effort OR else you will give up the habits you started with. For eg. workouts. . Boredom (not failure) is impediment to success. Professionals stick to schedule while amateurs lets life &quot;get in the way&quot; . Importance of Genes in building habits . Genes only clarify what we need to choose to work on (not eliminate the need to work on) . References . James Clear Website and Book .",
            "url": "https://ablearn.io/others/2021/03/09/AtomicHabits.html",
            "relUrl": "/others/2021/03/09/AtomicHabits.html",
            "date": " • Mar 9, 2021"
        }
        
    
  
    
        ,"post18": {
            "title": "MUSHR - Final Config - Part 4",
            "content": "Purpose . Create a map, configure VISUALIZATION and watch the car self-drive! . . Visualization . What is RVIZ and how do i get it? . rviz is a 3D visualizer for the Robot Operating System (ROS) framework. Here we need to visualize and track car&#39;s movements in your laptop computer. So you have to set the variables correctly so that CAR can send the messages to your computer and vice-versa. . FAQs . 1. Why can&#39;t I open RVIZ directly by SSH-ing to car? In other words, why do i need laptop computer to open up RVIZ? RVIZ does not support X11 windows, and hence ssh -X will not work . 2. I have a mac/windows. How do I use RVIZ? Download the virtual machine image that already has the MuSHR stack and follow instructions here (highly recommended) . Set the ROS IP&#39;s Variables . The setup is best shown visually below . . Instructions . 1) Open .bashrc and set the variables. This will make sure we dont have to do this everytime you open a command shell. (trust me, you have to open a ton of times for testing) . sudo nano ~/.bashrc . Go to the last line of the script and add the following lines. Depending on whether you are setting the variables in the car or your laptop, set the IP ADDRESS accordingly. . export ROS_IP= &lt;&lt;LAPTOP_IP_ADDRESS&gt;&gt; export ROS_MASTER_URI=http://&lt;&lt;CAR_IP_ADDRESS&gt;&gt;:11311 . For those wondering how to get the IP ADDRESS . # To get the ip address ifconfig . LAPTOP IP Address will be dynamic by default and might be pain to enter each time you login. You can use the following script to extract &quot;IP Address&quot; from the configuration. . . enp0s17 = wireless interface that shows the IP address . laptop-ip-address . ifconfig enp0s17 | grep &quot;inet&quot; | awk &#39;NR==1{print $2}&#39; . . Important: NETWORK SETTING on the ORACLE VIRTUAL BOX Make sure the NETWORK SETTING on the ORACLE VIRTUAL BOX used to load MUSHR SIM (linux image) has &quot;Bridge Adapter&quot; (NAT is default. NAT will NOT work. It will translate the IP from the CAR to SIM and hence wont read any messages) . . Maps . For any car to drive, we need maps so that car can understanding the boundaries and navigate accordingly. . Create a map of your surrounding . Use GMAPPING Library . Start each one of following commands in a separate window as suggested here . # Start teleop (1st window) roslaunch mushr_base teleop.launch # Start gmapping scan (2nd window). MUSHR with a LIDAR publishes on the &quot;base_scan&quot; topic rosrun gmapping slam_gmapping scan:=scan . Drive around the house or park or whereever. . Tips for producing a good map . Drive slowly. | Drive around the same place couple times | Finish a loop | . We can save the built map using the following command. This command will listen to the map topic and save into the image. The map server package does this operation . rosrun map_server map_saver -f real-floor0 . The map generated will be clumsy at times. . . But, we can correct it using any photo editing software. After using GIMP it was modified to . . Note: You might to download the file to your local desktop, Open in GIMP/Photoshop to edit the software. . #download to local desktop scp robot@&lt;&lt;CAR IP&gt;&gt;:~/real-floor0.pgm ~/Desktop/ . #note that this uploads to home folder. Move it to appropriate folder later #upload from local desktop to car scp ~/Desktop/real-floor0.pgm robot@&lt;&lt;CAR IP&gt;&gt;:~ . CROP Map . By default the map contains an abundance of blank space around the area of detected surfaces By default the file will be 16MB, which will slow down the HALTON SAMPLER when you load the maps and run your car. So we need to crop unnecessary part of the image. But care has to be taken not to blindly crop and lose the origin. For this purpose there is an undocumented utility CROP_MAP to crop the map, removing the blank spaces around the map.In my case, it reduced by 20x. . . Edit CONFIG files to use the map . Base Package . Make sure mushr/mushr_base/mushr_base/mushr_base/maps has your map and mushr_base/launch/includes/map_server.launch is set to your map . RHC Package . Navigate to MUSHR RHC maps directory and place your maps. Note that you need both the PGM and YAML file. . Note: Make sure the YAML file is referring the correct PGM file . roscd mushr_rhc_ros cd maps #copy pgm files to the current directory (mushr_base/maps) cp &quot;pgm files from whichever directory you have them&quot; . . Now, change the launch files . #go 1 level higher. Current directory assumed (mushr_base/maps) cd .. cd launch #edit launch files. see screenshot nano map_server.launch . . Set the static map . Navigate to mushr_rhc_ros/launch/params/ . | Set the parameter static_map: &quot;/static_map&quot; . | . Battery . Battery is one of the most important elements in a self driving car. If the JETSON NANO is not powered long enough, you cannot TEST, DEBUG the car. I tried the NIiMH batteries suggested by the MUSHR team, but due to lmited battery knowledge ended up draining them and hence making it unusable!! So after thorough research, I ended buying LIPO BATTERY pack here and battery charger here . Warning: DO NOT IGNORE THIS SECTION. Having a good battery and taking good care of will save you ton of time downstream when you are stuck in code debugging or testing on track. . FAQs . Why LIPOS Battery (instead of NiMH)? . The advantages of lithium batteries compared to NiMH batteries are undeniable. The weight/power ratio in LiPo batteries is significantly better. LiPo batteries are noticeably lighter and they can store the same amount or more energy relative to their capacity than NiMH batteries. Also, . It costs the same as NiMH | Using a smart battery charger, control the charge and use it for years!! | . Ok, I agree LIPOS are good. What the hell is BALANCE CHARGE vs FAST CHARGE vs SLOW CHARGE vs STORAGE CHARGE? Which one do I use? . BALANCE CHARGE Charges both the cells inside the battery equally. Use this setting 99.99% of the time. . | STORAGE CHARGE Charges the cells in such a way so that you can store them for months (and hence the name). LIPOS if they are allowed to drain, will drain completely and not work this requiring them to jump start. . | FAST and SLOW CHARGE Charges fast and slow as name suggest. Used in certain special cases. . | . Ok, basic question, How do I connect and charge? . . . Do I ever going to need buy NiMH Batteries in the HARDWARE list section ? No . Low Voltage - If you keep the lipo batteries unused too long, the voltage drains and results in an &quot;low voltage error&quot; when you charge them back in a battery . . Use the NiMh option and charge the LIPO battey. this acts as a booster and increases voltage rapidly. | Increase above 4A and keep it right around 5AImportant :After 5A, after you feel its stabliizies around 5A mark, UNPLUG, Dont get greedy.- charge LIPO using LIPO options as normal and now the LOW VOLTAGE message will be gone. . | . . Here is good video about various battery sources for your JETSON NANO . https://www.youtube.com/watch?v=B4afWen1CsY . Jetson Booting Problem . Jetson sometime does not bootup when you set the voltage to 5V in the BUCK Converter . Slightly raising the output voltage of the buck converter (say to 5.2V) It is possible that when the Jetson Nano boots, the sudden required load current causes the buck converter&#39;s output voltage to momentarily drop below the Nano&#39;s minimum required voltage, so having it output a slightly higher voltage to compensate might allow it to successfully boot. Note that the Jetson Nano is spec&#39;d to handle up to 5.25V, and also has an overvoltage protection circuit . . References . Connect to RVIZ from the linux image . Low Voltage Video .",
            "url": "https://ablearn.io/robotics/2020/09/07/FinalSteps.html",
            "relUrl": "/robotics/2020/09/07/FinalSteps.html",
            "date": " • Sep 7, 2020"
        }
        
    
  
    
        ,"post19": {
            "title": "Useful Linux Commands",
            "content": "Purpose . Useful Commands for Linux . Commands . Install CUDA (11.0) . wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-ubuntu1804.pin sudo mv cuda-ubuntu1804.pin /etc/apt/preferences.d/cuda-repository-pin-600 wget https://developer.download.nvidia.com/compute/cuda/11.0.3/local_installers/cuda-repo-ubuntu1804-11-0-local_11.0.3-450.51.06-1_amd64.deb sudo dpkg -i cuda-repo-ubuntu1804-11-0-local_11.0.3-450.51.06-1_amd64.deb sudo apt-key add /var/cuda-repo-ubuntu1804-11-0-local/7fa2af80.pub sudo apt-get update sudo apt-get -y install cuda . Uninstall CUDA Toolkits (for upgrading purposes) . For runfile installations, you can check here . | If using apt-get in linux, then . | . sudo apt-get --purge remove cuda sudo apt-get autoremove dpkg --list |grep &quot;^rc&quot; | cut -d &quot; &quot; -f 3 | xargs sudo dpkg --purge . . Warning: This will remove all CUDNN files also. . Remove files from /usr/local folder also. | . cd /usr/local rm -rf cuda* . Update PATH VARIABLES. (XX.Y refers to your CUDA version) | . export PATH=${PATH}:/usr/local/cuda-XX.Y/bin export CUDA_HOME=${CUDA_HOME}:/usr/local/cuda:/usr/local/cuda-XX.Y export LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:/usr/local/cuda-XX.Y/lib64 . Removing CUDA also removed CUDNN so we need install from here so that ML frameworks such as tensorflow/pytorch work. | . Install XRDP . #!/bin/bash touch /etc/apt/sources.list sudo apt-get -y upgrade sudo apt-get -y update #install gnome desktop sudo apt-get install ubuntu-gnome-desktop -y #install xrdp sudo apt-get install xrdp -y sudo sed -i &#39;s/allowed_users=console/allowed_users=anybody/&#39; /etc/X11/Xwrapper.config #start remote desktop session sudo service xrdp restart . Install TWEAK TOOL to switch off animation and add icons to desktop . sudo apt install gnome-tweak-tool . After running the above command, you can use different settings and playaround. . . Setup SSH on a different port . for eg. 63000 . echo &quot;Port 63000&quot; &gt;&gt; /etc/ssh/sshd_config systemctl restart sshd sudo ufw allow 63000/tcp . To install ufw (incase its not installed) . sudo apt-get install ufw sudo ufw status sudo ufw enable . Error loading docker file . sudo chown &quot;$USER&quot;:&quot;$USER&quot; /home/&quot;$USER&quot;/.docker -R sudo chmod g+rwx &quot;/home/$USER/.docker&quot; -R . Stop all the containers . docker stop $(docker ps -a -q) . Remove all the containers . docker rm $(docker ps -a -q) . Install ssh on ubuntu . Install sudo apt-get install openssh-server | Enable the ssh service by typing sudo systemctl enable ssh | Start the ssh service by typing sudo systemctl start ssh | Test it by login into the system using ssh user@server-name | . Install CUDnn . Cudnn is needed to work with PYTORCH with c++. CudaToolkit does not install them. Start here and download the CUDNN for your machine and then type the following commands . tar -xzvf cudnn-11.0-linux-x64-v8.0.2.39.tgz sudo cp cuda/include/cudnn*.h /usr/local/cuda/include sudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64 sudo chmod a+r /usr/local/cuda/include/cudnn*.h /usr/local/cuda/lib64/libcudnn* . Disable nouveau for CUDA TOOLKIT to be installed . nano /etc/modprobe.d/blacklist-nvidia-nouveau.conf . Enter the following and save . blacklist nouveau options nouveau modeset=0 . References . CUDA Uninstall .",
            "url": "https://ablearn.io/linux/2020/09/06/LinuxCommands.html",
            "relUrl": "/linux/2020/09/06/LinuxCommands.html",
            "date": " • Sep 6, 2020"
        }
        
    
  
    
        ,"post20": {
            "title": "Azure Solutions Architect Prep",
            "content": "Purpose . These are the notes that I took when studying for AZ300, AZ301 certification. These are neither in the tradtional list format nor just a bunch of links pointing to Microsoft documentation. These are meant to be a quick cheatsheet or reference before the exam. This can also serve as a reference for general Azure questions as well (not necessarily for exam). I have provided them in image and PDF. Choose which ever suits your need. . How to use this? . For pass the certification one needs to have Azure experience for about 2-3 years and solved myriad of problems at their work place and there is not substitute for that. As mentioned before, it acts as a good cheatsheet or reference materials for the exam. . Important: Tips to navigate the mind map . Start at the center. | Pick a topic represented by a branch. | Follow along the branch and its sub-branches (representing sub-topics) | Come back to center and pick a different topic and repeat steps 2 and 3 | NOTES . Azure STORAGE . PDF Link . . . Azure VIRTUAL MACHINES . PDF Link . . . Azure ACTIVE DIRECTORY . PDF Link . . . Azure APIs . PDF Link . . . Azure DB and WEB APPS . PDF Link . . Collection . Steps to set outbound IP in a azure web app (app service) . . . Azure LOAD BALANCERS . PDF Link . . . Azure Virtual Networks . PDF Link . . . Azure Misc . PDF Link . . . References . Webapp Outbound IPs .",
            "url": "https://ablearn.io/azure/2020/09/05/AzExamNotes.html",
            "relUrl": "/azure/2020/09/05/AzExamNotes.html",
            "date": " • Sep 5, 2020"
        }
        
    
  
    
        ,"post21": {
            "title": "MUSHR - Hardware Build - Part 2",
            "content": "Purpose . The intention of this page is NOT to replace the existing documentation by MUSHR. Please follow them here, buit visit this page if things dont go as planned. I will highlight the sections that I ran into problems and solutions for them. This is NOT a TUTORIAL rather a buffet of practical problems faced and solutions provided to overcome them easily. . . Servo Motor Install Tips . During the installation of the SERVO MOTOR, You need to remove the hex screws. BUT the hex screws are tough to remove and might end up messing up the screw head. This damage can be permananent and will prevent from removing the existing SERVO MOTOR! In other words, the screws will SCREW you! and it did in my case. See below. To counter that, I had to remove the TOP parts (melt them, crack them etc.) and naturally the screws will open up at the bottom. This will take few hours to complete, since the screw heads was damaged there was any other way to remove screws. . . Weak 3D Printing Parts . There could be 3D printing parts that are weak and break. No issues, just use the glue and put them back! . . Buck Converter Steps . If you have a connector different than the MUSHR videos, then you might need to connect as shows to configure the buck converter . . Push Button Install Tip . Use the threads instead of screws. The screws didnt fit well to the back of the push button . . Camera Install Screw Alternative . This screw is provided as a part of BUCK CONVERTER . . Lower Platform Imperfections . 3D Imperfections are OK . . Upper Platform Trick . Stick a tape to the nut and them remove it. This is useful to screws on the backside of the platform which might fall off. . . Final Door Trick . If you screw the final door, then you need to remove it again for charging the batteries (which is more often than you think). To solve the issue, use the VELCRO as shown . . Misc Hardware Pics .",
            "url": "https://ablearn.io/robotics/2020/05/10/MUSHRHw.html",
            "relUrl": "/robotics/2020/05/10/MUSHRHw.html",
            "date": " • May 10, 2020"
        }
        
    
  
    
        ,"post22": {
            "title": "MUSHR - Software Install - Part 3",
            "content": "Purpose . The intention of this page is NOT to replace the existing documentation by MUSHR. Please follow them here, buit visit this page if things dont go as planned. This is NOT a TUTORIAL rather a buffet of practical problems faced and solutions provided to overcome them easily. . . Scenarios . Monitor not booting up, after MUSHR image is installed. . Issue . When you install the stock image of JETSON NANO using BelenaEtcher, connect JETSON NANO using HDMI, NAno boots and you can see the Linux GUI. But, if you do the same exact same thing using MUSHR IMAGE, it does not show the GUI. Without the GUI, software config is almost impossible. Well, almost. There is a &quot;command line&quot; solution provided as well for all the geeks out there. . Solution 1 . USE DISPLAY PORT . Important: If you have a monitor that supports DISPLAY PORT, use it!! Instant solution. (Time saving : 3-4 days). You can buy the display port from the link below. . DISPLAY PORT to DISPLAY PORT cable | . If you only have HDMI port, read on . Solution 2 . USE COMMAND LINE . STEP 1: SSH . You need to LOGIN (using SSH). For SSH, you need WIFI, For WIFI you need to LOGIN. Get it! chicken-egg problem? Almost. There is one possible solution, Hard-wired USB connection . Connect the MICRO USB port of NANO to the USB PORT of your laptop (also known as HEADLESS LOGIN) | . HEADLESS LOGIN (on a Mac) . #Go to dev directory cd /dev #check for all things like tty and usb ls *tty*usb* tty.usbmodem146123 # result (yours might be slightly different) #use the screen command with the result of previous step + serial interface number screen tty.usbmodem146123 115200 # 115200 is the Jetson serial interface. &quot;screen&quot; is installed on a mac by default # (It will ask for SUDO login and password) . . ACTIVATE THE ROOT in JETSON NANO . Be aware that the root account is not active by default. You will need to do: . sudo passwd root . Then go ahead to set password. Use the root user using . su root . USE WPA_SUPPLICANT TO CONNECT . # CREATE a file at */etc/wpa_supplicant/wpa_supplicant.conf* vi /etc/wpa_supplicant/wpa_supplicant.conf # PRESS &quot;I&quot; to start insert mode # Add this content to the file ctrl_interface=/run/wpa_supplicant update_config=1 # PRESS &quot;:wq&quot; to save and quit the editor # Then, run this command wpa_supplicant -B -i wlan0 -c /etc/wpa_supplicant/wpa_supplicant.conf wpa_cli -i wlan0 # This will present an interactive prompt (&gt;), which has tab completion and descriptions of completed commands. scan # Ok # &lt;3&gt;CTRL-EVENT-SCAN-RESULTS scan_results # bssid / frequency / signal level / flags / ssid # 00:00:00:00:00:00 2462 -49 [WPA2-PSK-CCMP][ESS] MYSSID # 11:11:11:11:11:11 2437 -64 [WPA2-PSK-CCMP][ESS] ANOTHERSSID add_network # 0 set_network 0 ssid &quot;MYSSID&quot; set_network 0 psk &quot;passphrase&quot; enable_network 0 #&lt;2&gt;CTRL-EVENT-CONNECTED - Connection to 00:00:00:00:00:00 completed (reauth) [id=0 id_str=] save_config # OK quit . Check if the wireless device is up . ip link show wlan0 #Look for the word &quot;UP&quot; inside the brackets in the first line of the output. /sbin/iw wlan0 link # Connected to &lt;your network&gt;&gt; ping google.com # testing if it connects ifconfig #GET the IP. Needed to connect to SSH. Uff, finally! . **Connect to the JETSON NANO . Open a terminal window . ssh -X robot@IP_ADDRESS #enter your IP address (Notice -X) . STEP 2: GPARTED, BLUETOOTH, IP ADDRESS FIX . Before installing anything, there seems to be some sort of lock on the &quot;linux&quot; installer to donwload. For eg. . E: Could not get lock /var/lib/dpkg/lock – open (11: Resource temporarily unavailable) E: Unable to lock the administration directory (/var/lib/dpkg/), is another process using it? . Here is the fix : . sudo rm /var/lib/apt/lists/lock sudo rm /var/cache/apt/archives/lock sudo rm /var/lib/dpkg/lock sudo rm /var/lib/dpkg/lock-frontend . GPARTED . sudo apt-get install gparted gparted # opens up the GUI as shown below. Now, follow instructions from the MUSHR site. . . . BLUETOOTH . hcitool dev . If the Bluetooth adapter is working and available, you should see its address listed (each address is unique): . Devices: hci0 38:##:##:##:##:## . bluetoothctl #opens up a bluetooth prompt agent on scan on # scan near by bluetooth devices. Start your PS4 controller scan off # Switch it off after getting the MAC ADDRESS of the wireless controller trust MAC_ADDRESS # trust, pair and connect pair MAC_ADDRRESS connect MAC_ADDRESS . . After connecting to the bluetooth, we should test it. . sudo apt-get install -y jstest-gtk # install the jstest package jstest-gtk #run this. It will open up GUI as follows . . IP ADDRESS FIX . ROS IP and MASTER URI will not be set to your IP automatically. USe this commands below in the local bashrc file so that it is set everytime you login (vs you typing these commands everytime) . export ROS_IP=&lt;&lt;YOUR_IP_ADDRESS&gt;&gt; export ROS_MASTER_URI=http://&lt;&lt;YOUR_IP_ADDRESS&gt;&gt;:11311 . . BLDC Tool . . Important: Now, this step can only be done when you finish the Hardware configuration since you need the VESC connected to JETSON NANO + MOTOR to test it properly. . cd &amp;&amp; ./bldc-tool/BLDC_Tool . Version Problem . If you have ordered Flipsky VESC, chances are that you will have 3.X firmware installed (The hardware version is 410/411/412, but the firmware version is different). . Warning: BLDC tool can only support 2.18 (and hence MUSHR car). This wil throw an SERIAL PORT ERROR and will NOT connect to BLDC TOOL GUI. BLDC is old tool and is replaced by VESC TOOL . Solution . Download the VESC TOOL from the website and it will let you connect easily. | It will ask you to install the latest software.DO NOT install latest. | Use &quot;custom&quot; firmware (follow this link). The custom firmware is nothing but the BLDC servoout.bin file that is installed via BLDC tool. (also available here) | Now, open the BLDC tool and hit &quot;connect&quot; and it will be successful. | Follow the instructions from the MUSHR Software Setup i.e load the sensorless xml config file and &quot;write&quot; it back to VESC | . Note: If VESC tool is a newer version of the BLDC tool, why is heck do we need BLDC tool? MUSHR parameters has been set to BLDC tool and works only when you use it. . Note: Everything spinning in the right direction?? In the BLDC tool, Push the &quot;top&quot; arrow key and the wheels should move forward, and vice versa, if it moves in the reverse direction, it means the &quot;wires&quot; from the VESC to the motor has been criss crossed. Reverse the wire connection and you should be good. See below for visual. I personally used VESC tool to check it. . Installing MUSHR from Scratch . START from here . https://mushr.io/hardware/nano_install/ . RealSense SDK Install Prior to MUSHR Stack . DO NOT install using just 1 script mentioned here, but | Go step wise Install ROS | STOP! Do not mushr stack, Install REAL SENSE hardware block (&lt; critical) | Then, - install MUSHR | . | . Pytorch Version Issue . Mushr supports 1.1 version of torch as shown here . torch 1.1 needs CUDA 10.0 as shown here | . | Mushr also supports only python 2.7 . | Keeping these 2 constraints in mind, you cannot take the latest STOCK image from NVIDIA (Jetpack 4.5 at the time of writing which supports CUDA 10.2) since CUDA 10.2 is not compatible with torch 1.1 (rather its only compatible with torch 1.5 and above) . | . So 2 options . Install this and know your version first . If you have not started, pick JETPACK 4.2.X as stock image | If you are wrapping up then dont want to go back all the way to starting point of installation install torch 1.4 from here. Technically, we need torch 1.5, BUT torch 1.5 only supports python 3.6. so there are 2 sub-options | . | . | . - update python 2.7 ==&gt; 3.6 &amp;&amp; update to torch 1.5. But python update is a nightmare - update to torch 1.4 (as close as possible to 1.5) and install fix any minor errors - install &quot;libopenblas&quot; library [here](https://www.openblas.net/) - update bashrc as shown [here](https://www.programmersought.com/article/90674363798/) . References . wiki.archlinux | bbs.archlinux.org | linuxcommando | desertbot | .",
            "url": "https://ablearn.io/robotics/2020/05/02/MUSHRSw.html",
            "relUrl": "/robotics/2020/05/02/MUSHRSw.html",
            "date": " • May 2, 2020"
        }
        
    
  
    
        ,"post23": {
            "title": "MUSHR - Purchase and Setup - Part 1",
            "content": "Acquiring the parts . All the materials needed for the projects and outlined here It even provides links to various websites to order parts. . . Tip 1 : Order the 3D parts first . . Important: There are 3D printed parts needed to complete the assembly. Might add &quot;weeks&quot; to your project if taken lightly. Order the 3D parts here . Ordering the 3D printed parts is tricky. The local library will be cheap but will take weeks before they complete. On the other hand, there are industries who can finish in 2 days, but will charge ~400-500 bucks. After a lot of research I found 3DHUBS. They are fast (3-4 day shippng time) and cheaper than most of them I could find on the internet. . They need your offical/school email ID (not personal gmail/outlook) | Choose Standard PLA (FDM) This is the cheapest material. | DO NOT order racecar_cover_number, racecar_cover_text and racecar_cover_image It&#39;s not needed to build the assembly and will save you about $30-40. Moreover, 3dhubs will report these as errors which are tough to fix (not a good use of your time) . Important: By the way it will cost you around ~$175 | . Tip 2 : Order Tools and Accessories . Apart from ordering parts from https://mushr.io/hardware/build_instructions/ there are extra tools needed which are assumed to be present in every household. Make sure you order the following parts also. . Screw Driver Set (115 in 1 Magnetic Screwdriver) You need screw driver bits of various shapes and sizes to finish the project, so do not assume your 1/2 favorite screw driver will work in this case | Crimping tool | HandGun | Soldering Kit | HXT 4mm Bullet to Male T-Plug Connector | . Optional Tools . Important: Applicable to only MACBOOK and USBC-only windows laptop. Find similar products to suit your laptop configuration. . MEMORY CARD reader | . Tip 3 : Order REDCAT with Brushless Motor installed . In the instructions manual, there are 2 options for provided for the Redcat Racing Blackout. . Choose the one with BRUSHLESS MOTOR | DO NOT ORDER &quot;Sensorless Brushless Motor&quot; (saves $30) | . Tip 4 : Picking the right VESC . This is the order of recommendation. . Turnigy version (Recommended. No soldering required) . | Flipsky version (I ordered this due to &quot;out of stock&quot; on other items) . | MayTech version . | . Tip 5 : Misc Tips . Intel RealSense T265 is not needed for your first version of the car. You can add later if you choose to. | Solderless Wire Connector from the instructions is currently unavailable. Use this link as an alternative | .",
            "url": "https://ablearn.io/robotics/2020/04/26/MUSHRSetup.html",
            "relUrl": "/robotics/2020/04/26/MUSHRSetup.html",
            "date": " • Apr 26, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I am Abhishek Sreesaila, certified Azure Cloud Solution Architect at EY, with a passion for data and machine learning. I am also a self-driving car enthusiast, built a self driving car during my spare time and using it to learn robotics. . In the blog, you will find pratical tips on data, azure cloud, machine learning and robotics. Also, I will throw in couple of life hacks I have learnt along the way such as mind maps, clean code, self-help books that will possibly enrich your career and personal life. I play tennis and chess but not at a level that I can write about it :) You can reach me at this email address .",
          "url": "https://ablearn.io/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  

  
  

  
      ,"page12": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://ablearn.io/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}